{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实验T3和T4：考虑时间问题，对AllTrain（AllTest为0）进行按时排序，有前20%进行第一次训练，预测第20%+1个样本的犯罪类型，后续再增加一个block（可能有几百或者几千个样本）进行训练，对第20%+block+1个样本进行预测，然后循环进行直至最后一个样本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "\n",
    "from keras import layers,metrics\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.layers import Dense, LSTM, Dropout, Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import PReLU, ELU\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.utils import np_utils, multi_gpu_model\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "from sklearn.utils import shuffle as reset\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV,StratifiedShuffleSplit\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import log_loss,make_scorer\n",
    "\n",
    "from matplotlib.colors import LogNorm\n",
    "# import \n",
    "from matplotlib.pylab import plt\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "from imblearn.over_sampling import RandomOverSampler #https://imbalanced-learn.org/stable/generated/imblearn.over_sampling.RandomOverSampler.html?highlight=randomoversampler\n",
    "from frplayer import FilterResponseNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_DataFrame(data, test_size=0.2, considerTime=True, random_state=None):\n",
    "    # ConsiderTime-------trainDF和testDF分割时是否考虑时间问题，即是否需要随机打乱。True:按照‘Dates’列进行降序排列,False：随机打乱样本的顺序，\n",
    "    if considerTime:\n",
    "        data=data.sort_values(by=\"Dates\", ascending=True)\n",
    "    else:\n",
    "        data=reset(data, random_state=random_state)\n",
    "    train=data[int(len(data)*test_size):].reset_index(drop=True)\n",
    "    test=data[:int(len(data)*test_size)].reset_index(drop=True)\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_time(x):\n",
    "    DD=datetime.strptime(x,\"%Y-%m-%d %H:%M\")#zj    \n",
    "    time=DD.hour#*60+DD.minute\n",
    "    day=DD.day\n",
    "    month=DD.month\n",
    "    year=DD.year\n",
    "    return time,day,month,year\n",
    "def Dates2TDMY(x):\n",
    "    DD=datetime.strptime(x,\"%Y-%m-%d %H:%M\")#zj  \n",
    "    time=DD.hour#*60+DD.minute\n",
    "    day=DD.day\n",
    "    month=DD.month\n",
    "    year=DD.year\n",
    "    #T_D_M_Y=str(time)+str(day)+str(month)+str(year)\n",
    "    T_D_M_Y=str(time)+str(day)+str(month)\n",
    "    return T_D_M_Y\n",
    "def get_season(x):\n",
    "    summer=0\n",
    "    fall=0\n",
    "    winter=0\n",
    "    spring=0\n",
    "    if (x in [5, 6, 7]):\n",
    "        summer=1\n",
    "    if (x in [8, 9, 10]):\n",
    "        fall=1\n",
    "    if (x in [11, 0, 1]):\n",
    "        winter=1\n",
    "    if (x in [2, 3, 4]):\n",
    "        spring=1\n",
    "    return summer, fall, winter, spring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def field2Vec(trainDF,testDF,fieldStr):\n",
    "    fields=sorted(trainDF[fieldStr].unique())\n",
    "    categories=sorted(trainDF[\"Category\"].unique())\n",
    "    C_counts=trainDF.groupby([\"Category\"]).size()\n",
    "    F_C_counts=trainDF.groupby([fieldStr,\"Category\"]).size()\n",
    "    F_counts=trainDF.groupby([fieldStr]).size()\n",
    "    logodds={}\n",
    "    logoddsPF={}\n",
    "    MIN_CAT_COUNTS=2\n",
    "    default_logodds=np.log(C_counts/len(trainDF))-np.log(1.0-C_counts/float(len(trainDF)))\n",
    "    for f in fields:\n",
    "        PA=F_counts[f]/float(len(trainDF))\n",
    "        logoddsPF[f]=np.log(PA)-np.log(1.-PA)\n",
    "        logodds[f]=deepcopy(default_logodds)\n",
    "        for cat in F_C_counts[f].keys():\n",
    "            if (F_C_counts[f][cat]>MIN_CAT_COUNTS) and F_C_counts[f][cat]<F_counts[f]:\n",
    "                PA=F_C_counts[f][cat]/float(F_counts[f])\n",
    "                logodds[f][categories.index(cat)]=np.log(PA)-np.log(1.0-PA)\n",
    "        logodds[f]=pd.Series(logodds[f])\n",
    "        logodds[f].index=range(len(categories))\n",
    "    ########此部分代码，从逻辑上不应该出现在此处，但是为了编程的方便，放在了此处#########\n",
    "    #fieldsTest=sorted(testDF[fieldStr].unique())\n",
    "    #N_count=0\n",
    "    #for f in fieldsTest:\n",
    "        #if f not in fields:\n",
    "            #logoddsPF[f]=-50.0  #np.log(0.)-np.log(1.)=-inf,便于计算，改为-99999.0\n",
    "            #logodds[f]=deepcopy(default_logodds)\n",
    "            #pa=1.0/float(len(categories))\n",
    "            #logodds[f][range(len(categories))]=np.log(pa)-np.log(1.0-pa)\n",
    "            #logodds[f]=pd.Series(logodds[f])\n",
    "            #logodds[f].index=range(len(categories))\n",
    "            #N_count=N_count+1\n",
    "    #print(fieldStr+' N_count: '+str(N_count))\n",
    "    ########此部分代码，从逻辑上不应该出现在此处，但是为了编程的方便，放在了此处#########\n",
    "    #引进代码原作者的新思想\n",
    "    if testDF.shape[0]>0: #如果testDF里有样本,......\n",
    "        print('There are some new:'+fieldStr)\n",
    "        new_fields=sorted(testDF[fieldStr].unique())\n",
    "        new_F_counts=testDF.groupby(fieldStr).size()\n",
    "        only_new=set(new_fields+fields)-set(fields)\n",
    "        only_old=set(new_fields+fields)-set(new_fields)\n",
    "        in_both=set(new_fields).intersection(fields)\n",
    "        print('# only_new_fieldds:'+str(len(only_new)))\n",
    "        for f in only_new:\n",
    "            PA=new_F_counts[f]/float(len(testDF)+len(trainDF))\n",
    "            logoddsPF[f]=np.log(PA)-np.log(1.-PA)\n",
    "            logodds[f]=deepcopy(default_logodds)\n",
    "            logodds[f].index=range(len(categories))\n",
    "        for f in in_both:\n",
    "            PA=(F_counts[f]+new_F_counts[f])/float(len(testDF)+len(trainDF))\n",
    "            logoddsPF[f]=np.log(PA)-np.log(1.-PA)    \n",
    "    return logodds,logoddsPF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data(df,logodds_A,logoddsPF_A,logodds_T,logoddsPF_T,needT_D_M_Y=False):\n",
    "    feature_list=df.columns.tolist()\n",
    "    if \"Descript\" in feature_list:\n",
    "        feature_list.remove(\"Descript\")\n",
    "    if \"Resolution\" in feature_list:\n",
    "        feature_list.remove(\"Resolution\")\n",
    "    if \"Category\" in feature_list:\n",
    "        feature_list.remove(\"Category\")\n",
    "    if \"Id\" in feature_list:\n",
    "        feature_list.remove(\"Id\")\n",
    "\n",
    "    cleanData=df[feature_list]\n",
    "    cleanData.index=range(len(df))\n",
    "    print(\"Creating address features\")###Creating address features###\n",
    "    address_features=cleanData[\"Address\"].apply(lambda x: logodds_A[x])\n",
    "    address_features.columns=[\"logodds_A\"+str(x) for x in range(len(address_features.columns))]\n",
    "    if needT_D_M_Y:\n",
    "        print(\"Creating time T_D_M_Y features\")###Creating time T_D_M_Y features###\n",
    "        T_D_M_Y_features=cleanData[\"T_D_M_Y\"].apply(lambda xx: logodds_T[xx])\n",
    "        T_D_M_Y_features.columns=[\"logodds_T\"+str(xx) for xx in range(len(T_D_M_Y_features.columns))]\n",
    "\n",
    "    print(\"Parsing dates\")            ###Creating address features###\n",
    "    cleanData[\"Time\"], cleanData[\"Day\"], cleanData[\"Month\"], cleanData[\"Year\"]=zip(*cleanData[\"Dates\"].apply(parse_time))\n",
    "    #     dummy_ranks_DAY = pd.get_dummies(cleanData['DayOfWeek'], prefix='DAY')\n",
    "    days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "    #     cleanData[\"DayOfWeek\"]=cleanData[\"DayOfWeek\"].apply(lambda x: days.index(x)/float(len(days)))\n",
    "    print(\"Creating one-hot variables\")\n",
    "    dummy_ranks_PD = pd.get_dummies(cleanData['PdDistrict'], prefix='PD')\n",
    "    dummy_ranks_DAY = pd.get_dummies(cleanData[\"DayOfWeek\"], prefix='DAY')\n",
    "    cleanData[\"IsInterection\"]=cleanData[\"Address\"].apply(lambda x: 1 if \"/\" in x else 0)\n",
    "    cleanData[\"logoddsPF_A\"]=cleanData[\"Address\"].apply(lambda x: logoddsPF_A[x])\n",
    "    if needT_D_M_Y:\n",
    "        cleanData[\"logoddsPF_T\"]=cleanData[\"T_D_M_Y\"].apply(lambda x: logoddsPF_T[x])\n",
    "    print(\"droping processed columns\")\n",
    "    cleanData=cleanData.drop(\"PdDistrict\",axis=1)\n",
    "    cleanData=cleanData.drop(\"DayOfWeek\",axis=1)\n",
    "    cleanData=cleanData.drop(\"Address\",axis=1)    \n",
    "    cleanData=cleanData.drop(\"Dates\",axis=1)\n",
    "    if needT_D_M_Y:\n",
    "        cleanData=cleanData.drop(\"T_D_M_Y\",axis=1)\n",
    "    feature_list=cleanData.columns.tolist()\n",
    "    print(\"joining one-hot features\")\n",
    "    if needT_D_M_Y:\n",
    "        features = cleanData[feature_list].join(dummy_ranks_PD.iloc[:,:]).join(dummy_ranks_DAY.iloc[:,:]).join(address_features.iloc[:,:]).join(T_D_M_Y_features.iloc[:,:])\n",
    "    else:\n",
    "        features = cleanData[feature_list].join(dummy_ranks_PD.iloc[:,:]).join(dummy_ranks_DAY.iloc[:,:]).join(address_features.iloc[:,:])\n",
    "    print(\"creating new features\")\n",
    "    features[\"IsDup\"]=pd.Series(features.duplicated()|features.duplicated(keep='last')).apply(int)\n",
    "    features[\"Awake\"]=features[\"Time\"].apply(lambda x: 1 if (x==0 or (x>=8 and x<=23)) else 0)\n",
    "    features[\"Summer\"], features[\"Fall\"], features[\"Winter\"], features[\"Spring\"]=zip(*features[\"Month\"].apply(get_season))\n",
    "    if \"Category\" in df.columns:\n",
    "        labels = df[\"Category\"].astype('category')\n",
    "    else:\n",
    "        labels=None\n",
    "    return features,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(X, Y, lookback, delay, min_index, max_index, shuffle=False, batch_size=128, step=6):\n",
    "    if max_index is None:\n",
    "        max_index = len(X) - delay - 1\n",
    "    i = min_index + lookback\n",
    "    while 1:\n",
    "        if shuffle:\n",
    "            rows = np.random.randint(min_index + lookback, max_index, size=batch_size)\n",
    "        else:\n",
    "            if i + batch_size >= max_index:\n",
    "                i = min_index + lookback\n",
    "            rows = np.arange(i, min(i + batch_size, max_index))\n",
    "            i += len(rows)\n",
    "\n",
    "        samples = np.zeros((len(rows), lookback // step, X.shape[-1]))\n",
    "        targets = np.zeros((len(rows),Y.shape[1]))\n",
    "        for j, row in enumerate(rows):\n",
    "            indices = range(rows[j] - lookback, rows[j], step)\n",
    "            samples[j] = X[indices]\n",
    "            targets[j] = Y[rows[j]+delay]\n",
    "        yield samples, targets\n",
    "    #Now here is the data generator that we will use. It yields a tuple (samples, targets) where samples is one batch of input data and targets is the corresponding array of target temperatures. It takes the following arguments:\n",
    "        # •data: The original array of floating point data, which we just normalized in the code snippet above.\n",
    "        # •lookback: How many timesteps back should our input data go.\n",
    "        # •delay: How many timesteps in the future should our target be.\n",
    "        # •min_index and max_index: Indices in the data array that delimit which timesteps to draw from. This is useful for keeping a segment of the data for validation and another one for testing.\n",
    "        # •shuffle: Whether to shuffle our samples or draw them in chronological order.\n",
    "        # •batch_size: The number of samples per batch.\n",
    "        # •step: The period, in timesteps, at which we sample data. We will set it 6 in order to draw one data point every hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of OrginalAllDF: (145962, 9)\n",
      "The shape of AllDF after del wrong X and Y values: (145960, 9)\n",
      "The shape of AllDF after drop_duplicates: (145876, 9)\n",
      "(145859, 2)\n",
      "Address_counts_allDF_trainDF_testDF: 17161_17161_0\n",
      "The # of AllDF, AllTrain, AllTest, is: 145876,145876,0\n",
      "-----------LOGOODS: Address-------------\n",
      "-----------LOGOODS: T_D_M_Y-------------\n",
      "-----------LOGOODS: parse_data of Alltrain  -------------\n",
      "Creating address features\n",
      "Creating time T_D_M_Y features\n",
      "Parsing dates\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating one-hot variables\n",
      "droping processed columns\n",
      "joining one-hot features\n",
      "creating new features\n",
      "['X', 'Y', 'Time', 'Day', 'Month', 'Year', 'IsInterection', 'logoddsPF_A', 'logoddsPF_T', 'PD_星期一', 'PD_星期三', 'PD_星期二', 'PD_星期五', 'PD_星期六', 'PD_星期四', 'PD_星期日', 'DAY_星期一', 'DAY_星期三', 'DAY_星期二', 'DAY_星期五', 'DAY_星期六', 'DAY_星期四', 'DAY_星期日', 'logodds_A0', 'logodds_A1', 'logodds_A2', 'logodds_A3', 'logodds_A4', 'logodds_A5', 'logodds_A6', 'logodds_A7', 'logodds_A8', 'logodds_A9', 'logodds_A10', 'logodds_A11', 'logodds_A12', 'logodds_A13', 'logodds_A14', 'logodds_A15', 'logodds_A16', 'logodds_A17', 'logodds_A18', 'logodds_A19', 'logodds_A20', 'logodds_A21', 'logodds_A22', 'logodds_A23', 'logodds_A24', 'logodds_A25', 'logodds_A26', 'logodds_A27', 'logodds_A28', 'logodds_A29', 'logodds_A30', 'logodds_A31', 'logodds_A32', 'logodds_A33', 'logodds_A34', 'logodds_A35', 'logodds_A36', 'logodds_A37', 'logodds_A38', 'logodds_A39', 'logodds_A40', 'logodds_A41', 'logodds_A42', 'logodds_A43', 'logodds_A44', 'logodds_A45', 'logodds_A46', 'logodds_A47', 'logodds_A48', 'logodds_A49', 'logodds_T0', 'logodds_T1', 'logodds_T2', 'logodds_T3', 'logodds_T4', 'logodds_T5', 'logodds_T6', 'logodds_T7', 'logodds_T8', 'logodds_T9', 'logodds_T10', 'logodds_T11', 'logodds_T12', 'logodds_T13', 'logodds_T14', 'logodds_T15', 'logodds_T16', 'logodds_T17', 'logodds_T18', 'logodds_T19', 'logodds_T20', 'logodds_T21', 'logodds_T22', 'logodds_T23', 'logodds_T24', 'logodds_T25', 'logodds_T26', 'logodds_T27', 'logodds_T28', 'logodds_T29', 'logodds_T30', 'logodds_T31', 'logodds_T32', 'logodds_T33', 'logodds_T34', 'logodds_T35', 'logodds_T36', 'logodds_T37', 'logodds_T38', 'logodds_T39', 'logodds_T40', 'logodds_T41', 'logodds_T42', 'logodds_T43', 'logodds_T44', 'logodds_T45', 'logodds_T46', 'logodds_T47', 'logodds_T48', 'logodds_T49', 'IsDup', 'Awake', 'Summer', 'Fall', 'Winter', 'Spring']\n",
      "129\n",
      "------------Attention: we do not RandomOverSampler---------------\n",
      "------------ConsiderTime:  Sorting--------------\n"
     ]
    }
   ],
   "source": [
    "#Import data\n",
    "ConsiderTime=True#False# True##trainDF和testDF分割时是否考虑时间问题，即是否需要随机打乱。True:按照‘Dates’列进行降序排列,False：随机打乱样本的顺序，\n",
    "Rate_ALL=0.0 #0.0即不保留测试机\n",
    "needOverSampler=False\n",
    "needT_D_M_Y=True #False  使用_T_D_M_Y和周几\n",
    "allDF=pd.read_csv(\"./HuZhou.csv\")\n",
    "print('The shape of OrginalAllDF: '+str(allDF.shape))\n",
    "\n",
    "xy_scaler=preprocessing.StandardScaler()\n",
    "xy_scaler.fit(allDF[[\"X\",\"Y\"]])\n",
    "allDF[[\"X\",\"Y\"]]=xy_scaler.transform(allDF[[\"X\",\"Y\"]])\n",
    "allDF=allDF[abs(allDF[\"Y\"])<100]\n",
    "allDF.index=range(len(allDF))\n",
    "print('The shape of AllDF after del wrong X and Y values: '+str(allDF.shape))\n",
    "\n",
    "def listCat(x):\n",
    "    return list(x)\n",
    "allDF.drop_duplicates(inplace=True,subset=['Dates', 'DayOfWeek', 'PdDistrict', 'Address', 'X', 'Y', 'Category'])\n",
    "Train_duplicated=pd.pivot_table(allDF,index=['Dates','DayOfWeek','PdDistrict', 'Address', 'X', 'Y'], values='Category',aggfunc=[len,listCat])\n",
    "print('The shape of AllDF after drop_duplicates: '+str(allDF.shape))\n",
    "print(Train_duplicated.shape)\n",
    "\n",
    "trainDF,testDF=train_test_split_DataFrame(allDF, test_size=Rate_ALL, considerTime=ConsiderTime, random_state=None)\n",
    "print('Address_counts_allDF_trainDF_testDF: ' + str(len(allDF[\"Address\"].unique())) + '_'+ str(len(trainDF[\"Address\"].unique())) + '_' + str(len(testDF[\"Address\"].unique())))\n",
    "\n",
    "N_AllSample=allDF.shape[0]\n",
    "N_AllTrain=trainDF.shape[0]\n",
    "N_AllTest=testDF.shape[0]\n",
    "N_CLASS=len(allDF[\"Category\"].unique())\n",
    "print('The # of AllDF, AllTrain, AllTest, is: '+str(N_AllSample)+','+str(N_AllTrain)+','+str(N_AllTest))\n",
    "#################Now proceed as before#################\n",
    "print('-----------LOGOODS: Address-------------')\n",
    "logodds_A,logoddsPF_A=field2Vec(trainDF,testDF,\"Address\")\n",
    "if needT_D_M_Y:\n",
    "    trainDF[\"T_D_M_Y\"]=trainDF[\"Dates\"].apply(Dates2TDMY)\n",
    "    trainDF[\"T_D_M_Y\"]=trainDF[\"T_D_M_Y\"]+trainDF[\"DayOfWeek\"]\n",
    "    if Rate_ALL>0:\n",
    "        testDF[[\"X\",\"Y\"]]=xy_scaler.transform(testDF[[\"X\",\"Y\"]])\n",
    "        testDF[\"T_D_M_Y\"]=testDF[\"Dates\"].apply(Dates2TDMY)\n",
    "        testDF[\"T_D_M_Y\"]=testDF[\"T_D_M_Y\"]+testDF[\"DayOfWeek\"]\n",
    "    print('-----------LOGOODS: T_D_M_Y-------------')\n",
    "    logodds_T,logoddsPF_T=field2Vec(trainDF,testDF,\"T_D_M_Y\")    \n",
    "else:\n",
    "    logodds_T=None\n",
    "    logoddsPF_T=None\n",
    "    \n",
    "print('-----------LOGOODS: parse_data of Alltrain  -------------')\n",
    "features, labels=parse_data(trainDF,logodds_A,logoddsPF_A,logodds_T,logoddsPF_T,needT_D_M_Y) \n",
    "if Rate_ALL>0:\n",
    "    print('-----------LOGOODS: parse_data of Alltest  -------------')\n",
    "    features_test, labels_test=parse_data(testDF,logodds_A,logoddsPF_A,logodds_T,logoddsPF_T,needT_D_M_Y)###########和训练集使用同样的时间和地点Logoodds值#####\n",
    "    x_test=features_test.values\n",
    "    y_test=labels_test.values\n",
    "    y_test = keras.utils.to_categorical(LabelEncoder().fit_transform(np.array(y_test)), num_classes=N_CLASS)\n",
    "\n",
    "print(features.columns.tolist())\n",
    "print(len(features.columns))\n",
    "\n",
    "collist=features.columns.tolist()\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scaler.fit(features)\n",
    "features[collist]=scaler.transform(features)\n",
    "if Rate_ALL>0:\n",
    "    features_test[collist]=scaler.transform(features_test)###########和训练集使用同样的scaler值#####\n",
    "######################################################\n",
    "#############################先进行过采样，然后再根据时间来排序##################################\n",
    "if needOverSampler:\n",
    "    print('------------RandomOverSampler--------------')\n",
    "    ros = RandomOverSampler()\n",
    "    featuresArrayOverSampler, labelsArrayOverSampler = ros.fit_resample(features.values,labels.values)#####过采样#####\n",
    "    N_AllTrain_OverSampler=int(featuresArrayOverSampler.shape[0])\n",
    "    print('Shape of OverSampler of AllTrain: '+str(featuresArrayOverSampler.shape))\n",
    "else:\n",
    "    featuresArrayOverSampler=features.values\n",
    "    labelsArrayOverSampler=labels.values\n",
    "    N_AllTrain_OverSampler=int(featuresArrayOverSampler.shape[0])\n",
    "    print('------------Attention: we do not RandomOverSampler---------------')\n",
    "if ConsiderTime:\n",
    "    #####按照年（第6列）月（第5列）日（第4列）时（第3列）排序\n",
    "    print('------------ConsiderTime:  Sorting--------------')\n",
    "    time_temp=featuresArrayOverSampler[:,2]+np.dot(featuresArrayOverSampler[:,3],100)+np.dot(featuresArrayOverSampler[:,4],10000)+np.dot(featuresArrayOverSampler[:,5],1000000)\n",
    "    features_label_time=np.column_stack((featuresArrayOverSampler,labelsArrayOverSampler))\n",
    "    features_label_time=np.column_stack((features_label_time,time_temp))\n",
    "    features_label_time =features_label_time[np.argsort(features_label_time[:,-1])]\n",
    "    labelsArrayOverSampler=features_label_time[:,-2]\n",
    "    featuresArrayOverSampler=features_label_time[:,0:featuresArrayOverSampler.shape[1]]\n",
    "    del features_label_time\n",
    "    #############################先进行过采样，然后再根据时间来排序----结束############################\n",
    "if Rate_ALL>0:\n",
    "    print('------------RandomOverSampler for AllTest--------------')\n",
    "    ros = RandomOverSampler()\n",
    "    featuresArray_test, labelsArray_test = ros.fit_resample(features_test.values,labels_test.values)#####过采样#####\n",
    "    N_AllTest_OverSampler=int(featuresArray_test.shape[0])\n",
    "    labelsArray_test = keras.utils.to_categorical(LabelEncoder().fit_transform(np.array(labelsArray_test)), num_classes=N_CLASS)\n",
    "    print('Shape of OverSampler of AllTest: '+str(featuresArray_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实验T3：只对新的Block进行Transfer Learning，不是和旧的TrainSet混在一起重新学习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------Building DNN model--------------\n",
      "-----------------Build DNN model and start the 1st training!!---------------------\n",
      "--------Spllit train val according to time!---------\n",
      "(29175, 129)\n",
      "(1, 129)\n",
      "------------DNN Training Go! Go! Go!!!!-----------\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 29175 samples, validate on 1 samples\n",
      "Epoch 1/7\n",
      "29175/29175 [==============================] - 6s 215us/step - loss: 2.0881 - accuracy: 0.5393 - top_k_categorical_accuracy: 0.7853 - val_loss: 3.8362 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "Epoch 2/7\n",
      "29175/29175 [==============================] - 6s 198us/step - loss: 1.4507 - accuracy: 0.5980 - top_k_categorical_accuracy: 0.9001 - val_loss: 4.0129 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "Epoch 3/7\n",
      "29175/29175 [==============================] - 5s 177us/step - loss: 1.4077 - accuracy: 0.6024 - top_k_categorical_accuracy: 0.9044 - val_loss: 4.2723 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "Epoch 4/7\n",
      "29175/29175 [==============================] - 5s 176us/step - loss: 1.3834 - accuracy: 0.6056 - top_k_categorical_accuracy: 0.9065 - val_loss: 3.7566 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "Epoch 5/7\n",
      "29175/29175 [==============================] - 6s 195us/step - loss: 1.3647 - accuracy: 0.6085 - top_k_categorical_accuracy: 0.9102 - val_loss: 3.7209 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "Epoch 6/7\n",
      "29175/29175 [==============================] - 6s 199us/step - loss: 1.3472 - accuracy: 0.6087 - top_k_categorical_accuracy: 0.9131 - val_loss: 3.6474 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "Epoch 7/7\n",
      "29175/29175 [==============================] - 6s 197us/step - loss: 1.3290 - accuracy: 0.6151 - top_k_categorical_accuracy: 0.9154 - val_loss: 3.9018 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "[3.9018359184265137, 0.0, 0.0]\n",
      "-----------------Start the loop training!!---------------------\n",
      "i_s=0 , [[0.31893954 1.         1.        ]]\n",
      "i_s=1 , [[0.52976257 1.         1.        ]]\n",
      "i_s=2 , [[1.44761395 0.         1.        ]]\n",
      "i_s=3 , [[0.4390004 1.        1.       ]]\n",
      "i_s=4 , [[0.06993234 1.         1.        ]]\n",
      "i_s=5 , [[0.54340827 1.         1.        ]]\n",
      "i_s=6 , [[0.51864213 1.         1.        ]]\n",
      "i_s=7 , [[0.06094646 1.         1.        ]]\n",
      "i_s=8 , [[3.46417999 0.         1.        ]]\n",
      "i_s=9 , [[0.07773785 1.         1.        ]]\n",
      "i_s=10 , [[0.72365224 1.         1.        ]]\n",
      "i_s=11 , [[1.16282213 0.         1.        ]]\n",
      "i_s=12 , [[2.06921172 0.         1.        ]]\n",
      "i_s=13 , [[0.23412439 1.         1.        ]]\n",
      "i_s=14 , [[1.12244356 1.         1.        ]]\n",
      "i_s=15 , [[0.66388726 1.         1.        ]]\n",
      "i_s=16 , [[2.86234856 0.         1.        ]]\n",
      "i_s=17 , [[0.38076714 1.         1.        ]]\n",
      "i_s=18 , [[0.23127425 1.         1.        ]]\n",
      "i_s=19 , [[0.17204469 1.         1.        ]]\n",
      "i_s=20 , [[0.03248449 1.         1.        ]]\n",
      "i_s=21 , [[5.63660526 0.         0.        ]]\n",
      "i_s=22 , [[0.11065814 1.         1.        ]]\n",
      "i_s=23 , [[0.14039768 1.         1.        ]]\n",
      "i_s=24 , [[2.88421154 0.         1.        ]]\n",
      "i_s=25 , [[0.06159443 1.         1.        ]]\n",
      "i_s=26 , [[0.52573174 1.         1.        ]]\n",
      "i_s=27 , [[0.48307416 1.         1.        ]]\n",
      "i_s=28 , [[0.39824346 1.         1.        ]]\n",
      "i_s=29 , [[4.62420034 0.         0.        ]]\n",
      "i_s=30 , [[0.49272865 1.         1.        ]]\n",
      "i_s=31 , [[2.04252911 0.         1.        ]]\n",
      "i_s=32 , [[0.83216107 0.         1.        ]]\n",
      "i_s=33 , [[0.21562059 1.         1.        ]]\n",
      "i_s=34 , [[1.31680059 0.         1.        ]]\n",
      "i_s=35 , [[1.22443283 0.         1.        ]]\n",
      "i_s=36 , [[3.12085295 0.         0.        ]]\n",
      "i_s=37 , [[0.3064816 1.        1.       ]]\n",
      "i_s=38 , [[1.69812536 0.         1.        ]]\n",
      "i_s=39 , [[1.06801414 1.         1.        ]]\n",
      "i_s=40 , [[0.4408884 1.        1.       ]]\n",
      "i_s=41 , [[0.93971652 1.         1.        ]]\n",
      "i_s=42 , [[3.06031561 0.         0.        ]]\n",
      "i_s=43 , [[0.99737751 1.         1.        ]]\n",
      "i_s=44 , [[0.56905431 1.         1.        ]]\n",
      "i_s=45 , [[0.59126097 1.         1.        ]]\n",
      "i_s=46 , [[1.25796223 1.         1.        ]]\n",
      "i_s=47 , [[3.31186676 0.         1.        ]]\n",
      "i_s=48 , [[0.77970052 1.         1.        ]]\n",
      "i_s=49 , [[0.21998726 1.         1.        ]]\n",
      "i_s=50 , [[1.41500318 0.         1.        ]]\n",
      "i_s=51 , [[3.24626589 0.         0.        ]]\n",
      "i_s=52 , [[0.29786724 1.         1.        ]]\n",
      "i_s=53 , [[0.50686753 1.         1.        ]]\n",
      "i_s=54 , [[0.80171311 1.         1.        ]]\n",
      "i_s=55 , [[0.30926862 1.         1.        ]]\n",
      "i_s=56 , [[0.14850825 1.         1.        ]]\n",
      "i_s=57 , [[0.04684068 1.         1.        ]]\n",
      "i_s=58 , [[1.36174619 0.         1.        ]]\n",
      "i_s=59 , [[0.80047905 1.         1.        ]]\n",
      "i_s=60 , [[0.05358038 1.         1.        ]]\n",
      "i_s=61 , [[0.1945249 1.        1.       ]]\n",
      "i_s=62 , [[0.0967003 1.        1.       ]]\n",
      "i_s=63 , [[0.33298275 1.         1.        ]]\n",
      "i_s=64 , [[1.01368916 1.         1.        ]]\n",
      "i_s=65 , [[1.36858404 0.         1.        ]]\n",
      "i_s=66 , [[0.2655108 1.        1.       ]]\n",
      "i_s=67 , [[1.37074184 0.         1.        ]]\n",
      "i_s=68 , [[4.34284115 0.         1.        ]]\n",
      "i_s=69 , [[2.03905797 0.         1.        ]]\n",
      "i_s=70 , [[0.13883902 1.         1.        ]]\n",
      "i_s=71 , [[0.19899364 1.         1.        ]]\n",
      "i_s=72 , [[2.28124142 0.         1.        ]]\n",
      "i_s=73 , [[1.5549078 0.        1.       ]]\n",
      "i_s=74 , [[0.47458026 1.         1.        ]]\n",
      "i_s=75 , [[5.799335 0.       0.      ]]\n",
      "i_s=76 , [[0.46374077 1.         1.        ]]\n",
      "i_s=77 , [[1.53947735 0.         1.        ]]\n",
      "i_s=78 , [[0.50855577 1.         1.        ]]\n",
      "i_s=79 , [[0.50312042 1.         1.        ]]\n",
      "i_s=80 , [[0.7397244 1.        1.       ]]\n",
      "i_s=81 , [[0.50750566 1.         1.        ]]\n",
      "i_s=82 , [[2.01965499 0.         1.        ]]\n",
      "i_s=83 , [[0.26074368 1.         1.        ]]\n",
      "i_s=84 , [[2.44916177 0.         1.        ]]\n",
      "i_s=85 , [[2.31510401 0.         1.        ]]\n",
      "i_s=86 , [[0.18833892 1.         1.        ]]\n",
      "i_s=87 , [[0.69513077 1.         1.        ]]\n",
      "i_s=88 , [[0.62374103 1.         1.        ]]\n",
      "i_s=89 , [[0.4755736 1.        1.       ]]\n",
      "i_s=90 , [[0.17409509 1.         1.        ]]\n",
      "i_s=91 , [[0.10339934 1.         1.        ]]\n",
      "i_s=92 , [[0.62917686 1.         1.        ]]\n",
      "i_s=93 , [[0.68412745 1.         1.        ]]\n",
      "i_s=94 , [[0.09980597 1.         1.        ]]\n",
      "i_s=95 , [[0.07627429 1.         1.        ]]\n",
      "i_s=96 , [[0.36990133 1.         1.        ]]\n",
      "i_s=97 , [[0.32783717 1.         1.        ]]\n",
      "i_s=98 , [[0.13689649 1.         1.        ]]\n",
      "i_s=99 , [[2.13595963 0.         1.        ]]\n",
      "i_s=100 , [[0.42333254 1.         1.        ]]\n",
      "i_s=101 , [[1.17278647 0.         1.        ]]\n",
      "i_s=102 , [[2.86173582 0.         0.        ]]\n",
      "i_s=103 , [[0.29998168 1.         1.        ]]\n",
      "i_s=104 , [[0.32897797 1.         1.        ]]\n",
      "i_s=105 , [[0.18213333 1.         1.        ]]\n",
      "i_s=106 , [[6.72414637 0.         0.        ]]\n",
      "i_s=107 , [[0.74065447 1.         1.        ]]\n",
      "i_s=108 , [[4.12960529 0.         0.        ]]\n",
      "i_s=109 , [[1.61063933 0.         1.        ]]\n",
      "i_s=110 , [[0.14803453 1.         1.        ]]\n",
      "i_s=111 , [[0.40906686 1.         1.        ]]\n",
      "i_s=112 , [[6.25062227 0.         0.        ]]\n",
      "i_s=113 , [[1.08842123 0.         1.        ]]\n",
      "i_s=114 , [[0.17428759 1.         1.        ]]\n",
      "i_s=115 , [[2.06119919 0.         1.        ]]\n",
      "i_s=116 , [[0.62418836 1.         1.        ]]\n",
      "i_s=117 , [[0.31950343 1.         1.        ]]\n",
      "i_s=118 , [[2.06346416 0.         1.        ]]\n",
      "i_s=119 , [[0.3544642 1.        1.       ]]\n",
      "i_s=120 , [[0.76902717 1.         1.        ]]\n",
      "i_s=121 , [[0.33080021 1.         1.        ]]\n",
      "i_s=122 , [[0.06621478 1.         1.        ]]\n",
      "i_s=123 , [[0.43121096 1.         1.        ]]\n",
      "i_s=124 , [[0.39502418 1.         1.        ]]\n",
      "i_s=125 , [[0.16935344 1.         1.        ]]\n",
      "i_s=126 , [[3.11191368 0.         1.        ]]\n",
      "i_s=127 , [[2.74031496 0.         1.        ]]\n",
      "i_s=128 , [[2.98168993 0.         1.        ]]\n",
      "i_s=129 , [[0.17673497 1.         1.        ]]\n",
      "i_s=130 , [[4.29021549 0.         0.        ]]\n",
      "i_s=131 , [[2.79013491 0.         1.        ]]\n",
      "i_s=132 , [[0.50363547 1.         1.        ]]\n",
      "i_s=133 , [[0.55307221 1.         1.        ]]\n",
      "i_s=134 , [[1.20505095 0.         1.        ]]\n",
      "i_s=135 , [[1.36908484 0.         1.        ]]\n",
      "i_s=136 , [[2.37126064 0.         1.        ]]\n",
      "i_s=137 , [[3.70770788 0.         0.        ]]\n",
      "i_s=138 , [[0.51309621 1.         1.        ]]\n",
      "i_s=139 , [[4.94783258 0.         0.        ]]\n",
      "i_s=140 , [[0.35061479 1.         1.        ]]\n",
      "i_s=141 , [[0.34009397 1.         1.        ]]\n",
      "i_s=142 , [[0.45872635 1.         1.        ]]\n",
      "i_s=143 , [[0.07179461 1.         1.        ]]\n",
      "i_s=144 , [[5.24645376 0.         0.        ]]\n",
      "i_s=145 , [[0.06216704 1.         1.        ]]\n",
      "i_s=146 , [[1.01635599 0.         1.        ]]\n",
      "i_s=147 , [[0.25709212 1.         1.        ]]\n",
      "i_s=148 , [[0.75078976 1.         1.        ]]\n",
      "i_s=149 , [[0.67119443 1.         1.        ]]\n",
      "i_s=150 , [[0.38540539 1.         1.        ]]\n",
      "i_s=151 , [[4.41132927 0.         0.        ]]\n",
      "i_s=152 , [[1.24633288 0.         1.        ]]\n",
      "i_s=153 , [[0.4986214 1.        1.       ]]\n",
      "i_s=154 , [[0.59319437 1.         1.        ]]\n",
      "i_s=155 , [[0.15864907 1.         1.        ]]\n",
      "i_s=156 , [[2.09000754 0.         1.        ]]\n",
      "i_s=157 , [[0.58802736 1.         1.        ]]\n",
      "i_s=158 , [[0.57636714 1.         1.        ]]\n",
      "i_s=159 , [[2.58215189 0.         1.        ]]\n",
      "i_s=160 , [[0.10137276 1.         1.        ]]\n",
      "i_s=161 , [[3.71204185 0.         1.        ]]\n",
      "i_s=162 , [[1.71720767 0.         1.        ]]\n",
      "i_s=163 , [[0.54363531 1.         1.        ]]\n",
      "i_s=164 , [[3.38286042 0.         0.        ]]\n",
      "i_s=165 , [[0.40159294 1.         1.        ]]\n",
      "i_s=166 , [[3.24141979 0.         0.        ]]\n",
      "i_s=167 , [[0.39628395 1.         1.        ]]\n",
      "i_s=168 , [[0.5324508 1.        1.       ]]\n",
      "i_s=169 , [[0.70766258 1.         1.        ]]\n",
      "i_s=170 , [[1.73434591 0.         1.        ]]\n",
      "i_s=171 , [[0.52040571 1.         1.        ]]\n",
      "i_s=172 , [[5.02750206 0.         0.        ]]\n",
      "i_s=173 , [[2.70606232 0.         1.        ]]\n",
      "i_s=174 , [[2.64182496 0.         1.        ]]\n",
      "i_s=175 , [[0.08196658 1.         1.        ]]\n",
      "i_s=176 , [[0.73580539 1.         1.        ]]\n",
      "i_s=177 , [[1.1906935 1.        1.       ]]\n",
      "i_s=178 , [[5.87483501 0.         0.        ]]\n",
      "i_s=179 , [[0.54330754 1.         1.        ]]\n",
      "i_s=180 , [[0.52859855 1.         1.        ]]\n",
      "i_s=181 , [[0.36886501 1.         1.        ]]\n",
      "i_s=182 , [[0.97097808 1.         1.        ]]\n",
      "i_s=183 , [[0.80780005 1.         1.        ]]\n",
      "i_s=184 , [[1.62598789 0.         1.        ]]\n",
      "i_s=185 , [[0.84911406 1.         1.        ]]\n",
      "i_s=186 , [[1.96773636 0.         1.        ]]\n",
      "i_s=187 , [[0.23067926 1.         1.        ]]\n",
      "i_s=188 , [[2.13892174 0.         1.        ]]\n",
      "i_s=189 , [[4.79558086 0.         0.        ]]\n",
      "i_s=190 , [[0.68953592 1.         1.        ]]\n",
      "i_s=191 , [[0.34713489 1.         1.        ]]\n",
      "i_s=192 , [[0.41741276 1.         1.        ]]\n",
      "i_s=193 , [[0.21742262 1.         1.        ]]\n",
      "i_s=194 , [[1.19968402 1.         1.        ]]\n",
      "i_s=195 , [[3.0194931 0.        1.       ]]\n",
      "i_s=196 , [[1.98918915 0.         1.        ]]\n",
      "i_s=197 , [[0.33794081 1.         1.        ]]\n",
      "i_s=198 , [[0.08071849 1.         1.        ]]\n",
      "i_s=199 , [[0.18055017 1.         1.        ]]\n",
      "i_s=200 , [[0.24125214 1.         1.        ]]\n",
      "i_s=201 , [[2.83214068 0.         1.        ]]\n",
      "i_s=202 , [[1.73276949 0.         1.        ]]\n",
      "i_s=203 , [[1.05986536 0.         1.        ]]\n",
      "i_s=204 , [[2.49629998 0.         1.        ]]\n",
      "i_s=205 , [[0.14650247 1.         1.        ]]\n",
      "i_s=206 , [[0.37008727 1.         1.        ]]\n",
      "i_s=207 , [[1.97230482 0.         1.        ]]\n",
      "i_s=208 , [[3.01176357 0.         1.        ]]\n",
      "i_s=209 , [[0.32249632 1.         1.        ]]\n",
      "i_s=210 , [[2.49194622 0.         1.        ]]\n",
      "i_s=211 , [[2.46094465 0.         1.        ]]\n",
      "i_s=212 , [[0.435065 1.       1.      ]]\n",
      "i_s=213 , [[0.53442258 1.         1.        ]]\n",
      "i_s=214 , [[3.25590158 0.         1.        ]]\n",
      "i_s=215 , [[0.34025556 1.         1.        ]]\n",
      "i_s=216 , [[1.5922426 0.        1.       ]]\n",
      "i_s=217 , [[0.26995048 1.         1.        ]]\n",
      "i_s=218 , [[0.12312574 1.         1.        ]]\n",
      "i_s=219 , [[0.64873469 1.         1.        ]]\n",
      "i_s=220 , [[2.74014258 0.         1.        ]]\n",
      "i_s=221 , [[5.05539942 0.         1.        ]]\n",
      "i_s=222 , [[1.43164706 0.         1.        ]]\n",
      "i_s=223 , [[2.04115438 0.         1.        ]]\n",
      "i_s=224 , [[3.03223467 0.         1.        ]]\n",
      "i_s=225 , [[0.42134225 1.         1.        ]]\n",
      "i_s=226 , [[2.19563627 0.         1.        ]]\n",
      "i_s=227 , [[0.94456756 1.         1.        ]]\n",
      "i_s=228 , [[3.43363047 0.         0.        ]]\n",
      "i_s=229 , [[2.69927502 0.         1.        ]]\n",
      "i_s=230 , [[5.92822742 0.         0.        ]]\n",
      "i_s=231 , [[1.13358569 0.         1.        ]]\n",
      "i_s=232 , [[2.45961666 0.         1.        ]]\n",
      "i_s=233 , [[3.31571031 0.         1.        ]]\n",
      "i_s=234 , [[2.01363277 0.         1.        ]]\n",
      "i_s=235 , [[0.50185198 1.         1.        ]]\n",
      "i_s=236 , [[0.09117714 1.         1.        ]]\n",
      "i_s=237 , [[0.60290176 1.         1.        ]]\n",
      "i_s=238 , [[0.19444206 1.         1.        ]]\n",
      "i_s=239 , [[1.64656699 0.         1.        ]]\n",
      "i_s=240 , [[1.00719833 1.         1.        ]]\n",
      "i_s=241 , [[0.46806917 1.         1.        ]]\n",
      "i_s=242 , [[1.62207723 0.         1.        ]]\n",
      "i_s=243 , [[0.46321464 1.         1.        ]]\n",
      "i_s=244 , [[3.40819073 0.         1.        ]]\n",
      "i_s=245 , [[0.63978916 1.         1.        ]]\n",
      "i_s=246 , [[3.09981203 0.         1.        ]]\n",
      "i_s=247 , [[0.58686376 1.         1.        ]]\n",
      "i_s=248 , [[1.0729084 0.        1.       ]]\n",
      "i_s=249 , [[1.57256031 0.         1.        ]]\n",
      "i_s=250 , [[2.21770811 0.         1.        ]]\n",
      "i_s=251 , [[0.66857052 1.         1.        ]]\n",
      "i_s=252 , [[0.30487448 1.         1.        ]]\n",
      "i_s=253 , [[0.31546974 1.         1.        ]]\n",
      "i_s=254 , [[2.30455518 0.         1.        ]]\n",
      "i_s=255 , [[1.09633148 0.         1.        ]]\n",
      "i_s=256 , [[12.33237648  0.          0.        ]]\n",
      "i_s=257 , [[0.69292378 1.         1.        ]]\n",
      "i_s=258 , [[0.63404369 1.         1.        ]]\n",
      "i_s=259 , [[3.61005831 0.         0.        ]]\n",
      "i_s=260 , [[2.22625232 0.         1.        ]]\n",
      "i_s=261 , [[0.7067951 1.        1.       ]]\n",
      "i_s=262 , [[0.4196623 1.        1.       ]]\n",
      "i_s=263 , [[0.03849496 1.         1.        ]]\n",
      "i_s=264 , [[1.78746998 0.         1.        ]]\n",
      "i_s=265 , [[0.95637017 1.         1.        ]]\n",
      "i_s=266 , [[3.97854829 0.         0.        ]]\n",
      "i_s=267 , [[2.7274518 0.        1.       ]]\n",
      "i_s=268 , [[2.7515316 0.        1.       ]]\n",
      "i_s=269 , [[0.30340567 1.         1.        ]]\n",
      "i_s=270 , [[0.47391632 1.         1.        ]]\n",
      "i_s=271 , [[0.94301492 0.         1.        ]]\n",
      "i_s=272 , [[2.22221565 0.         1.        ]]\n",
      "i_s=273 , [[0.31465575 1.         1.        ]]\n",
      "i_s=274 , [[2.50579095 0.         1.        ]]\n",
      "i_s=275 , [[0.34880731 1.         1.        ]]\n",
      "i_s=276 , [[1.36663449 0.         1.        ]]\n",
      "i_s=277 , [[2.08466029 0.         1.        ]]\n",
      "i_s=278 , [[0.50013226 1.         1.        ]]\n",
      "i_s=279 , [[0.82393038 1.         1.        ]]\n",
      "i_s=280 , [[0.80383861 1.         1.        ]]\n",
      "i_s=281 , [[0.43996015 1.         1.        ]]\n",
      "i_s=282 , [[0.21836215 1.         1.        ]]\n",
      "i_s=283 , [[0.39900261 1.         1.        ]]\n",
      "i_s=284 , [[2.29279828 0.         1.        ]]\n",
      "i_s=285 , [[0.03803568 1.         1.        ]]\n",
      "i_s=286 , [[0.74104315 1.         1.        ]]\n",
      "i_s=287 , [[0.43604934 1.         1.        ]]\n",
      "i_s=288 , [[4.03045559 0.         0.        ]]\n",
      "i_s=289 , [[1.82341444 0.         1.        ]]\n",
      "i_s=290 , [[1.10705423 0.         1.        ]]\n",
      "i_s=291 , [[3.91745448 0.         1.        ]]\n",
      "i_s=292 , [[1.42642498 1.         1.        ]]\n",
      "i_s=293 , [[0.23128106 1.         1.        ]]\n",
      "i_s=294 , [[0.43422762 1.         1.        ]]\n",
      "i_s=295 , [[5.76130819 0.         0.        ]]\n",
      "i_s=296 , [[1.58135295 0.         1.        ]]\n",
      "i_s=297 , [[0.54269034 1.         1.        ]]\n",
      "i_s=298 , [[3.24330997 0.         1.        ]]\n",
      "i_s=299 , [[0.6399011 1.        1.       ]]\n",
      "i_s=300 , [[0.32601923 1.         1.        ]]\n",
      "i_s=301 , [[0.1238838 1.        1.       ]]\n",
      "i_s=302 , [[0.57537341 1.         1.        ]]\n",
      "i_s=303 , [[1.10669518 1.         1.        ]]\n",
      "i_s=304 , [[0.76023954 1.         1.        ]]\n",
      "i_s=305 , [[0.53504956 1.         1.        ]]\n",
      "i_s=306 , [[0.09835342 1.         1.        ]]\n",
      "i_s=307 , [[1.82795262 0.         1.        ]]\n",
      "i_s=308 , [[4.36480999 0.         0.        ]]\n",
      "i_s=309 , [[0.15522897 1.         1.        ]]\n",
      "i_s=310 , [[2.66475725 0.         1.        ]]\n",
      "i_s=311 , [[1.44392526 0.         1.        ]]\n",
      "i_s=312 , [[0.22447762 1.         1.        ]]\n",
      "i_s=313 , [[1.52440917 0.         1.        ]]\n",
      "i_s=314 , [[0.69383264 1.         1.        ]]\n",
      "i_s=315 , [[1.01629329 1.         1.        ]]\n",
      "i_s=316 , [[0.36410955 1.         1.        ]]\n",
      "i_s=317 , [[1.53837681 0.         1.        ]]\n",
      "i_s=318 , [[3.73528004 0.         1.        ]]\n",
      "i_s=319 , [[1.19958758 0.         1.        ]]\n",
      "i_s=320 , [[4.81844234 0.         0.        ]]\n",
      "i_s=321 , [[1.25125098 0.         1.        ]]\n",
      "i_s=322 , [[2.93542314 0.         0.        ]]\n",
      "i_s=323 , [[0.23273921 1.         1.        ]]\n",
      "i_s=324 , [[0.28238225 1.         1.        ]]\n",
      "i_s=325 , [[6.02556801 0.         0.        ]]\n",
      "i_s=326 , [[1.28419566 0.         1.        ]]\n",
      "i_s=327 , [[0.11890679 1.         1.        ]]\n",
      "i_s=328 , [[3.78915024 0.         0.        ]]\n",
      "i_s=329 , [[0.65376812 1.         1.        ]]\n",
      "i_s=330 , [[0.04957046 1.         1.        ]]\n",
      "i_s=331 , [[0.46911401 1.         1.        ]]\n",
      "i_s=332 , [[0.8757993 1.        1.       ]]\n",
      "i_s=333 , [[0.62639916 1.         1.        ]]\n",
      "i_s=334 , [[0.49711245 1.         1.        ]]\n",
      "i_s=335 , [[0.10510229 1.         1.        ]]\n",
      "i_s=336 , [[0.28304222 1.         1.        ]]\n",
      "i_s=337 , [[0.16660981 1.         1.        ]]\n",
      "i_s=338 , [[0.41479662 1.         1.        ]]\n",
      "i_s=339 , [[0.97092509 1.         1.        ]]\n",
      "i_s=340 , [[1.79285705 0.         1.        ]]\n",
      "i_s=341 , [[0.67928523 1.         1.        ]]\n",
      "i_s=342 , [[0.590051 1.       1.      ]]\n",
      "i_s=343 , [[0.37089819 1.         1.        ]]\n",
      "i_s=344 , [[1.18103576 0.         1.        ]]\n",
      "i_s=345 , [[0.37954816 1.         1.        ]]\n",
      "i_s=346 , [[0.33315551 1.         1.        ]]\n",
      "i_s=347 , [[0.29032576 1.         1.        ]]\n",
      "i_s=348 , [[0.17821306 1.         1.        ]]\n",
      "i_s=349 , [[0.28043479 1.         1.        ]]\n",
      "i_s=350 , [[2.52198696 0.         1.        ]]\n",
      "i_s=351 , [[0.25017369 1.         1.        ]]\n",
      "i_s=352 , [[0.11680154 1.         1.        ]]\n",
      "i_s=353 , [[0.180958 1.       1.      ]]\n",
      "i_s=354 , [[0.94133812 1.         1.        ]]\n",
      "i_s=355 , [[2.65678978 0.         1.        ]]\n",
      "i_s=356 , [[0.66382486 1.         1.        ]]\n",
      "i_s=357 , [[0.39569682 1.         1.        ]]\n",
      "i_s=358 , [[0.1950897 1.        1.       ]]\n",
      "i_s=359 , [[1.80736256 0.         1.        ]]\n",
      "i_s=360 , [[0.05816066 1.         1.        ]]\n",
      "i_s=361 , [[0.76711839 1.         1.        ]]\n",
      "i_s=362 , [[3.19142938 0.         0.        ]]\n",
      "i_s=363 , [[6.8039875 0.        0.       ]]\n",
      "i_s=364 , [[1.21372616 1.         1.        ]]\n",
      "i_s=365 , [[6.82741499 0.         0.        ]]\n",
      "i_s=366 , [[0.25769815 1.         1.        ]]\n",
      "i_s=367 , [[0.07256497 1.         1.        ]]\n",
      "i_s=368 , [[1.85827482 0.         1.        ]]\n",
      "i_s=369 , [[4.37500572 0.         0.        ]]\n",
      "i_s=370 , [[0.55621397 1.         1.        ]]\n",
      "i_s=371 , [[0.22792242 1.         1.        ]]\n",
      "i_s=372 , [[0.24781364 1.         1.        ]]\n",
      "i_s=373 , [[0.09182942 1.         1.        ]]\n",
      "i_s=374 , [[4.54565287 0.         0.        ]]\n",
      "i_s=375 , [[0.35280064 1.         1.        ]]\n",
      "i_s=376 , [[0.22309491 1.         1.        ]]\n",
      "i_s=377 , [[4.11981106 0.         1.        ]]\n",
      "i_s=378 , [[0.03923409 1.         1.        ]]\n",
      "i_s=379 , [[0.40292647 1.         1.        ]]\n",
      "i_s=380 , [[1.10784698 0.         1.        ]]\n",
      "i_s=381 , [[1.54084921 0.         1.        ]]\n",
      "i_s=382 , [[2.19182277 0.         1.        ]]\n",
      "i_s=383 , [[1.46846771 0.         1.        ]]\n",
      "i_s=384 , [[0.65367031 1.         1.        ]]\n",
      "i_s=385 , [[0.21441771 1.         1.        ]]\n",
      "i_s=386 , [[0.60126394 1.         1.        ]]\n",
      "i_s=387 , [[0.25114548 1.         1.        ]]\n",
      "i_s=388 , [[2.28684092 0.         1.        ]]\n",
      "i_s=389 , [[1.47459078 0.         1.        ]]\n",
      "i_s=390 , [[0.75107419 1.         1.        ]]\n",
      "i_s=391 , [[0.49058312 1.         1.        ]]\n",
      "i_s=392 , [[2.38245177 0.         1.        ]]\n",
      "i_s=393 , [[1.4889431 0.        1.       ]]\n",
      "i_s=394 , [[0.23089342 1.         1.        ]]\n",
      "i_s=395 , [[0.74240744 1.         1.        ]]\n",
      "i_s=396 , [[0.41348174 1.         1.        ]]\n",
      "i_s=397 , [[0.21968059 1.         1.        ]]\n",
      "i_s=398 , [[2.449646 0.       1.      ]]\n",
      "i_s=399 , [[0.11827596 1.         1.        ]]\n",
      "i_s=400 , [[4.33143139 0.         0.        ]]\n",
      "i_s=401 , [[0.26132473 1.         1.        ]]\n",
      "i_s=402 , [[0.34173584 1.         1.        ]]\n",
      "i_s=403 , [[0.55630708 1.         1.        ]]\n",
      "i_s=404 , [[0.24607837 1.         1.        ]]\n",
      "i_s=405 , [[0.48270845 1.         1.        ]]\n",
      "i_s=406 , [[0.74743104 1.         1.        ]]\n",
      "i_s=407 , [[2.28530645 0.         1.        ]]\n",
      "i_s=408 , [[0.62268293 1.         1.        ]]\n",
      "i_s=409 , [[0.66699219 1.         1.        ]]\n",
      "i_s=410 , [[2.03099775 0.         1.        ]]\n",
      "i_s=411 , [[1.33359694 1.         1.        ]]\n",
      "i_s=412 , [[0.87491274 0.         1.        ]]\n",
      "i_s=413 , [[3.05011439 0.         1.        ]]\n",
      "i_s=414 , [[0.46406651 1.         1.        ]]\n",
      "i_s=415 , [[1.28726602 0.         1.        ]]\n",
      "i_s=416 , [[0.7941432 1.        1.       ]]\n",
      "i_s=417 , [[0.05255989 1.         1.        ]]\n",
      "i_s=418 , [[0.60705 1.      1.     ]]\n",
      "i_s=419 , [[0.1575447 1.        1.       ]]\n",
      "i_s=420 , [[8.88403893 0.         0.        ]]\n",
      "i_s=421 , [[0.58718038 1.         1.        ]]\n",
      "i_s=422 , [[0.25956082 1.         1.        ]]\n",
      "i_s=423 , [[0.09724375 1.         1.        ]]\n",
      "i_s=424 , [[0.1043495 1.        1.       ]]\n",
      "i_s=425 , [[0.13401937 1.         1.        ]]\n",
      "i_s=426 , [[0.15597951 1.         1.        ]]\n",
      "i_s=427 , [[0.42074627 1.         1.        ]]\n",
      "i_s=428 , [[0.10552888 1.         1.        ]]\n",
      "i_s=429 , [[0.2286175 1.        1.       ]]\n",
      "i_s=430 , [[1.89047098 0.         1.        ]]\n",
      "i_s=431 , [[0.97211802 1.         1.        ]]\n",
      "i_s=432 , [[0.06336632 1.         1.        ]]\n",
      "i_s=433 , [[0.14379323 1.         1.        ]]\n",
      "i_s=434 , [[3.36534095 0.         1.        ]]\n",
      "i_s=435 , [[2.53453875 0.         1.        ]]\n",
      "i_s=436 , [[0.19516718 1.         1.        ]]\n",
      "i_s=437 , [[0.45289364 1.         1.        ]]\n",
      "i_s=438 , [[4.32902336 0.         0.        ]]\n",
      "i_s=439 , [[2.11086893 0.         1.        ]]\n",
      "i_s=440 , [[0.35758403 1.         1.        ]]\n",
      "i_s=441 , [[0.81983256 1.         1.        ]]\n",
      "i_s=442 , [[1.03831768 1.         1.        ]]\n",
      "i_s=443 , [[0.68186736 1.         1.        ]]\n",
      "i_s=444 , [[0.8098191 1.        1.       ]]\n",
      "i_s=445 , [[1.2655102 0.        1.       ]]\n",
      "i_s=446 , [[0.41069773 1.         1.        ]]\n",
      "i_s=447 , [[1.14408123 1.         1.        ]]\n",
      "i_s=448 , [[0.20130602 1.         1.        ]]\n",
      "i_s=449 , [[0.40957239 1.         1.        ]]\n",
      "i_s=450 , [[0.39843789 1.         1.        ]]\n",
      "i_s=451 , [[0.36770049 1.         1.        ]]\n",
      "i_s=452 , [[3.42120743 0.         1.        ]]\n",
      "i_s=453 , [[2.47135401 0.         1.        ]]\n",
      "i_s=454 , [[1.01781178 1.         1.        ]]\n",
      "i_s=455 , [[2.30360174 0.         1.        ]]\n",
      "i_s=456 , [[0.379298 1.       1.      ]]\n",
      "i_s=457 , [[1.84113026 0.         1.        ]]\n",
      "i_s=458 , [[0.24747798 1.         1.        ]]\n",
      "i_s=459 , [[2.5678668 0.        1.       ]]\n",
      "i_s=460 , [[2.7888639 0.        1.       ]]\n",
      "i_s=461 , [[3.55517125 0.         0.        ]]\n",
      "i_s=462 , [[2.40945768 0.         1.        ]]\n",
      "i_s=463 , [[0.99023956 1.         1.        ]]\n",
      "i_s=464 , [[1.47862506 0.         1.        ]]\n",
      "i_s=465 , [[0.73101646 1.         1.        ]]\n",
      "i_s=466 , [[1.35843503 0.         1.        ]]\n",
      "i_s=467 , [[0.29275209 1.         1.        ]]\n",
      "i_s=468 , [[0.05678112 1.         1.        ]]\n",
      "i_s=469 , [[0.40365961 1.         1.        ]]\n",
      "i_s=470 , [[1.83395588 0.         1.        ]]\n",
      "i_s=471 , [[0.32047191 1.         1.        ]]\n",
      "i_s=472 , [[2.43610382 0.         1.        ]]\n",
      "i_s=473 , [[1.01736736 1.         1.        ]]\n",
      "i_s=474 , [[0.60362309 1.         1.        ]]\n",
      "i_s=475 , [[0.46800774 1.         1.        ]]\n",
      "i_s=476 , [[1.09584069 1.         1.        ]]\n",
      "i_s=477 , [[1.20817411 1.         1.        ]]\n",
      "i_s=478 , [[3.21708632 0.         0.        ]]\n",
      "i_s=479 , [[0.40607503 1.         1.        ]]\n",
      "i_s=480 , [[0.66097409 1.         1.        ]]\n",
      "i_s=481 , [[6.32695532 0.         0.        ]]\n",
      "i_s=482 , [[0.98558789 0.         1.        ]]\n",
      "i_s=483 , [[0.38298863 1.         1.        ]]\n",
      "i_s=484 , [[0.77344221 1.         1.        ]]\n",
      "i_s=485 , [[1.19868398 1.         1.        ]]\n",
      "i_s=486 , [[0.93738234 1.         1.        ]]\n",
      "i_s=487 , [[0.6362409 1.        1.       ]]\n",
      "i_s=488 , [[0.93409133 1.         1.        ]]\n",
      "i_s=489 , [[2.59991074 0.         1.        ]]\n",
      "i_s=490 , [[3.26376724 0.         0.        ]]\n",
      "i_s=491 , [[0.5029608 1.        1.       ]]\n",
      "i_s=492 , [[0.49730968 1.         1.        ]]\n",
      "i_s=493 , [[0.59040064 1.         1.        ]]\n",
      "i_s=494 , [[0.2719624 1.        1.       ]]\n",
      "i_s=495 , [[2.05917287 0.         1.        ]]\n",
      "i_s=496 , [[6.87309027 0.         0.        ]]\n",
      "i_s=497 , [[0.35967246 1.         1.        ]]\n",
      "i_s=498 , [[0.51695037 1.         1.        ]]\n",
      "i_s=499 , [[9.21505737 0.         0.        ]]\n",
      "i_s=500 , [[0.1945139 1.        1.       ]]\n",
      "i_s=501 , [[1.40427554 0.         1.        ]]\n",
      "i_s=502 , [[0.40717342 1.         1.        ]]\n",
      "i_s=503 , [[2.10222435 0.         1.        ]]\n",
      "i_s=504 , [[2.35366869 0.         1.        ]]\n",
      "i_s=505 , [[2.90745401 0.         1.        ]]\n",
      "i_s=506 , [[0.94187355 1.         1.        ]]\n",
      "i_s=507 , [[4.67596197 0.         0.        ]]\n",
      "i_s=508 , [[0.80809504 1.         1.        ]]\n",
      "i_s=509 , [[0.20442612 1.         1.        ]]\n",
      "i_s=510 , [[1.34244096 0.         1.        ]]\n",
      "i_s=511 , [[2.85200453 0.         1.        ]]\n",
      "i_s=512 , [[0.69081509 1.         1.        ]]\n",
      "i_s=513 , [[0.7045303 1.        1.       ]]\n",
      "i_s=514 , [[0.40087393 1.         1.        ]]\n",
      "i_s=515 , [[0.76229399 1.         1.        ]]\n",
      "i_s=516 , [[0.36059129 1.         1.        ]]\n",
      "i_s=517 , [[0.04497109 1.         1.        ]]\n",
      "i_s=518 , [[1.52847898 0.         1.        ]]\n",
      "i_s=519 , [[1.53037715 0.         1.        ]]\n",
      "i_s=520 , [[0.20166603 1.         1.        ]]\n",
      "i_s=521 , [[0.30359522 1.         1.        ]]\n",
      "i_s=522 , [[1.45258069 0.         1.        ]]\n",
      "i_s=523 , [[0.18529958 1.         1.        ]]\n",
      "i_s=524 , [[0.31026149 1.         1.        ]]\n",
      "i_s=525 , [[1.9547832 0.        1.       ]]\n",
      "i_s=526 , [[1.58303773 0.         1.        ]]\n",
      "i_s=527 , [[0.43381801 1.         1.        ]]\n",
      "i_s=528 , [[0.09174643 1.         1.        ]]\n",
      "i_s=529 , [[0.63740194 1.         1.        ]]\n",
      "i_s=530 , [[4.68443966 0.         0.        ]]\n",
      "i_s=531 , [[2.59665728 0.         1.        ]]\n",
      "i_s=532 , [[0.93684512 0.         1.        ]]\n",
      "i_s=533 , [[3.04264593 0.         1.        ]]\n",
      "i_s=534 , [[0.63618881 1.         1.        ]]\n",
      "i_s=535 , [[0.36462542 1.         1.        ]]\n",
      "i_s=536 , [[0.56586361 1.         1.        ]]\n",
      "i_s=537 , [[0.18328632 1.         1.        ]]\n",
      "i_s=538 , [[1.60613132 0.         1.        ]]\n",
      "i_s=539 , [[0.94436753 1.         1.        ]]\n",
      "i_s=540 , [[0.38803273 1.         1.        ]]\n",
      "i_s=541 , [[2.66392326 0.         1.        ]]\n",
      "i_s=542 , [[2.6546855 0.        1.       ]]\n",
      "i_s=543 , [[2.93453288 0.         1.        ]]\n",
      "i_s=544 , [[0.20679387 1.         1.        ]]\n",
      "i_s=545 , [[0.07279521 1.         1.        ]]\n",
      "i_s=546 , [[0.87388581 1.         1.        ]]\n",
      "i_s=547 , [[1.09003353 1.         1.        ]]\n",
      "i_s=548 , [[0.72847086 1.         1.        ]]\n",
      "i_s=549 , [[2.40275097 0.         1.        ]]\n",
      "i_s=550 , [[2.96802282 0.         1.        ]]\n",
      "i_s=551 , [[5.11154079 0.         0.        ]]\n",
      "i_s=552 , [[0.28037632 1.         1.        ]]\n",
      "i_s=553 , [[1.96145201 0.         1.        ]]\n",
      "i_s=554 , [[0.94350606 1.         1.        ]]\n",
      "i_s=555 , [[1.91574121 0.         1.        ]]\n",
      "i_s=556 , [[2.43459702 0.         1.        ]]\n",
      "i_s=557 , [[0.97205627 1.         1.        ]]\n",
      "i_s=558 , [[0.75420088 1.         1.        ]]\n",
      "i_s=559 , [[2.28779554 0.         1.        ]]\n",
      "i_s=560 , [[0.20614581 1.         1.        ]]\n",
      "i_s=561 , [[2.82498622 0.         1.        ]]\n",
      "i_s=562 , [[0.17983319 1.         1.        ]]\n",
      "i_s=563 , [[0.12979703 1.         1.        ]]\n",
      "i_s=564 , [[0.94935024 1.         1.        ]]\n",
      "i_s=565 , [[6.66179609 0.         0.        ]]\n",
      "i_s=566 , [[0.22154152 1.         1.        ]]\n",
      "i_s=567 , [[0.24216637 1.         1.        ]]\n",
      "i_s=568 , [[1.22171199 1.         1.        ]]\n",
      "i_s=569 , [[1.26130736 0.         1.        ]]\n",
      "i_s=570 , [[0.21433768 1.         1.        ]]\n",
      "i_s=571 , [[0.35013184 1.         1.        ]]\n",
      "i_s=572 , [[0.18721408 1.         1.        ]]\n",
      "i_s=573 , [[4.00229549 0.         1.        ]]\n",
      "i_s=574 , [[0.14667955 1.         1.        ]]\n",
      "i_s=575 , [[0.90263844 1.         1.        ]]\n",
      "i_s=576 , [[3.04925823 0.         1.        ]]\n",
      "i_s=577 , [[0.4380503 1.        1.       ]]\n",
      "i_s=578 , [[0.52326411 1.         1.        ]]\n",
      "i_s=579 , [[0.75318223 1.         1.        ]]\n",
      "i_s=580 , [[0.10174098 1.         1.        ]]\n",
      "i_s=581 , [[0.39394844 1.         1.        ]]\n",
      "i_s=582 , [[0.05311532 1.         1.        ]]\n",
      "i_s=583 , [[5.42922115 0.         0.        ]]\n",
      "i_s=584 , [[0.66975117 1.         1.        ]]\n",
      "i_s=585 , [[0.43883124 1.         1.        ]]\n",
      "i_s=586 , [[2.23337865 0.         1.        ]]\n",
      "i_s=587 , [[2.28072667 0.         1.        ]]\n",
      "i_s=588 , [[2.45643616 0.         1.        ]]\n",
      "i_s=589 , [[0.60328734 1.         1.        ]]\n",
      "i_s=590 , [[6.10132074 0.         0.        ]]\n",
      "i_s=591 , [[4.66661501 0.         0.        ]]\n",
      "i_s=592 , [[4.35437202 0.         0.        ]]\n",
      "i_s=593 , [[3.2984314 0.        1.       ]]\n",
      "i_s=594 , [[0.96079969 1.         1.        ]]\n",
      "i_s=595 , [[0.1706668 1.        1.       ]]\n",
      "i_s=596 , [[0.20364857 1.         1.        ]]\n",
      "i_s=597 , [[0.86041534 1.         1.        ]]\n",
      "i_s=598 , [[0.88999939 1.         1.        ]]\n",
      "i_s=599 , [[2.54586124 0.         0.        ]]\n",
      "i_s=600 , [[1.51416445 0.         1.        ]]\n",
      "i_s=601 , [[0.23888978 1.         1.        ]]\n",
      "i_s=602 , [[0.59221065 1.         1.        ]]\n",
      "i_s=603 , [[0.5224303 1.        1.       ]]\n",
      "i_s=604 , [[3.11440086 0.         1.        ]]\n",
      "i_s=605 , [[0.40507069 1.         1.        ]]\n",
      "i_s=606 , [[1.3727988 0.        1.       ]]\n",
      "i_s=607 , [[0.91228014 1.         1.        ]]\n",
      "i_s=608 , [[4.05460501 0.         1.        ]]\n",
      "i_s=609 , [[1.02509654 1.         1.        ]]\n",
      "i_s=610 , [[0.79398799 1.         1.        ]]\n",
      "i_s=611 , [[0.38358179 1.         1.        ]]\n",
      "i_s=612 , [[4.19951057 0.         1.        ]]\n",
      "i_s=613 , [[4.00985289 0.         0.        ]]\n",
      "i_s=614 , [[7.18491268 0.         0.        ]]\n",
      "i_s=615 , [[1.9913435 0.        1.       ]]\n",
      "i_s=616 , [[0.28323129 1.         1.        ]]\n",
      "i_s=617 , [[1.34635401 0.         1.        ]]\n",
      "i_s=618 , [[1.39467907 0.         1.        ]]\n",
      "i_s=619 , [[1.01812506 1.         1.        ]]\n",
      "i_s=620 , [[0.20914528 1.         1.        ]]\n",
      "i_s=621 , [[0.77646077 1.         1.        ]]\n",
      "i_s=622 , [[0.63890409 1.         1.        ]]\n",
      "i_s=623 , [[0.22513551 1.         1.        ]]\n",
      "i_s=624 , [[0.80125868 1.         1.        ]]\n",
      "i_s=625 , [[0.41184118 1.         1.        ]]\n",
      "i_s=626 , [[2.08428741 0.         1.        ]]\n",
      "i_s=627 , [[2.27717018 0.         1.        ]]\n",
      "i_s=628 , [[0.3072027 1.        1.       ]]\n",
      "i_s=629 , [[0.37244058 1.         1.        ]]\n",
      "i_s=630 , [[2.14764762 0.         1.        ]]\n",
      "i_s=631 , [[0.26986903 1.         1.        ]]\n",
      "i_s=632 , [[0.34852451 1.         1.        ]]\n",
      "i_s=633 , [[0.26533976 1.         1.        ]]\n",
      "i_s=634 , [[0.2147989 1.        1.       ]]\n",
      "i_s=635 , [[2.86851144 0.         0.        ]]\n",
      "i_s=636 , [[0.32057974 1.         1.        ]]\n",
      "i_s=637 , [[3.52590489 0.         1.        ]]\n",
      "i_s=638 , [[4.53778315 0.         1.        ]]\n",
      "i_s=639 , [[0.77019912 1.         1.        ]]\n",
      "i_s=640 , [[0.75109661 1.         1.        ]]\n",
      "i_s=641 , [[1.6268363 0.        1.       ]]\n",
      "i_s=642 , [[1.19596422 0.         1.        ]]\n",
      "i_s=643 , [[1.01404095 1.         1.        ]]\n",
      "i_s=644 , [[0.65778345 1.         1.        ]]\n",
      "i_s=645 , [[0.76389474 1.         1.        ]]\n",
      "i_s=646 , [[5.952847 0.       0.      ]]\n",
      "i_s=647 , [[0.87872523 1.         1.        ]]\n",
      "i_s=648 , [[0.22898127 1.         1.        ]]\n",
      "i_s=649 , [[0.51728022 1.         1.        ]]\n",
      "i_s=650 , [[2.6026566 0.        0.       ]]\n",
      "i_s=651 , [[1.9572109 0.        1.       ]]\n",
      "i_s=652 , [[2.78590441 0.         0.        ]]\n",
      "i_s=653 , [[5.64539433 0.         0.        ]]\n",
      "i_s=654 , [[1.41003823 0.         1.        ]]\n",
      "i_s=655 , [[3.20802903 0.         1.        ]]\n",
      "i_s=656 , [[0.16605221 1.         1.        ]]\n",
      "i_s=657 , [[3.0853467 0.        0.       ]]\n",
      "i_s=658 , [[1.07565784 1.         1.        ]]\n",
      "i_s=659 , [[4.28444147 0.         0.        ]]\n",
      "i_s=660 , [[1.25072789 0.         1.        ]]\n",
      "i_s=661 , [[3.63603234 0.         1.        ]]\n",
      "i_s=662 , [[0.89751202 1.         1.        ]]\n",
      "i_s=663 , [[0.28078935 1.         1.        ]]\n",
      "i_s=664 , [[0.11675826 1.         1.        ]]\n",
      "i_s=665 , [[3.52230167 0.         0.        ]]\n",
      "i_s=666 , [[3.50542498 0.         0.        ]]\n",
      "i_s=667 , [[0.11668804 1.         1.        ]]\n",
      "i_s=668 , [[0.0797621 1.        1.       ]]\n",
      "i_s=669 , [[2.420403 0.       1.      ]]\n",
      "i_s=670 , [[0.10348835 1.         1.        ]]\n",
      "i_s=671 , [[2.28677368 0.         1.        ]]\n",
      "i_s=672 , [[0.17876025 1.         1.        ]]\n",
      "i_s=673 , [[1.86046338 0.         1.        ]]\n",
      "i_s=674 , [[4.25326729 0.         1.        ]]\n",
      "i_s=675 , [[0.63754219 1.         1.        ]]\n",
      "i_s=676 , [[2.32274628 0.         1.        ]]\n",
      "i_s=677 , [[2.82789278 0.         1.        ]]\n",
      "i_s=678 , [[4.77625847 0.         0.        ]]\n",
      "i_s=679 , [[3.18186569 0.         0.        ]]\n",
      "i_s=680 , [[0.34843498 1.         1.        ]]\n",
      "i_s=681 , [[2.60383916 0.         1.        ]]\n",
      "i_s=682 , [[2.1446116 0.        1.       ]]\n",
      "i_s=683 , [[4.73376036 0.         0.        ]]\n",
      "i_s=684 , [[0.34036136 1.         1.        ]]\n",
      "i_s=685 , [[0.0409706 1.        1.       ]]\n",
      "i_s=686 , [[2.26696968 0.         1.        ]]\n",
      "i_s=687 , [[3.38934469 0.         1.        ]]\n",
      "i_s=688 , [[1.50985265 0.         1.        ]]\n",
      "i_s=689 , [[1.00782204 1.         1.        ]]\n",
      "i_s=690 , [[2.07093811 0.         1.        ]]\n",
      "i_s=691 , [[0.4640919 1.        1.       ]]\n",
      "i_s=692 , [[0.41840377 1.         1.        ]]\n",
      "i_s=693 , [[3.19975209 0.         0.        ]]\n",
      "i_s=694 , [[3.14809513 0.         1.        ]]\n",
      "i_s=695 , [[1.66352558 0.         1.        ]]\n",
      "i_s=696 , [[0.16564178 1.         1.        ]]\n",
      "i_s=697 , [[0.81041491 0.         1.        ]]\n",
      "i_s=698 , [[0.97056383 1.         1.        ]]\n",
      "i_s=699 , [[1.08491564 0.         1.        ]]\n",
      "i_s=700 , [[4.51652718 0.         0.        ]]\n",
      "i_s=701 , [[1.3880806 0.        1.       ]]\n",
      "i_s=702 , [[0.66413277 1.         1.        ]]\n",
      "i_s=703 , [[0.97534519 0.         1.        ]]\n",
      "i_s=704 , [[1.42015791 0.         1.        ]]\n",
      "i_s=705 , [[6.85793638 0.         0.        ]]\n",
      "i_s=706 , [[0.18465498 1.         1.        ]]\n",
      "i_s=707 , [[2.39556074 0.         1.        ]]\n",
      "i_s=708 , [[0.62873846 1.         1.        ]]\n",
      "i_s=709 , [[0.12426972 1.         1.        ]]\n",
      "i_s=710 , [[0.08098182 1.         1.        ]]\n",
      "i_s=711 , [[1.31704462 0.         1.        ]]\n",
      "i_s=712 , [[1.73213816 0.         1.        ]]\n",
      "i_s=713 , [[0.27694988 1.         1.        ]]\n",
      "i_s=714 , [[1.49832118 0.         1.        ]]\n",
      "i_s=715 , [[0.30995506 1.         1.        ]]\n",
      "i_s=716 , [[9.60445404 0.         0.        ]]\n",
      "i_s=717 , [[0.49829954 1.         1.        ]]\n",
      "i_s=718 , [[0.80480951 1.         1.        ]]\n",
      "i_s=719 , [[1.7719512 0.        1.       ]]\n",
      "i_s=720 , [[0.35690007 1.         1.        ]]\n",
      "i_s=721 , [[1.76049328 0.         1.        ]]\n",
      "i_s=722 , [[3.54546523 0.         0.        ]]\n",
      "i_s=723 , [[0.21977456 1.         1.        ]]\n",
      "i_s=724 , [[3.32216024 0.         1.        ]]\n",
      "i_s=725 , [[0.15407349 1.         1.        ]]\n",
      "i_s=726 , [[2.57897353 0.         1.        ]]\n",
      "i_s=727 , [[0.477707 1.       1.      ]]\n",
      "i_s=728 , [[0.71908492 1.         1.        ]]\n",
      "i_s=729 , [[0.19986863 1.         1.        ]]\n",
      "i_s=730 , [[2.82474422 0.         1.        ]]\n",
      "i_s=731 , [[2.22024298 0.         1.        ]]\n",
      "i_s=732 , [[0.7797147 1.        1.       ]]\n",
      "i_s=733 , [[0.71880543 1.         1.        ]]\n",
      "i_s=734 , [[3.42206621 0.         0.        ]]\n",
      "i_s=735 , [[3.73019385 0.         0.        ]]\n",
      "i_s=736 , [[0.11504851 1.         1.        ]]\n",
      "i_s=737 , [[0.75671142 1.         1.        ]]\n",
      "i_s=738 , [[0.78708607 1.         1.        ]]\n",
      "i_s=739 , [[0.08547623 1.         1.        ]]\n",
      "i_s=740 , [[0.11787014 1.         1.        ]]\n",
      "i_s=741 , [[0.79304779 1.         1.        ]]\n",
      "i_s=742 , [[0.53547335 1.         1.        ]]\n",
      "i_s=743 , [[0.08562385 1.         1.        ]]\n",
      "i_s=744 , [[0.2404566 1.        1.       ]]\n",
      "i_s=745 , [[1.1740787 1.        1.       ]]\n",
      "i_s=746 , [[1.0391041 0.        1.       ]]\n",
      "i_s=747 , [[2.19018054 0.         1.        ]]\n",
      "i_s=748 , [[0.62820792 1.         1.        ]]\n",
      "i_s=749 , [[0.48579144 1.         1.        ]]\n",
      "i_s=750 , [[3.43166447 0.         0.        ]]\n",
      "i_s=751 , [[0.66669339 1.         1.        ]]\n",
      "i_s=752 , [[0.19845644 1.         1.        ]]\n",
      "i_s=753 , [[3.49563789 0.         0.        ]]\n",
      "i_s=754 , [[1.63607049 0.         1.        ]]\n",
      "i_s=755 , [[0.52053857 1.         1.        ]]\n",
      "i_s=756 , [[0.47386098 1.         1.        ]]\n",
      "i_s=757 , [[2.34546566 0.         1.        ]]\n",
      "i_s=758 , [[0.47815013 1.         1.        ]]\n",
      "i_s=759 , [[0.77917343 1.         1.        ]]\n",
      "i_s=760 , [[0.42397138 1.         1.        ]]\n",
      "i_s=761 , [[1.7460928 0.        1.       ]]\n",
      "i_s=762 , [[0.87640435 1.         1.        ]]\n",
      "i_s=763 , [[3.09385777 0.         0.        ]]\n",
      "i_s=764 , [[0.15073705 1.         1.        ]]\n",
      "i_s=765 , [[3.03051329 0.         0.        ]]\n",
      "i_s=766 , [[7.02215481 0.         0.        ]]\n",
      "i_s=767 , [[0.2849372 1.        1.       ]]\n",
      "i_s=768 , [[1.65797853 0.         1.        ]]\n",
      "i_s=769 , [[0.07573026 1.         1.        ]]\n",
      "i_s=770 , [[0.34200853 1.         1.        ]]\n",
      "i_s=771 , [[2.11751175 0.         1.        ]]\n",
      "i_s=772 , [[1.66573119 0.         1.        ]]\n",
      "i_s=773 , [[0.42663521 1.         1.        ]]\n",
      "i_s=774 , [[1.63525343 0.         1.        ]]\n",
      "i_s=775 , [[1.88360238 0.         1.        ]]\n",
      "i_s=776 , [[3.38269806 0.         1.        ]]\n",
      "i_s=777 , [[0.44820321 1.         1.        ]]\n",
      "i_s=778 , [[1.82059634 0.         1.        ]]\n",
      "i_s=779 , [[1.12317288 0.         1.        ]]\n",
      "i_s=780 , [[0.87127912 1.         1.        ]]\n",
      "i_s=781 , [[0.48378474 1.         1.        ]]\n",
      "i_s=782 , [[0.34772208 1.         1.        ]]\n",
      "i_s=783 , [[0.55882204 1.         1.        ]]\n",
      "i_s=784 , [[0.48140329 1.         1.        ]]\n",
      "i_s=785 , [[0.0443394 1.        1.       ]]\n",
      "i_s=786 , [[2.78572989 0.         1.        ]]\n",
      "i_s=787 , [[1.74505305 0.         1.        ]]\n",
      "i_s=788 , [[0.94997978 1.         1.        ]]\n",
      "i_s=789 , [[1.01058888 1.         1.        ]]\n",
      "i_s=790 , [[0.83142149 1.         1.        ]]\n",
      "i_s=791 , [[4.15832329 0.         0.        ]]\n",
      "i_s=792 , [[0.95730656 1.         1.        ]]\n",
      "i_s=793 , [[1.6323477 0.        1.       ]]\n",
      "i_s=794 , [[0.8141818 1.        1.       ]]\n",
      "i_s=795 , [[2.42992926 0.         1.        ]]\n",
      "i_s=796 , [[0.23735371 1.         1.        ]]\n",
      "i_s=797 , [[0.6135571 1.        1.       ]]\n",
      "i_s=798 , [[3.77596283 0.         0.        ]]\n",
      "i_s=799 , [[1.15097857 0.         1.        ]]\n",
      "i_s=800 , [[2.80323601 0.         1.        ]]\n",
      "i_s=801 , [[0.30545521 1.         1.        ]]\n",
      "i_s=802 , [[2.88332248 0.         1.        ]]\n",
      "i_s=803 , [[0.9291054 1.        1.       ]]\n",
      "i_s=804 , [[0.24257144 1.         1.        ]]\n",
      "i_s=805 , [[1.09034681 1.         1.        ]]\n",
      "i_s=806 , [[0.33595362 1.         1.        ]]\n",
      "i_s=807 , [[1.94144881 0.         1.        ]]\n",
      "i_s=808 , [[0.55296487 1.         1.        ]]\n",
      "i_s=809 , [[0.3401458 1.        1.       ]]\n",
      "i_s=810 , [[1.32607567 0.         1.        ]]\n",
      "i_s=811 , [[1.76887381 0.         1.        ]]\n",
      "i_s=812 , [[0.95035791 1.         1.        ]]\n",
      "i_s=813 , [[0.4970791 1.        1.       ]]\n",
      "i_s=814 , [[1.45576644 0.         1.        ]]\n",
      "i_s=815 , [[1.98566961 0.         1.        ]]\n",
      "i_s=816 , [[2.90398645 0.         1.        ]]\n",
      "i_s=817 , [[1.04183865 1.         1.        ]]\n",
      "i_s=818 , [[0.54016113 1.         1.        ]]\n",
      "i_s=819 , [[0.06309059 1.         1.        ]]\n",
      "i_s=820 , [[0.36837927 1.         1.        ]]\n",
      "i_s=821 , [[0.80883735 1.         1.        ]]\n",
      "i_s=822 , [[3.17434788 0.         0.        ]]\n",
      "i_s=823 , [[1.19175947 1.         1.        ]]\n",
      "i_s=824 , [[0.67327398 1.         1.        ]]\n",
      "i_s=825 , [[0.55470365 1.         1.        ]]\n",
      "i_s=826 , [[3.09285164 0.         1.        ]]\n",
      "i_s=827 , [[2.48420954 0.         1.        ]]\n",
      "i_s=828 , [[2.4771862 0.        1.       ]]\n",
      "i_s=829 , [[0.29762694 1.         1.        ]]\n",
      "i_s=830 , [[1.96008253 0.         1.        ]]\n",
      "i_s=831 , [[3.97395658 0.         0.        ]]\n",
      "i_s=832 , [[2.12315631 0.         1.        ]]\n",
      "i_s=833 , [[4.63971329 0.         0.        ]]\n",
      "i_s=834 , [[0.18570013 1.         1.        ]]\n",
      "i_s=835 , [[0.09723596 1.         1.        ]]\n",
      "i_s=836 , [[1.80511951 0.         1.        ]]\n",
      "i_s=837 , [[0.27450678 1.         1.        ]]\n",
      "i_s=838 , [[1.49406409 0.         1.        ]]\n",
      "i_s=839 , [[1.09703076 0.         1.        ]]\n",
      "i_s=840 , [[0.89745098 1.         1.        ]]\n",
      "i_s=841 , [[0.48842782 1.         1.        ]]\n",
      "i_s=842 , [[7.86747599 0.         0.        ]]\n",
      "i_s=843 , [[3.57412839 0.         1.        ]]\n",
      "i_s=844 , [[0.4405936 1.        1.       ]]\n",
      "i_s=845 , [[0.70545977 1.         1.        ]]\n",
      "i_s=846 , [[0.22258769 1.         1.        ]]\n",
      "i_s=847 , [[1.94034457 0.         1.        ]]\n",
      "i_s=848 , [[1.84720206 0.         1.        ]]\n",
      "i_s=849 , [[0.349105 1.       1.      ]]\n",
      "i_s=850 , [[0.07042576 1.         1.        ]]\n",
      "i_s=851 , [[2.6903646 0.        1.       ]]\n",
      "i_s=852 , [[4.9145956 0.        0.       ]]\n",
      "i_s=853 , [[0.53760397 1.         1.        ]]\n",
      "i_s=854 , [[0.59004813 1.         1.        ]]\n",
      "i_s=855 , [[0.44185323 1.         1.        ]]\n",
      "i_s=856 , [[0.70290571 1.         1.        ]]\n",
      "i_s=857 , [[2.32521677 0.         1.        ]]\n",
      "i_s=858 , [[0.938914 1.       1.      ]]\n",
      "i_s=859 , [[4.1057291 0.        0.       ]]\n",
      "i_s=860 , [[0.40137717 1.         1.        ]]\n",
      "i_s=861 , [[0.49331951 1.         1.        ]]\n",
      "i_s=862 , [[0.46863574 1.         1.        ]]\n",
      "i_s=863 , [[0.46392334 1.         1.        ]]\n",
      "i_s=864 , [[1.37375951 0.         1.        ]]\n",
      "i_s=865 , [[0.36535352 1.         1.        ]]\n",
      "i_s=866 , [[0.3158015 1.        1.       ]]\n",
      "i_s=867 , [[1.30510366 0.         1.        ]]\n",
      "i_s=868 , [[4.22923326 0.         0.        ]]\n",
      "i_s=869 , [[0.54221028 1.         1.        ]]\n",
      "i_s=870 , [[0.35898376 1.         1.        ]]\n",
      "i_s=871 , [[2.49361205 0.         1.        ]]\n",
      "i_s=872 , [[2.00262856 0.         1.        ]]\n",
      "i_s=873 , [[0.27045444 1.         1.        ]]\n",
      "i_s=874 , [[0.75516343 1.         1.        ]]\n",
      "i_s=875 , [[3.74567366 0.         1.        ]]\n",
      "i_s=876 , [[1.9377991 0.        1.       ]]\n",
      "i_s=877 , [[0.41482535 1.         1.        ]]\n",
      "i_s=878 , [[5.69307375 0.         0.        ]]\n",
      "i_s=879 , [[3.94362831 0.         0.        ]]\n",
      "i_s=880 , [[2.85342073 0.         1.        ]]\n",
      "i_s=881 , [[1.78291714 0.         1.        ]]\n",
      "i_s=882 , [[3.00279331 0.         1.        ]]\n",
      "i_s=883 , [[0.902412 1.       1.      ]]\n",
      "i_s=884 , [[3.02229881 0.         1.        ]]\n",
      "i_s=885 , [[2.74562311 0.         1.        ]]\n",
      "i_s=886 , [[0.18491639 1.         1.        ]]\n",
      "i_s=887 , [[5.67701197 0.         0.        ]]\n",
      "i_s=888 , [[1.12786138 1.         1.        ]]\n",
      "i_s=889 , [[1.71281445 0.         1.        ]]\n",
      "i_s=890 , [[1.90592504 0.         1.        ]]\n",
      "i_s=891 , [[0.35999545 1.         1.        ]]\n",
      "i_s=892 , [[0.33662727 1.         1.        ]]\n",
      "i_s=893 , [[0.21943432 1.         1.        ]]\n",
      "i_s=894 , [[3.27393007 0.         1.        ]]\n",
      "i_s=895 , [[0.6419304 1.        1.       ]]\n",
      "i_s=896 , [[0.52768677 1.         1.        ]]\n",
      "i_s=897 , [[0.42455268 1.         1.        ]]\n",
      "i_s=898 , [[0.12758712 1.         1.        ]]\n",
      "i_s=899 , [[2.17543578 0.         1.        ]]\n",
      "i_s=900 , [[0.23848227 1.         1.        ]]\n",
      "i_s=901 , [[0.20564166 1.         1.        ]]\n",
      "i_s=902 , [[0.19104621 1.         1.        ]]\n",
      "i_s=903 , [[1.33172691 0.         1.        ]]\n",
      "i_s=904 , [[2.16946316 0.         1.        ]]\n",
      "i_s=905 , [[3.45413113 0.         1.        ]]\n",
      "i_s=906 , [[1.50156128 1.         1.        ]]\n",
      "i_s=907 , [[0.57216126 1.         1.        ]]\n",
      "i_s=908 , [[0.28644475 1.         1.        ]]\n",
      "i_s=909 , [[1.18137705 0.         1.        ]]\n",
      "i_s=910 , [[1.41180253 0.         1.        ]]\n",
      "i_s=911 , [[0.14458324 1.         1.        ]]\n",
      "i_s=912 , [[0.24615945 1.         1.        ]]\n",
      "i_s=913 , [[0.69254392 1.         1.        ]]\n",
      "i_s=914 , [[1.86733174 0.         1.        ]]\n",
      "i_s=915 , [[0.15043372 1.         1.        ]]\n",
      "i_s=916 , [[3.69104528 0.         1.        ]]\n",
      "i_s=917 , [[0.27772316 1.         1.        ]]\n",
      "i_s=918 , [[0.0705253 1.        1.       ]]\n",
      "i_s=919 , [[1.5696907 0.        1.       ]]\n",
      "i_s=920 , [[6.93053722 0.         0.        ]]\n",
      "i_s=921 , [[0.72252285 1.         1.        ]]\n",
      "i_s=922 , [[0.28858587 1.         1.        ]]\n",
      "i_s=923 , [[2.34256554 0.         1.        ]]\n",
      "i_s=924 , [[0.79075319 1.         1.        ]]\n",
      "i_s=925 , [[0.06755709 1.         1.        ]]\n",
      "i_s=926 , [[0.31702375 1.         1.        ]]\n",
      "i_s=927 , [[2.11910224 0.         1.        ]]\n",
      "i_s=928 , [[0.6835202 1.        1.       ]]\n",
      "i_s=929 , [[0.52375782 1.         1.        ]]\n",
      "i_s=930 , [[0.89395452 1.         1.        ]]\n",
      "i_s=931 , [[1.26121581 1.         1.        ]]\n",
      "i_s=932 , [[0.81097323 1.         1.        ]]\n",
      "i_s=933 , [[2.16425562 0.         1.        ]]\n",
      "i_s=934 , [[0.51472008 1.         1.        ]]\n",
      "i_s=935 , [[0.54459101 1.         1.        ]]\n",
      "i_s=936 , [[0.67046332 1.         1.        ]]\n",
      "i_s=937 , [[0.26089257 1.         1.        ]]\n",
      "i_s=938 , [[2.90552735 0.         1.        ]]\n",
      "i_s=939 , [[0.6064043 1.        1.       ]]\n",
      "i_s=940 , [[0.19624335 1.         1.        ]]\n",
      "i_s=941 , [[0.10419828 1.         1.        ]]\n",
      "i_s=942 , [[2.48464251 0.         1.        ]]\n",
      "i_s=943 , [[0.20532386 1.         1.        ]]\n",
      "i_s=944 , [[1.37761331 0.         1.        ]]\n",
      "i_s=945 , [[0.21045959 1.         1.        ]]\n",
      "i_s=946 , [[0.05975292 1.         1.        ]]\n",
      "i_s=947 , [[0.08358194 1.         1.        ]]\n",
      "i_s=948 , [[0.64673758 1.         1.        ]]\n",
      "i_s=949 , [[1.9647553 0.        1.       ]]\n",
      "i_s=950 , [[1.2808708 0.        1.       ]]\n",
      "i_s=951 , [[0.91392988 1.         1.        ]]\n",
      "i_s=952 , [[2.73190904 0.         1.        ]]\n",
      "i_s=953 , [[1.50857306 0.         1.        ]]\n",
      "i_s=954 , [[2.87834358 0.         1.        ]]\n",
      "i_s=955 , [[0.82281017 1.         1.        ]]\n",
      "i_s=956 , [[5.88614702 0.         0.        ]]\n",
      "i_s=957 , [[0.41986388 1.         1.        ]]\n",
      "i_s=958 , [[0.96826929 0.         1.        ]]\n",
      "i_s=959 , [[0.0847325 1.        1.       ]]\n",
      "i_s=960 , [[1.58596313 0.         1.        ]]\n",
      "i_s=961 , [[1.63284588 0.         1.        ]]\n",
      "i_s=962 , [[1.54495144 0.         1.        ]]\n",
      "i_s=963 , [[1.08974683 1.         1.        ]]\n",
      "i_s=964 , [[2.91135406 0.         1.        ]]\n",
      "i_s=965 , [[0.87875187 1.         1.        ]]\n",
      "i_s=966 , [[1.28296375 1.         1.        ]]\n",
      "i_s=967 , [[2.58801413 0.         1.        ]]\n",
      "i_s=968 , [[1.31626987 0.         1.        ]]\n",
      "i_s=969 , [[5.99856615 0.         0.        ]]\n",
      "i_s=970 , [[1.70520318 0.         1.        ]]\n",
      "i_s=971 , [[3.31558967 0.         0.        ]]\n",
      "i_s=972 , [[1.02074182 1.         1.        ]]\n",
      "i_s=973 , [[0.92056948 1.         1.        ]]\n",
      "i_s=974 , [[0.76959598 1.         1.        ]]\n",
      "i_s=975 , [[0.39110538 1.         1.        ]]\n",
      "i_s=976 , [[0.52776694 1.         1.        ]]\n",
      "i_s=977 , [[0.12458824 1.         1.        ]]\n",
      "i_s=978 , [[0.70007461 1.         1.        ]]\n",
      "i_s=979 , [[2.08323812 0.         1.        ]]\n",
      "i_s=980 , [[0.71644431 1.         1.        ]]\n",
      "i_s=981 , [[6.54700184 0.         0.        ]]\n",
      "i_s=982 , [[1.29903722 0.         1.        ]]\n",
      "i_s=983 , [[0.0915649 1.        1.       ]]\n",
      "i_s=984 , [[1.60489595 0.         1.        ]]\n",
      "i_s=985 , [[0.07691373 1.         1.        ]]\n",
      "i_s=986 , [[0.65277451 1.         1.        ]]\n",
      "i_s=987 , [[2.36597729 0.         1.        ]]\n",
      "i_s=988 , [[5.87068415 0.         0.        ]]\n",
      "i_s=989 , [[0.30987263 1.         1.        ]]\n",
      "i_s=990 , [[0.66222364 1.         1.        ]]\n",
      "i_s=991 , [[2.82939863 0.         1.        ]]\n",
      "i_s=992 , [[0.13432272 1.         1.        ]]\n",
      "i_s=993 , [[0.23814638 1.         1.        ]]\n",
      "i_s=994 , [[2.61729121 0.         1.        ]]\n",
      "i_s=995 , [[0.06573904 1.         1.        ]]\n",
      "i_s=996 , [[0.44994709 1.         1.        ]]\n",
      "i_s=997 , [[0.50899506 1.         1.        ]]\n",
      "i_s=998 , [[0.06795267 1.         1.        ]]\n",
      "i_s=999 , [[3.58604336 0.         1.        ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1.42878651, 0.587     , 0.901     ])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####TEST DNN\n",
    "print('------------Building DNN model--------------')\n",
    "ShuffleInTraining=True\n",
    "N_EPOCHS_0=7\n",
    "N_EPOCHS=3\n",
    "N_HN_1=128\n",
    "N_HN=128\n",
    "N_LAYERS=1\n",
    "N_BATCH=64\n",
    "\n",
    "Rate_Val=0.8\n",
    "N_Val_OverSampler=int(np.around(N_AllTrain_OverSampler*Rate_Val))\n",
    "N_Train_OverSampler=int(N_AllTrain_OverSampler-N_Val_OverSampler)\n",
    "N_CLASS=len(allDF[\"Category\"].unique())\n",
    "input_dim=featuresArrayOverSampler.shape[1]\n",
    "output_dim=N_CLASS\n",
    "\n",
    "N_Split=1000\n",
    "BlockSize=int(np.floor(N_Val_OverSampler/N_Split))\n",
    "print('-----------------Build DNN model and start the 1st training!!---------------------')\n",
    "model = Sequential()\n",
    "model.add(Dense(N_HN_1,input_dim=input_dim))\n",
    "model.add(BatchNormalization())\n",
    "model.add(PReLU())\n",
    "for i in range(N_LAYERS):\n",
    "    model.add(Dense(N_HN))\n",
    "    model.add(BatchNormalization())   \n",
    "    model.add(PReLU())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(output_dim))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy', metrics.top_k_categorical_accuracy])\n",
    "labelsArrayOverSampler_1hot=keras.utils.to_categorical(LabelEncoder().fit_transform(np.array(labelsArrayOverSampler)), num_classes=N_CLASS)\n",
    "if ConsiderTime:\n",
    "    print('--------Spllit train val according to time!---------')\n",
    "    x_train=featuresArrayOverSampler[0:N_Train_OverSampler,:]\n",
    "    y_train=labelsArrayOverSampler_1hot[0:N_Train_OverSampler,:]\n",
    "else:\n",
    "    x_train,x_val,y_train,y_val = train_test_split(featuresArrayOverSampler,labelsArrayOverSampler_1hot,test_size=N_Val_OverSampler,train_size=N_Train_OverSampler, shuffle=True)\n",
    "# y_train = keras.utils.to_categorical(LabelEncoder().fit_transform(np.array(y_train)), num_classes=N_CLASS)\n",
    "print(str(x_train.shape))\n",
    "\n",
    "x_val_i=featuresArrayOverSampler[N_Train_OverSampler:N_Train_OverSampler+1,:]\n",
    "y_val_i=labelsArrayOverSampler_1hot[N_Train_OverSampler:N_Train_OverSampler+1,:]\n",
    "print(str(x_val_i.shape))\n",
    "\n",
    "# y_val_i = keras.utils.to_categorical(LabelEncoder().fit_transform(np.array(y_val_i)), num_classes=N_CLASS)\n",
    "print('------------DNN Training Go! Go! Go!!!!-----------')\n",
    "fitting=model.fit(x_train, y_train, epochs=N_EPOCHS_0, batch_size=N_BATCH,verbose=1,validation_data=(x_val_i,y_val_i),shuffle=True)\n",
    "# score01=model.predict(x_val_i, batch_size=None, verbose=0, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False)\n",
    "score0=model.evaluate(x=x_val_i, y=y_val_i, batch_size=None, verbose=1, sample_weight=None, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False)\n",
    "print(str(score0))\n",
    "print('-----------------Start the loop training!!---------------------')\n",
    "Scores_all=np.zeros([N_Split,3])\n",
    "for i_s in range(N_Split):\n",
    "    x_train=featuresArrayOverSampler[N_Train_OverSampler+i_s*BlockSize:N_Train_OverSampler+(i_s+1)*BlockSize,:]\n",
    "    y_train=labelsArrayOverSampler_1hot[N_Train_OverSampler+i_s*BlockSize:N_Train_OverSampler+(i_s+1)*BlockSize,:]\n",
    "    x_val_i=featuresArrayOverSampler[N_Train_OverSampler+(i_s+1)*BlockSize:N_Train_OverSampler+(i_s+1)*BlockSize+1,:]\n",
    "    y_val_i=labelsArrayOverSampler_1hot[N_Train_OverSampler+(i_s+1)*BlockSize:N_Train_OverSampler+(i_s+1)*BlockSize+1,:]\n",
    "    fitting=model.fit(x_train, y_train, epochs=N_EPOCHS, batch_size=N_BATCH,verbose=0,validation_data=(x_val_i,y_val_i),shuffle=True)\n",
    "    loss_i, acc_i, top5acc_i=model.evaluate(x=x_val_i, y=y_val_i, batch_size=None, verbose=0, sample_weight=None, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False)\n",
    "    print('i_s='+str(i_s)+' , '+str(np.array([[loss_i, acc_i, top5acc_i]])))\n",
    "    Scores_all[i_s,:]=np.array([[loss_i, acc_i, top5acc_i]])\n",
    "    \n",
    "np.mean(Scores_all, axis=0)    \n",
    "# y_val = keras.utils.to_categorical(LabelEncoder().fit_transform(np.array(y_val)), num_classes=N_CLASS)\n",
    "# \n",
    "# model.save('jjs_model_0124V3.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实验T4：对新的Block和旧的TrainSet混在一起重进行Transfer　Learning学习，这样速度非常慢，分成100个Block大概需要10个小时以上【由于操作失误，重新运行了此程序段，但是又考虑到耗时太长，就终止了，实验结果可以参考word记录】"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------Building DNN model--------------\n",
      "-----------------Build DNN model and start the 1st training!!---------------------\n",
      "--------Spllit train val according to time!---------\n",
      "(162506, 110)\n",
      "(1, 110)\n",
      "------------DNN Training Go! Go! Go!!!!-----------\n",
      "Train on 162506 samples, validate on 1 samples\n",
      "Epoch 1/7\n",
      " 29184/162506 [====>.........................] - ETA: 46s - loss: 2.7513 - accuracy: 0.2656 - top_k_categorical_accuracy: 0.6205"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-7ebc97721b52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;31m# y_val_i = keras.utils.to_categorical(LabelEncoder().fit_transform(np.array(y_val_i)), num_classes=N_CLASS)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'------------DNN Training Go! Go! Go!!!!-----------'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0mfitting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mN_EPOCHS_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mN_BATCH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val_i\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_val_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;31m# score01=model.predict(x_val_i, batch_size=None, verbose=0, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0mscore0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_val_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_val_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3292\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "####TEST DNN\n",
    "print('------------Building DNN model--------------')\n",
    "ShuffleInTraining=True\n",
    "N_EPOCHS_0=7\n",
    "N_EPOCHS=3\n",
    "N_HN_1=128\n",
    "N_HN=128\n",
    "N_LAYERS=1\n",
    "N_BATCH=64\n",
    "\n",
    "Rate_Val=0.8\n",
    "N_Val_OverSampler=int(np.around(N_AllTrain_OverSampler*Rate_Val))\n",
    "N_Train_OverSampler=int(N_AllTrain_OverSampler-N_Val_OverSampler)\n",
    "N_CLASS=len(allDF[\"Category\"].unique())\n",
    "input_dim=featuresArrayOverSampler.shape[1]\n",
    "output_dim=N_CLASS\n",
    "\n",
    "N_Split=100\n",
    "BlockSize=int(np.floor(N_Val_OverSampler/N_Split))\n",
    "print('-----------------Build DNN model and start the 1st training!!---------------------')\n",
    "model = Sequential()\n",
    "model.add(Dense(N_HN_1,input_dim=input_dim))\n",
    "model.add(BatchNormalization())\n",
    "model.add(PReLU())\n",
    "for i in range(N_LAYERS):\n",
    "    model.add(Dense(N_HN))\n",
    "    model.add(BatchNormalization())   \n",
    "    model.add(PReLU())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(output_dim))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy', metrics.top_k_categorical_accuracy])\n",
    "labelsArrayOverSampler_1hot=keras.utils.to_categorical(LabelEncoder().fit_transform(np.array(labelsArrayOverSampler)), num_classes=N_CLASS)\n",
    "if ConsiderTime:\n",
    "    print('--------Spllit train val according to time!---------')\n",
    "    x_train=featuresArrayOverSampler[0:N_Train_OverSampler,:]\n",
    "    y_train=labelsArrayOverSampler_1hot[0:N_Train_OverSampler,:]\n",
    "else:\n",
    "    x_train,x_val,y_train,y_val = train_test_split(featuresArrayOverSampler,labelsArrayOverSampler_1hot,test_size=N_Val_OverSampler,train_size=N_Train_OverSampler, shuffle=True)\n",
    "# y_train = keras.utils.to_categorical(LabelEncoder().fit_transform(np.array(y_train)), num_classes=N_CLASS)\n",
    "print(str(x_train.shape))\n",
    "\n",
    "x_val_i=featuresArrayOverSampler[N_Train_OverSampler:N_Train_OverSampler+1,:]\n",
    "y_val_i=labelsArrayOverSampler_1hot[N_Train_OverSampler:N_Train_OverSampler+1,:]\n",
    "print(str(x_val_i.shape))\n",
    "\n",
    "# y_val_i = keras.utils.to_categorical(LabelEncoder().fit_transform(np.array(y_val_i)), num_classes=N_CLASS)\n",
    "print('------------DNN Training Go! Go! Go!!!!-----------')\n",
    "fitting=model.fit(x_train, y_train, epochs=N_EPOCHS_0, batch_size=N_BATCH,verbose=1,validation_data=(x_val_i,y_val_i),shuffle=True)\n",
    "# score01=model.predict(x_val_i, batch_size=None, verbose=0, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False)\n",
    "score0=model.evaluate(x=x_val_i, y=y_val_i, batch_size=None, verbose=1, sample_weight=None, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False)\n",
    "print(str(score0))\n",
    "print('-----------------Start the loop training!!---------------------')\n",
    "Scores_all=np.zeros([N_Split,3])\n",
    "for i_s in range(N_Split):\n",
    "    #model不需要重建\n",
    "    x_train=featuresArrayOverSampler[0:N_Train_OverSampler+(i_s+1)*BlockSize,:]\n",
    "    y_train=labelsArrayOverSampler_1hot[0:N_Train_OverSampler+(i_s+1)*BlockSize,:]\n",
    "    x_val_i=featuresArrayOverSampler[N_Train_OverSampler+(i_s+1)*BlockSize:N_Train_OverSampler+(i_s+1)*BlockSize+1,:]\n",
    "    y_val_i=labelsArrayOverSampler_1hot[N_Train_OverSampler+(i_s+1)*BlockSize:N_Train_OverSampler+(i_s+1)*BlockSize+1,:]\n",
    "    fitting=model.fit(x_train, y_train, epochs=N_EPOCHS, batch_size=N_BATCH,verbose=0,validation_data=(x_val_i,y_val_i),shuffle=True)\n",
    "    loss_i, acc_i, top5acc_i=model.evaluate(x=x_val_i, y=y_val_i, batch_size=None, verbose=0, sample_weight=None, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False)\n",
    "    print('i_s='+str(i_s)+' , '+str(np.array([[loss_i, acc_i, top5acc_i]])))\n",
    "    Scores_all[i_s,:]=np.array([[loss_i, acc_i, top5acc_i]])\n",
    "    \n",
    "np.mean(Scores_all, axis=0)    \n",
    "# y_val = keras.utils.to_categorical(LabelEncoder().fit_transform(np.array(y_val)), num_classes=N_CLASS)\n",
    "# \n",
    "# model.save('jjs_model_0124V3.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T5：考虑到时间太长的问题，我们分成500个block，并且只对新的Block进行Transfer Learning，而不是和前面的所有样本混在一起训练，T6,1000个"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------Building DNN model--------------\n",
      "-----------------Build DNN model and start the 1st training!!---------------------\n",
      "--------Spllit train val according to time!---------\n",
      "(162506, 110)\n",
      "(1, 110)\n",
      "------------DNN Training Go! Go! Go!!!!-----------\n",
      "Train on 162506 samples, validate on 1 samples\n",
      "Epoch 1/7\n",
      "162506/162506 [==============================] - 53s 329us/step - loss: 2.3323 - accuracy: 0.3053 - top_k_categorical_accuracy: 0.7178 - val_loss: 1.4861 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 2/7\n",
      "162506/162506 [==============================] - 53s 326us/step - loss: 2.1888 - accuracy: 0.3244 - top_k_categorical_accuracy: 0.7510 - val_loss: 1.9140 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 3/7\n",
      "162506/162506 [==============================] - 53s 326us/step - loss: 2.1705 - accuracy: 0.3291 - top_k_categorical_accuracy: 0.7560 - val_loss: 1.8580 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 4/7\n",
      "162506/162506 [==============================] - 54s 332us/step - loss: 2.1583 - accuracy: 0.3308 - top_k_categorical_accuracy: 0.7588 - val_loss: 1.7888 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 5/7\n",
      "162506/162506 [==============================] - 52s 322us/step - loss: 2.1481 - accuracy: 0.3333 - top_k_categorical_accuracy: 0.7615 - val_loss: 1.8041 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 6/7\n",
      "162506/162506 [==============================] - 52s 319us/step - loss: 2.1403 - accuracy: 0.3348 - top_k_categorical_accuracy: 0.7631 - val_loss: 1.8603 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 7/7\n",
      "162506/162506 [==============================] - 52s 318us/step - loss: 2.1322 - accuracy: 0.3364 - top_k_categorical_accuracy: 0.7648 - val_loss: 1.5676 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "[1.567645788192749, 1.0, 1.0]\n",
      "-----------------Start the loop training!!---------------------\n",
      "i_s=0 , [[1.8059833 0.        1.       ]]\n",
      "i_s=1 , [[4.2384119 0.        0.       ]]\n",
      "i_s=2 , [[2.13630438 0.         1.        ]]\n",
      "i_s=3 , [[1.88719523 0.         1.        ]]\n",
      "i_s=4 , [[0.77006292 1.         1.        ]]\n",
      "i_s=5 , [[1.76273441 0.         1.        ]]\n",
      "i_s=6 , [[0.05018072 1.         1.        ]]\n",
      "i_s=7 , [[5.76540375 0.         0.        ]]\n",
      "i_s=8 , [[5.31202221 0.         0.        ]]\n",
      "i_s=9 , [[2.84532261 0.         1.        ]]\n",
      "i_s=10 , [[1.80513334 0.         1.        ]]\n",
      "i_s=11 , [[1.87359035 0.         1.        ]]\n",
      "i_s=12 , [[3.59047961 0.         0.        ]]\n",
      "i_s=13 , [[2.01592469 0.         1.        ]]\n",
      "i_s=14 , [[2.05188799 0.         1.        ]]\n",
      "i_s=15 , [[1.46593618 1.         1.        ]]\n",
      "i_s=16 , [[2.7720871 0.        0.       ]]\n",
      "i_s=17 , [[2.25397158 0.         1.        ]]\n",
      "i_s=18 , [[1.94093466 0.         1.        ]]\n",
      "i_s=19 , [[3.23683453 0.         0.        ]]\n",
      "i_s=20 , [[2.42197943 0.         1.        ]]\n",
      "i_s=21 , [[1.43364835 1.         1.        ]]\n",
      "i_s=22 , [[2.12297916 0.         1.        ]]\n",
      "i_s=23 , [[2.71742916 0.         1.        ]]\n",
      "i_s=24 , [[1.02896452 1.         1.        ]]\n",
      "i_s=25 , [[0.84261185 1.         1.        ]]\n",
      "i_s=26 , [[2.32112026 0.         1.        ]]\n",
      "i_s=27 , [[1.60297906 0.         1.        ]]\n",
      "i_s=28 , [[1.3743813 1.        1.       ]]\n",
      "i_s=29 , [[1.00296795 1.         1.        ]]\n",
      "i_s=30 , [[1.30928218 1.         1.        ]]\n",
      "i_s=31 , [[1.85374928 0.         1.        ]]\n",
      "i_s=32 , [[3.1767478 0.        0.       ]]\n",
      "i_s=33 , [[1.46202397 0.         1.        ]]\n",
      "i_s=34 , [[0.91944933 1.         1.        ]]\n",
      "i_s=35 , [[4.9703455 0.        0.       ]]\n",
      "i_s=36 , [[6.02827311 0.         0.        ]]\n",
      "i_s=37 , [[1.23409867 0.         1.        ]]\n",
      "i_s=38 , [[0.49851924 1.         1.        ]]\n",
      "i_s=39 , [[3.31272244 0.         0.        ]]\n",
      "i_s=40 , [[0.11339063 1.         1.        ]]\n",
      "i_s=41 , [[2.84158564 0.         1.        ]]\n",
      "i_s=42 , [[7.68979359 0.         0.        ]]\n",
      "i_s=43 , [[3.00899887 0.         0.        ]]\n",
      "i_s=44 , [[4.94305992 0.         0.        ]]\n",
      "i_s=45 , [[3.24218464 0.         0.        ]]\n",
      "i_s=46 , [[1.57128608 0.         1.        ]]\n",
      "i_s=47 , [[3.93519592 0.         1.        ]]\n",
      "i_s=48 , [[4.11857176 0.         0.        ]]\n",
      "i_s=49 , [[1.58011317 1.         1.        ]]\n",
      "i_s=50 , [[1.17482829 0.         1.        ]]\n",
      "i_s=51 , [[4.07235098 0.         0.        ]]\n",
      "i_s=52 , [[0.33999419 1.         1.        ]]\n",
      "i_s=53 , [[1.14004719 1.         1.        ]]\n",
      "i_s=54 , [[1.57055914 0.         1.        ]]\n",
      "i_s=55 , [[1.32749104 1.         1.        ]]\n",
      "i_s=56 , [[1.62827361 0.         1.        ]]\n",
      "i_s=57 , [[1.86480021 0.         1.        ]]\n",
      "i_s=58 , [[1.61414742 0.         1.        ]]\n",
      "i_s=59 , [[4.45178556 0.         0.        ]]\n",
      "i_s=60 , [[1.73489213 0.         1.        ]]\n",
      "i_s=61 , [[0.88518053 1.         1.        ]]\n",
      "i_s=62 , [[0.87881237 1.         1.        ]]\n",
      "i_s=63 , [[0.38859376 1.         1.        ]]\n",
      "i_s=64 , [[4.75713682 0.         0.        ]]\n",
      "i_s=65 , [[1.62316394 0.         1.        ]]\n",
      "i_s=66 , [[1.74948645 0.         1.        ]]\n",
      "i_s=67 , [[5.62043095 0.         0.        ]]\n",
      "i_s=68 , [[4.6731658 0.        0.       ]]\n",
      "i_s=69 , [[4.04084778 0.         0.        ]]\n",
      "i_s=70 , [[1.45879626 0.         1.        ]]\n",
      "i_s=71 , [[2.29660797 0.         1.        ]]\n",
      "i_s=72 , [[3.47660756 0.         0.        ]]\n",
      "i_s=73 , [[1.24602175 1.         1.        ]]\n",
      "i_s=74 , [[0.89787263 1.         1.        ]]\n",
      "i_s=75 , [[2.71038771 0.         1.        ]]\n",
      "i_s=76 , [[1.33336699 0.         1.        ]]\n",
      "i_s=77 , [[4.7853756 0.        0.       ]]\n",
      "i_s=78 , [[3.15783596 0.         0.        ]]\n",
      "i_s=79 , [[5.27932072 0.         0.        ]]\n",
      "i_s=80 , [[5.51955986 0.         0.        ]]\n",
      "i_s=81 , [[6.09179115 0.         0.        ]]\n",
      "i_s=82 , [[3.24714303 0.         0.        ]]\n",
      "i_s=83 , [[7.49436903 0.         0.        ]]\n",
      "i_s=84 , [[1.90941417 0.         1.        ]]\n",
      "i_s=85 , [[7.5820179 0.        0.       ]]\n",
      "i_s=86 , [[1.70000625 1.         1.        ]]\n",
      "i_s=87 , [[0.79815501 1.         1.        ]]\n",
      "i_s=88 , [[2.32072592 0.         1.        ]]\n",
      "i_s=89 , [[1.82431817 0.         1.        ]]\n",
      "i_s=90 , [[0.66767859 1.         1.        ]]\n",
      "i_s=91 , [[3.29631114 0.         0.        ]]\n",
      "i_s=92 , [[3.46068263 0.         0.        ]]\n",
      "i_s=93 , [[3.00491834 0.         0.        ]]\n",
      "i_s=94 , [[0.57225102 1.         1.        ]]\n",
      "i_s=95 , [[2.31417656 0.         1.        ]]\n",
      "i_s=96 , [[0.60350776 1.         1.        ]]\n",
      "i_s=97 , [[1.97370577 0.         1.        ]]\n",
      "i_s=98 , [[0.5301249 1.        1.       ]]\n",
      "i_s=99 , [[0.38862431 1.         1.        ]]\n",
      "i_s=100 , [[1.64447021 0.         1.        ]]\n",
      "i_s=101 , [[1.82506478 0.         1.        ]]\n",
      "i_s=102 , [[0.47503322 1.         1.        ]]\n",
      "i_s=103 , [[2.36427736 0.         1.        ]]\n",
      "i_s=104 , [[2.05008221 0.         1.        ]]\n",
      "i_s=105 , [[1.91217971 0.         1.        ]]\n",
      "i_s=106 , [[3.16759825 0.         0.        ]]\n",
      "i_s=107 , [[4.12296057 0.         0.        ]]\n",
      "i_s=108 , [[2.60342526 0.         1.        ]]\n",
      "i_s=109 , [[2.28932905 0.         1.        ]]\n",
      "i_s=110 , [[3.12750196 0.         0.        ]]\n",
      "i_s=111 , [[2.39580774 0.         0.        ]]\n",
      "i_s=112 , [[0.27059913 1.         1.        ]]\n",
      "i_s=113 , [[2.74118996 0.         1.        ]]\n",
      "i_s=114 , [[1.47388947 0.         1.        ]]\n",
      "i_s=115 , [[3.33689404 0.         0.        ]]\n",
      "i_s=116 , [[2.69435406 0.         1.        ]]\n",
      "i_s=117 , [[0.20305344 1.         1.        ]]\n",
      "i_s=118 , [[0.6387825 1.        1.       ]]\n",
      "i_s=119 , [[1.28672564 1.         1.        ]]\n",
      "i_s=120 , [[1.25084722 0.         1.        ]]\n",
      "i_s=121 , [[2.58508706 0.         1.        ]]\n",
      "i_s=122 , [[1.62043762 0.         1.        ]]\n",
      "i_s=123 , [[2.34470797 0.         1.        ]]\n",
      "i_s=124 , [[2.11416769 0.         1.        ]]\n",
      "i_s=125 , [[0.28315836 1.         1.        ]]\n",
      "i_s=126 , [[2.00156498 0.         1.        ]]\n",
      "i_s=127 , [[0.95048308 1.         1.        ]]\n",
      "i_s=128 , [[1.82770514 0.         1.        ]]\n",
      "i_s=129 , [[0.49492991 1.         1.        ]]\n",
      "i_s=130 , [[0.80855721 1.         1.        ]]\n",
      "i_s=131 , [[2.09316778 0.         1.        ]]\n",
      "i_s=132 , [[1.65463912 0.         1.        ]]\n",
      "i_s=133 , [[2.45494366 0.         1.        ]]\n",
      "i_s=134 , [[3.89790344 0.         0.        ]]\n",
      "i_s=135 , [[1.2064631 1.        1.       ]]\n",
      "i_s=136 , [[1.87924898 0.         1.        ]]\n",
      "i_s=137 , [[2.35712957 0.         1.        ]]\n",
      "i_s=138 , [[0.32224423 1.         1.        ]]\n",
      "i_s=139 , [[0.9646076 1.        1.       ]]\n",
      "i_s=140 , [[8.60892487 0.         0.        ]]\n",
      "i_s=141 , [[2.4412446 0.        1.       ]]\n",
      "i_s=142 , [[1.66654801 0.         1.        ]]\n",
      "i_s=143 , [[0.81325817 1.         1.        ]]\n",
      "i_s=144 , [[0.23541178 1.         1.        ]]\n",
      "i_s=145 , [[0.6669476 1.        1.       ]]\n",
      "i_s=146 , [[1.73571134 0.         1.        ]]\n",
      "i_s=147 , [[2.91661501 0.         0.        ]]\n",
      "i_s=148 , [[2.40052795 0.         1.        ]]\n",
      "i_s=149 , [[0.07052463 1.         1.        ]]\n",
      "i_s=150 , [[2.55439973 0.         1.        ]]\n",
      "i_s=151 , [[1.37005913 1.         1.        ]]\n",
      "i_s=152 , [[1.97056901 0.         1.        ]]\n",
      "i_s=153 , [[1.43657053 0.         1.        ]]\n",
      "i_s=154 , [[2.71306658 0.         1.        ]]\n",
      "i_s=155 , [[2.99125457 0.         0.        ]]\n",
      "i_s=156 , [[1.74453449 0.         1.        ]]\n",
      "i_s=157 , [[1.93787837 0.         1.        ]]\n",
      "i_s=158 , [[2.94882083 0.         1.        ]]\n",
      "i_s=159 , [[1.79313195 1.         1.        ]]\n",
      "i_s=160 , [[0.83581221 1.         1.        ]]\n",
      "i_s=161 , [[1.80988669 1.         1.        ]]\n",
      "i_s=162 , [[4.02127361 0.         0.        ]]\n",
      "i_s=163 , [[1.91115749 0.         1.        ]]\n",
      "i_s=164 , [[1.91796851 0.         1.        ]]\n",
      "i_s=165 , [[1.96603715 0.         1.        ]]\n",
      "i_s=166 , [[3.57553673 0.         0.        ]]\n",
      "i_s=167 , [[1.03530002 0.         1.        ]]\n",
      "i_s=168 , [[1.19595063 1.         1.        ]]\n",
      "i_s=169 , [[1.18694091 1.         1.        ]]\n",
      "i_s=170 , [[3.33285379 0.         0.        ]]\n",
      "i_s=171 , [[2.34704423 0.         1.        ]]\n",
      "i_s=172 , [[1.67999327 1.         1.        ]]\n",
      "i_s=173 , [[1.62518811 0.         1.        ]]\n",
      "i_s=174 , [[2.34452224 0.         1.        ]]\n",
      "i_s=175 , [[2.93446469 0.         1.        ]]\n",
      "i_s=176 , [[2.18379402 0.         1.        ]]\n",
      "i_s=177 , [[1.35417461 1.         1.        ]]\n",
      "i_s=178 , [[2.13090372 0.         1.        ]]\n",
      "i_s=179 , [[1.45823705 1.         1.        ]]\n",
      "i_s=180 , [[0.94845635 1.         1.        ]]\n",
      "i_s=181 , [[2.9658308 0.        1.       ]]\n",
      "i_s=182 , [[1.23745501 1.         1.        ]]\n",
      "i_s=183 , [[1.5019691 0.        1.       ]]\n",
      "i_s=184 , [[3.10232258 0.         0.        ]]\n",
      "i_s=185 , [[2.33705997 0.         1.        ]]\n",
      "i_s=186 , [[1.88523602 0.         1.        ]]\n",
      "i_s=187 , [[1.90329504 0.         1.        ]]\n",
      "i_s=188 , [[5.76220131 0.         0.        ]]\n",
      "i_s=189 , [[2.32236862 0.         1.        ]]\n",
      "i_s=190 , [[0.71231043 1.         1.        ]]\n",
      "i_s=191 , [[2.42314291 0.         1.        ]]\n",
      "i_s=192 , [[1.92931652 0.         1.        ]]\n",
      "i_s=193 , [[0.71203578 1.         1.        ]]\n",
      "i_s=194 , [[3.12159562 0.         0.        ]]\n",
      "i_s=195 , [[0.47056651 1.         1.        ]]\n",
      "i_s=196 , [[3.35263824 0.         0.        ]]\n",
      "i_s=197 , [[3.42709064 0.         0.        ]]\n",
      "i_s=198 , [[0.77352053 1.         1.        ]]\n",
      "i_s=199 , [[3.36887836 0.         0.        ]]\n",
      "i_s=200 , [[2.94384623 0.         0.        ]]\n",
      "i_s=201 , [[2.8899889 0.        1.       ]]\n",
      "i_s=202 , [[2.24276018 0.         1.        ]]\n",
      "i_s=203 , [[1.40120316 1.         1.        ]]\n",
      "i_s=204 , [[1.49672842 1.         1.        ]]\n",
      "i_s=205 , [[3.18846321 0.         0.        ]]\n",
      "i_s=206 , [[1.04423392 1.         1.        ]]\n",
      "i_s=207 , [[2.6024251 0.        1.       ]]\n",
      "i_s=208 , [[2.26955223 0.         1.        ]]\n",
      "i_s=209 , [[5.15035725 0.         0.        ]]\n",
      "i_s=210 , [[3.69228363 0.         1.        ]]\n",
      "i_s=211 , [[2.1743083 0.        1.       ]]\n",
      "i_s=212 , [[1.3380928 1.        1.       ]]\n",
      "i_s=213 , [[3.19377947 0.         0.        ]]\n",
      "i_s=214 , [[1.92666054 1.         1.        ]]\n",
      "i_s=215 , [[3.28464413 0.         0.        ]]\n",
      "i_s=216 , [[1.02463472 1.         1.        ]]\n",
      "i_s=217 , [[0.98335284 1.         1.        ]]\n",
      "i_s=218 , [[0.50326049 1.         1.        ]]\n",
      "i_s=219 , [[0.34138298 1.         1.        ]]\n",
      "i_s=220 , [[2.77374077 0.         0.        ]]\n",
      "i_s=221 , [[1.96957207 0.         1.        ]]\n",
      "i_s=222 , [[1.79095042 0.         1.        ]]\n",
      "i_s=223 , [[1.64250326 0.         1.        ]]\n",
      "i_s=224 , [[3.99100375 0.         0.        ]]\n",
      "i_s=225 , [[0.29386067 1.         1.        ]]\n",
      "i_s=226 , [[0.64237249 1.         1.        ]]\n",
      "i_s=227 , [[2.69645381 0.         1.        ]]\n",
      "i_s=228 , [[2.47352552 0.         1.        ]]\n",
      "i_s=229 , [[2.02300835 0.         1.        ]]\n",
      "i_s=230 , [[2.86497045 0.         1.        ]]\n",
      "i_s=231 , [[2.10990524 0.         1.        ]]\n",
      "i_s=232 , [[1.49576306 0.         1.        ]]\n",
      "i_s=233 , [[2.4677949 0.        1.       ]]\n",
      "i_s=234 , [[2.64677739 0.         1.        ]]\n",
      "i_s=235 , [[2.39166379 0.         1.        ]]\n",
      "i_s=236 , [[1.51918054 0.         1.        ]]\n",
      "i_s=237 , [[1.51999545 0.         1.        ]]\n",
      "i_s=238 , [[1.35891676 1.         1.        ]]\n",
      "i_s=239 , [[1.74553132 0.         1.        ]]\n",
      "i_s=240 , [[2.18751049 0.         1.        ]]\n",
      "i_s=241 , [[4.90302324 0.         0.        ]]\n",
      "i_s=242 , [[0.85348278 1.         1.        ]]\n",
      "i_s=243 , [[3.17685366 0.         1.        ]]\n",
      "i_s=244 , [[0.65530413 1.         1.        ]]\n",
      "i_s=245 , [[1.08742881 1.         1.        ]]\n",
      "i_s=246 , [[1.83454418 0.         1.        ]]\n",
      "i_s=247 , [[1.98052013 0.         1.        ]]\n",
      "i_s=248 , [[1.87617481 0.         1.        ]]\n",
      "i_s=249 , [[2.37941599 0.         1.        ]]\n",
      "i_s=250 , [[2.56389189 0.         1.        ]]\n",
      "i_s=251 , [[3.70877624 0.         0.        ]]\n",
      "i_s=252 , [[1.52895701 0.         1.        ]]\n",
      "i_s=253 , [[1.18670988 1.         1.        ]]\n",
      "i_s=254 , [[2.06546879 0.         1.        ]]\n",
      "i_s=255 , [[2.88404441 0.         0.        ]]\n",
      "i_s=256 , [[0.43022233 1.         1.        ]]\n",
      "i_s=257 , [[1.80239713 0.         1.        ]]\n",
      "i_s=258 , [[2.31224871 0.         1.        ]]\n",
      "i_s=259 , [[1.18552744 1.         1.        ]]\n",
      "i_s=260 , [[1.97447252 0.         1.        ]]\n",
      "i_s=261 , [[2.478374 0.       1.      ]]\n",
      "i_s=262 , [[1.33694351 1.         1.        ]]\n",
      "i_s=263 , [[1.35660732 1.         1.        ]]\n",
      "i_s=264 , [[2.2116046 0.        1.       ]]\n",
      "i_s=265 , [[0.53896666 1.         1.        ]]\n",
      "i_s=266 , [[0.87395608 1.         1.        ]]\n",
      "i_s=267 , [[0.26485801 1.         1.        ]]\n",
      "i_s=268 , [[1.5133239 1.        1.       ]]\n",
      "i_s=269 , [[2.1427784 0.        1.       ]]\n",
      "i_s=270 , [[3.4086864 0.        0.       ]]\n",
      "i_s=271 , [[2.79536295 0.         1.        ]]\n",
      "i_s=272 , [[3.99087858 0.         0.        ]]\n",
      "i_s=273 , [[1.62021136 0.         1.        ]]\n",
      "i_s=274 , [[1.27507436 0.         1.        ]]\n",
      "i_s=275 , [[3.10838652 0.         0.        ]]\n",
      "i_s=276 , [[0.50597668 1.         1.        ]]\n",
      "i_s=277 , [[1.4780364 0.        1.       ]]\n",
      "i_s=278 , [[1.79206228 0.         1.        ]]\n",
      "i_s=279 , [[0.25176892 1.         1.        ]]\n",
      "i_s=280 , [[3.53037357 0.         0.        ]]\n",
      "i_s=281 , [[1.11206961 1.         1.        ]]\n",
      "i_s=282 , [[0.53729284 1.         1.        ]]\n",
      "i_s=283 , [[0.89703804 1.         1.        ]]\n",
      "i_s=284 , [[0.24483088 1.         1.        ]]\n",
      "i_s=285 , [[2.45292425 0.         1.        ]]\n",
      "i_s=286 , [[0.66441411 1.         1.        ]]\n",
      "i_s=287 , [[2.7058301 0.        1.       ]]\n",
      "i_s=288 , [[1.32751691 1.         1.        ]]\n",
      "i_s=289 , [[3.69989872 0.         0.        ]]\n",
      "i_s=290 , [[1.02830911 1.         1.        ]]\n",
      "i_s=291 , [[1.55287445 1.         1.        ]]\n",
      "i_s=292 , [[1.55706835 0.         1.        ]]\n",
      "i_s=293 , [[1.48024309 0.         1.        ]]\n",
      "i_s=294 , [[3.94450426 0.         0.        ]]\n",
      "i_s=295 , [[0.33808458 1.         1.        ]]\n",
      "i_s=296 , [[2.65309381 0.         1.        ]]\n",
      "i_s=297 , [[1.17438936 1.         1.        ]]\n",
      "i_s=298 , [[1.12560153 1.         1.        ]]\n",
      "i_s=299 , [[2.61197948 0.         1.        ]]\n",
      "i_s=300 , [[1.80886567 0.         1.        ]]\n",
      "i_s=301 , [[1.75956798 0.         1.        ]]\n",
      "i_s=302 , [[1.80582416 0.         1.        ]]\n",
      "i_s=303 , [[5.04069424 0.         0.        ]]\n",
      "i_s=304 , [[4.42020178 0.         0.        ]]\n",
      "i_s=305 , [[3.17987275 0.         0.        ]]\n",
      "i_s=306 , [[0.93680191 1.         1.        ]]\n",
      "i_s=307 , [[1.75531936 0.         1.        ]]\n",
      "i_s=308 , [[3.04218102 0.         0.        ]]\n",
      "i_s=309 , [[3.62000608 0.         0.        ]]\n",
      "i_s=310 , [[5.94025612 0.         0.        ]]\n",
      "i_s=311 , [[3.17431951 0.         0.        ]]\n",
      "i_s=312 , [[2.30004191 0.         1.        ]]\n",
      "i_s=313 , [[2.51857758 0.         1.        ]]\n",
      "i_s=314 , [[1.66531169 0.         1.        ]]\n",
      "i_s=315 , [[1.26024115 1.         1.        ]]\n",
      "i_s=316 , [[1.77615964 0.         1.        ]]\n",
      "i_s=317 , [[1.24819672 1.         1.        ]]\n",
      "i_s=318 , [[2.58838129 0.         1.        ]]\n",
      "i_s=319 , [[0.53318757 1.         1.        ]]\n",
      "i_s=320 , [[2.54101205 0.         1.        ]]\n",
      "i_s=321 , [[2.74183083 0.         1.        ]]\n",
      "i_s=322 , [[1.63205528 1.         1.        ]]\n",
      "i_s=323 , [[1.50718355 1.         1.        ]]\n",
      "i_s=324 , [[2.814188 0.       0.      ]]\n",
      "i_s=325 , [[3.89767933 0.         0.        ]]\n",
      "i_s=326 , [[1.30862319 1.         1.        ]]\n",
      "i_s=327 , [[0.92578411 1.         1.        ]]\n",
      "i_s=328 , [[1.79790807 0.         1.        ]]\n",
      "i_s=329 , [[2.27902365 0.         1.        ]]\n",
      "i_s=330 , [[0.97710401 1.         1.        ]]\n",
      "i_s=331 , [[3.67347336 0.         0.        ]]\n",
      "i_s=332 , [[3.23552895 0.         1.        ]]\n",
      "i_s=333 , [[2.0532515 0.        1.       ]]\n",
      "i_s=334 , [[1.82433462 0.         1.        ]]\n",
      "i_s=335 , [[1.71368551 0.         1.        ]]\n",
      "i_s=336 , [[3.78780437 0.         0.        ]]\n",
      "i_s=337 , [[1.25625622 1.         1.        ]]\n",
      "i_s=338 , [[0.9599905 1.        1.       ]]\n",
      "i_s=339 , [[1.23667252 1.         1.        ]]\n",
      "i_s=340 , [[1.00358677 1.         1.        ]]\n",
      "i_s=341 , [[2.45850134 0.         1.        ]]\n",
      "i_s=342 , [[5.70331383 0.         0.        ]]\n",
      "i_s=343 , [[1.01561201 1.         1.        ]]\n",
      "i_s=344 , [[1.77354741 0.         1.        ]]\n",
      "i_s=345 , [[1.21449316 1.         1.        ]]\n",
      "i_s=346 , [[2.7659297 0.        1.       ]]\n",
      "i_s=347 , [[2.15704012 0.         1.        ]]\n",
      "i_s=348 , [[1.77663481 0.         1.        ]]\n",
      "i_s=349 , [[3.70567751 0.         0.        ]]\n",
      "i_s=350 , [[3.66964722 0.         0.        ]]\n",
      "i_s=351 , [[2.54731083 0.         0.        ]]\n",
      "i_s=352 , [[0.61884201 1.         1.        ]]\n",
      "i_s=353 , [[2.64625454 0.         0.        ]]\n",
      "i_s=354 , [[1.0249567 1.        1.       ]]\n",
      "i_s=355 , [[4.71035576 0.         0.        ]]\n",
      "i_s=356 , [[0.55126595 1.         1.        ]]\n",
      "i_s=357 , [[2.66670847 0.         0.        ]]\n",
      "i_s=358 , [[1.57189012 0.         1.        ]]\n",
      "i_s=359 , [[1.84502649 0.         1.        ]]\n",
      "i_s=360 , [[2.05887413 0.         1.        ]]\n",
      "i_s=361 , [[2.01129866 0.         1.        ]]\n",
      "i_s=362 , [[1.81037283 0.         1.        ]]\n",
      "i_s=363 , [[4.54231834 0.         0.        ]]\n",
      "i_s=364 , [[0.77923816 1.         1.        ]]\n",
      "i_s=365 , [[4.20253754 0.         0.        ]]\n",
      "i_s=366 , [[1.94619465 0.         1.        ]]\n",
      "i_s=367 , [[1.1087029 1.        1.       ]]\n",
      "i_s=368 , [[3.17252755 0.         0.        ]]\n",
      "i_s=369 , [[3.52703857 0.         1.        ]]\n",
      "i_s=370 , [[0.28577375 1.         1.        ]]\n",
      "i_s=371 , [[2.92718029 0.         0.        ]]\n",
      "i_s=372 , [[4.57109213 0.         0.        ]]\n",
      "i_s=373 , [[1.93306768 0.         1.        ]]\n",
      "i_s=374 , [[0.95758748 1.         1.        ]]\n",
      "i_s=375 , [[4.43864822 0.         0.        ]]\n",
      "i_s=376 , [[5.74217367 0.         0.        ]]\n",
      "i_s=377 , [[1.13590395 1.         1.        ]]\n",
      "i_s=378 , [[3.83814502 0.         0.        ]]\n",
      "i_s=379 , [[2.46470213 0.         1.        ]]\n",
      "i_s=380 , [[3.71827316 0.         0.        ]]\n",
      "i_s=381 , [[2.81087685 0.         0.        ]]\n",
      "i_s=382 , [[2.59752321 0.         0.        ]]\n",
      "i_s=383 , [[0.67816848 1.         1.        ]]\n",
      "i_s=384 , [[1.44163132 1.         1.        ]]\n",
      "i_s=385 , [[3.72445536 0.         0.        ]]\n",
      "i_s=386 , [[0.55455589 1.         1.        ]]\n",
      "i_s=387 , [[0.31908226 1.         1.        ]]\n",
      "i_s=388 , [[1.29166746 1.         1.        ]]\n",
      "i_s=389 , [[1.08699453 1.         1.        ]]\n",
      "i_s=390 , [[2.55697298 0.         1.        ]]\n",
      "i_s=391 , [[1.63613355 0.         1.        ]]\n",
      "i_s=392 , [[1.26422977 1.         1.        ]]\n",
      "i_s=393 , [[0.22256756 1.         1.        ]]\n",
      "i_s=394 , [[1.28837216 1.         1.        ]]\n",
      "i_s=395 , [[1.17290533 0.         1.        ]]\n",
      "i_s=396 , [[0.03219503 1.         1.        ]]\n",
      "i_s=397 , [[1.81939077 0.         1.        ]]\n",
      "i_s=398 , [[0.78403634 1.         1.        ]]\n",
      "i_s=399 , [[3.04624009 0.         0.        ]]\n",
      "i_s=400 , [[4.59443045 0.         0.        ]]\n",
      "i_s=401 , [[3.13618183 0.         0.        ]]\n",
      "i_s=402 , [[1.50453568 1.         1.        ]]\n",
      "i_s=403 , [[3.23402119 0.         0.        ]]\n",
      "i_s=404 , [[0.93923998 1.         1.        ]]\n",
      "i_s=405 , [[2.71623421 0.         0.        ]]\n",
      "i_s=406 , [[1.99771547 0.         1.        ]]\n",
      "i_s=407 , [[1.62950015 1.         1.        ]]\n",
      "i_s=408 , [[1.7824533 0.        1.       ]]\n",
      "i_s=409 , [[2.5979743 0.        1.       ]]\n",
      "i_s=410 , [[2.2417531 0.        1.       ]]\n",
      "i_s=411 , [[3.81300473 0.         0.        ]]\n",
      "i_s=412 , [[0.4156048 1.        1.       ]]\n",
      "i_s=413 , [[7.12755966 0.         0.        ]]\n",
      "i_s=414 , [[1.91005099 0.         1.        ]]\n",
      "i_s=415 , [[2.33973694 0.         1.        ]]\n",
      "i_s=416 , [[2.37103534 0.         1.        ]]\n",
      "i_s=417 , [[1.07467556 1.         1.        ]]\n",
      "i_s=418 , [[3.81498098 0.         0.        ]]\n",
      "i_s=419 , [[1.96151924 0.         1.        ]]\n",
      "i_s=420 , [[1.0613476 1.        1.       ]]\n",
      "i_s=421 , [[2.48395491 0.         0.        ]]\n",
      "i_s=422 , [[0.91316056 1.         1.        ]]\n",
      "i_s=423 , [[2.66017628 0.         1.        ]]\n",
      "i_s=424 , [[0.7406283 1.        1.       ]]\n",
      "i_s=425 , [[1.00436115 1.         1.        ]]\n",
      "i_s=426 , [[0.02980758 1.         1.        ]]\n",
      "i_s=427 , [[1.33656275 1.         1.        ]]\n",
      "i_s=428 , [[1.80866981 0.         1.        ]]\n",
      "i_s=429 , [[5.2450223 0.        0.       ]]\n",
      "i_s=430 , [[0.31629741 1.         1.        ]]\n",
      "i_s=431 , [[1.14052284 1.         1.        ]]\n",
      "i_s=432 , [[1.92892361 0.         1.        ]]\n",
      "i_s=433 , [[1.76196909 1.         1.        ]]\n",
      "i_s=434 , [[2.4134388 0.        1.       ]]\n",
      "i_s=435 , [[2.71707296 0.         1.        ]]\n",
      "i_s=436 , [[1.77947104 0.         1.        ]]\n",
      "i_s=437 , [[4.233634 0.       0.      ]]\n",
      "i_s=438 , [[3.09842587 0.         1.        ]]\n",
      "i_s=439 , [[1.77729881 1.         1.        ]]\n",
      "i_s=440 , [[2.74041581 0.         1.        ]]\n",
      "i_s=441 , [[0.87342155 1.         1.        ]]\n",
      "i_s=442 , [[2.20934725 0.         1.        ]]\n",
      "i_s=443 , [[3.57183266 0.         1.        ]]\n",
      "i_s=444 , [[0.95532608 0.         1.        ]]\n",
      "i_s=445 , [[2.73660183 0.         1.        ]]\n",
      "i_s=446 , [[0.95034039 1.         1.        ]]\n",
      "i_s=447 , [[2.53886938 0.         1.        ]]\n",
      "i_s=448 , [[1.47722542 0.         1.        ]]\n",
      "i_s=449 , [[0.7790345 1.        1.       ]]\n",
      "i_s=450 , [[2.59236717 0.         1.        ]]\n",
      "i_s=451 , [[0.2344625 1.        1.       ]]\n",
      "i_s=452 , [[1.2089299 1.        1.       ]]\n",
      "i_s=453 , [[0.26888093 1.         1.        ]]\n",
      "i_s=454 , [[0.95122242 1.         1.        ]]\n",
      "i_s=455 , [[1.84020686 0.         1.        ]]\n",
      "i_s=456 , [[1.38219512 0.         1.        ]]\n",
      "i_s=457 , [[2.28385305 0.         1.        ]]\n",
      "i_s=458 , [[1.53848207 1.         1.        ]]\n",
      "i_s=459 , [[6.14112186 0.         0.        ]]\n",
      "i_s=460 , [[3.350389 0.       0.      ]]\n",
      "i_s=461 , [[1.0900588 1.        1.       ]]\n",
      "i_s=462 , [[2.35656857 0.         1.        ]]\n",
      "i_s=463 , [[5.06042767 0.         0.        ]]\n",
      "i_s=464 , [[2.1646204 0.        1.       ]]\n",
      "i_s=465 , [[3.64783716 0.         0.        ]]\n",
      "i_s=466 , [[2.60395384 0.         1.        ]]\n",
      "i_s=467 , [[2.10678911 0.         1.        ]]\n",
      "i_s=468 , [[1.4606638 1.        1.       ]]\n",
      "i_s=469 , [[1.83238602 0.         1.        ]]\n",
      "i_s=470 , [[2.9031806 0.        0.       ]]\n",
      "i_s=471 , [[1.72722292 0.         1.        ]]\n",
      "i_s=472 , [[1.70736432 0.         1.        ]]\n",
      "i_s=473 , [[1.90384877 0.         1.        ]]\n",
      "i_s=474 , [[0.96331573 1.         1.        ]]\n",
      "i_s=475 , [[2.11643219 0.         1.        ]]\n",
      "i_s=476 , [[2.41801596 0.         1.        ]]\n",
      "i_s=477 , [[3.88601089 0.         0.        ]]\n",
      "i_s=478 , [[0.7218188 1.        1.       ]]\n",
      "i_s=479 , [[2.53130436 0.         1.        ]]\n",
      "i_s=480 , [[1.14043367 1.         1.        ]]\n",
      "i_s=481 , [[3.4309504 0.        0.       ]]\n",
      "i_s=482 , [[1.97809815 0.         1.        ]]\n",
      "i_s=483 , [[2.90766954 0.         1.        ]]\n",
      "i_s=484 , [[2.79460955 0.         1.        ]]\n",
      "i_s=485 , [[2.40718079 0.         1.        ]]\n",
      "i_s=486 , [[3.57805753 0.         0.        ]]\n",
      "i_s=487 , [[1.94234037 0.         1.        ]]\n",
      "i_s=488 , [[0.75128031 1.         1.        ]]\n",
      "i_s=489 , [[1.35302794 1.         1.        ]]\n",
      "i_s=490 , [[1.82829547 0.         1.        ]]\n",
      "i_s=491 , [[2.73660779 0.         1.        ]]\n",
      "i_s=492 , [[2.39249516 0.         1.        ]]\n",
      "i_s=493 , [[1.80837297 0.         1.        ]]\n",
      "i_s=494 , [[3.88611078 0.         0.        ]]\n",
      "i_s=495 , [[1.84002352 0.         1.        ]]\n",
      "i_s=496 , [[1.19176972 1.         1.        ]]\n",
      "i_s=497 , [[1.66917205 0.         1.        ]]\n",
      "i_s=498 , [[2.61855507 0.         1.        ]]\n",
      "i_s=499 , [[1.72862554 1.         1.        ]]\n",
      "i_s=500 , [[3.02256632 0.         0.        ]]\n",
      "i_s=501 , [[0.98024422 1.         1.        ]]\n",
      "i_s=502 , [[1.10689461 1.         1.        ]]\n",
      "i_s=503 , [[1.8410666 0.        1.       ]]\n",
      "i_s=504 , [[3.57751966 0.         0.        ]]\n",
      "i_s=505 , [[0.45530373 1.         1.        ]]\n",
      "i_s=506 , [[1.51293719 1.         1.        ]]\n",
      "i_s=507 , [[1.14210296 1.         1.        ]]\n",
      "i_s=508 , [[1.15668809 0.         1.        ]]\n",
      "i_s=509 , [[3.05950856 0.         0.        ]]\n",
      "i_s=510 , [[1.11722231 1.         1.        ]]\n",
      "i_s=511 , [[0.0291917 1.        1.       ]]\n",
      "i_s=512 , [[1.00384307 1.         1.        ]]\n",
      "i_s=513 , [[0.98788023 1.         1.        ]]\n",
      "i_s=514 , [[3.24070048 0.         0.        ]]\n",
      "i_s=515 , [[0.82405216 1.         1.        ]]\n",
      "i_s=516 , [[1.13928914 1.         1.        ]]\n",
      "i_s=517 , [[1.22604954 1.         1.        ]]\n",
      "i_s=518 , [[1.51275158 0.         1.        ]]\n",
      "i_s=519 , [[0.63902563 1.         1.        ]]\n",
      "i_s=520 , [[4.37182283 0.         0.        ]]\n",
      "i_s=521 , [[1.97279418 0.         1.        ]]\n",
      "i_s=522 , [[1.42214513 0.         1.        ]]\n",
      "i_s=523 , [[4.08484125 0.         0.        ]]\n",
      "i_s=524 , [[3.14128494 0.         0.        ]]\n",
      "i_s=525 , [[1.62230968 0.         1.        ]]\n",
      "i_s=526 , [[2.14320755 0.         1.        ]]\n",
      "i_s=527 , [[3.64883828 0.         1.        ]]\n",
      "i_s=528 , [[0.56609273 1.         1.        ]]\n",
      "i_s=529 , [[3.15989304 0.         0.        ]]\n",
      "i_s=530 , [[3.98567104 0.         0.        ]]\n",
      "i_s=531 , [[3.22368622 0.         0.        ]]\n",
      "i_s=532 , [[3.72051668 0.         0.        ]]\n",
      "i_s=533 , [[1.92468011 0.         1.        ]]\n",
      "i_s=534 , [[3.19113684 0.         1.        ]]\n",
      "i_s=535 , [[4.10566616 0.         0.        ]]\n",
      "i_s=536 , [[3.40007782 0.         1.        ]]\n",
      "i_s=537 , [[1.71532857 0.         1.        ]]\n",
      "i_s=538 , [[3.94472885 0.         0.        ]]\n",
      "i_s=539 , [[3.57875204 0.         1.        ]]\n",
      "i_s=540 , [[1.38649559 0.         1.        ]]\n",
      "i_s=541 , [[0.49628881 1.         1.        ]]\n",
      "i_s=542 , [[2.98791265 0.         0.        ]]\n",
      "i_s=543 , [[0.5610761 1.        1.       ]]\n",
      "i_s=544 , [[1.74463892 1.         1.        ]]\n",
      "i_s=545 , [[0.33408988 1.         1.        ]]\n",
      "i_s=546 , [[2.39785028 0.         1.        ]]\n",
      "i_s=547 , [[0.73723781 1.         1.        ]]\n",
      "i_s=548 , [[1.47457027 1.         1.        ]]\n",
      "i_s=549 , [[0.88057774 1.         1.        ]]\n",
      "i_s=550 , [[0.45090026 1.         1.        ]]\n",
      "i_s=551 , [[1.55071092 0.         1.        ]]\n",
      "i_s=552 , [[2.0356009 0.        1.       ]]\n",
      "i_s=553 , [[1.62943375 0.         1.        ]]\n",
      "i_s=554 , [[2.35075617 0.         1.        ]]\n",
      "i_s=555 , [[0.97725713 0.         1.        ]]\n",
      "i_s=556 , [[1.22426379 1.         1.        ]]\n",
      "i_s=557 , [[3.30046368 0.         0.        ]]\n",
      "i_s=558 , [[0.15348227 1.         1.        ]]\n",
      "i_s=559 , [[2.11755228 0.         1.        ]]\n",
      "i_s=560 , [[2.67179418 0.         0.        ]]\n",
      "i_s=561 , [[1.38595057 1.         1.        ]]\n",
      "i_s=562 , [[2.94308043 0.         1.        ]]\n",
      "i_s=563 , [[2.07393789 0.         1.        ]]\n",
      "i_s=564 , [[1.66566539 0.         1.        ]]\n",
      "i_s=565 , [[2.63415384 0.         0.        ]]\n",
      "i_s=566 , [[0.27823821 1.         1.        ]]\n",
      "i_s=567 , [[0.44770485 1.         1.        ]]\n",
      "i_s=568 , [[1.9217397 0.        1.       ]]\n",
      "i_s=569 , [[1.81152534 0.         1.        ]]\n",
      "i_s=570 , [[1.12699246 1.         1.        ]]\n",
      "i_s=571 , [[2.17374468 0.         1.        ]]\n",
      "i_s=572 , [[3.29609108 0.         0.        ]]\n",
      "i_s=573 , [[1.48775911 1.         1.        ]]\n",
      "i_s=574 , [[1.75735784 1.         1.        ]]\n",
      "i_s=575 , [[2.24790573 0.         1.        ]]\n",
      "i_s=576 , [[3.62400484 0.         0.        ]]\n",
      "i_s=577 , [[1.94856417 0.         1.        ]]\n",
      "i_s=578 , [[0.25331208 1.         1.        ]]\n",
      "i_s=579 , [[1.91647172 0.         1.        ]]\n",
      "i_s=580 , [[1.72159612 0.         1.        ]]\n",
      "i_s=581 , [[2.24817753 0.         1.        ]]\n",
      "i_s=582 , [[3.61095405 0.         0.        ]]\n",
      "i_s=583 , [[5.44043732 0.         0.        ]]\n",
      "i_s=584 , [[2.59036636 0.         1.        ]]\n",
      "i_s=585 , [[2.70103788 0.         1.        ]]\n",
      "i_s=586 , [[0.91834027 1.         1.        ]]\n",
      "i_s=587 , [[2.26873565 0.         1.        ]]\n",
      "i_s=588 , [[1.23272586 1.         1.        ]]\n",
      "i_s=589 , [[1.24977982 1.         1.        ]]\n",
      "i_s=590 , [[2.65973091 0.         1.        ]]\n",
      "i_s=591 , [[0.62089312 1.         1.        ]]\n",
      "i_s=592 , [[2.51552176 0.         1.        ]]\n",
      "i_s=593 , [[2.03402138 0.         1.        ]]\n",
      "i_s=594 , [[3.75083447 0.         0.        ]]\n",
      "i_s=595 , [[0.18250825 1.         1.        ]]\n",
      "i_s=596 , [[0.76862663 1.         1.        ]]\n",
      "i_s=597 , [[1.42133713 1.         1.        ]]\n",
      "i_s=598 , [[1.24102199 1.         1.        ]]\n",
      "i_s=599 , [[3.86917496 0.         0.        ]]\n",
      "i_s=600 , [[1.17342842 1.         1.        ]]\n",
      "i_s=601 , [[3.70329285 0.         0.        ]]\n",
      "i_s=602 , [[7.06895542 0.         0.        ]]\n",
      "i_s=603 , [[0.44759941 1.         1.        ]]\n",
      "i_s=604 , [[4.14219856 0.         0.        ]]\n",
      "i_s=605 , [[2.05337477 0.         1.        ]]\n",
      "i_s=606 , [[0.63817477 1.         1.        ]]\n",
      "i_s=607 , [[2.50605178 0.         1.        ]]\n",
      "i_s=608 , [[1.89719439 1.         1.        ]]\n",
      "i_s=609 , [[3.44212484 0.         0.        ]]\n",
      "i_s=610 , [[3.45069265 0.         0.        ]]\n",
      "i_s=611 , [[2.61286569 0.         1.        ]]\n",
      "i_s=612 , [[1.70144534 0.         1.        ]]\n",
      "i_s=613 , [[0.97918236 1.         1.        ]]\n",
      "i_s=614 , [[0.53316629 1.         1.        ]]\n",
      "i_s=615 , [[2.22936296 0.         1.        ]]\n",
      "i_s=616 , [[1.80318093 0.         1.        ]]\n",
      "i_s=617 , [[0.80565906 1.         1.        ]]\n",
      "i_s=618 , [[1.15248334 1.         1.        ]]\n",
      "i_s=619 , [[1.75117707 0.         1.        ]]\n",
      "i_s=620 , [[0.9508146 1.        1.       ]]\n",
      "i_s=621 , [[2.1050036 0.        1.       ]]\n",
      "i_s=622 , [[2.59091043 0.         1.        ]]\n",
      "i_s=623 , [[2.7369318 0.        1.       ]]\n",
      "i_s=624 , [[2.82445359 0.         0.        ]]\n",
      "i_s=625 , [[2.33578873 0.         1.        ]]\n",
      "i_s=626 , [[1.81560075 0.         1.        ]]\n",
      "i_s=627 , [[3.261446 0.       0.      ]]\n",
      "i_s=628 , [[0.46280271 1.         1.        ]]\n",
      "i_s=629 , [[0.49093589 1.         1.        ]]\n",
      "i_s=630 , [[1.97212434 0.         1.        ]]\n",
      "i_s=631 , [[3.47510672 0.         0.        ]]\n",
      "i_s=632 , [[2.71740246 0.         1.        ]]\n",
      "i_s=633 , [[2.48031664 0.         1.        ]]\n",
      "i_s=634 , [[1.82314086 0.         1.        ]]\n",
      "i_s=635 , [[2.03681469 0.         1.        ]]\n",
      "i_s=636 , [[7.68684483 0.         0.        ]]\n",
      "i_s=637 , [[3.97340822 0.         0.        ]]\n",
      "i_s=638 , [[2.02792645 0.         1.        ]]\n",
      "i_s=639 , [[0.90462929 1.         1.        ]]\n",
      "i_s=640 , [[1.03986382 1.         1.        ]]\n",
      "i_s=641 , [[2.88953733 0.         0.        ]]\n",
      "i_s=642 , [[3.57441783 0.         0.        ]]\n",
      "i_s=643 , [[2.34538674 0.         1.        ]]\n",
      "i_s=644 , [[2.49066806 0.         1.        ]]\n",
      "i_s=645 , [[1.40518308 1.         1.        ]]\n",
      "i_s=646 , [[4.31208944 0.         0.        ]]\n",
      "i_s=647 , [[2.14667392 0.         1.        ]]\n",
      "i_s=648 , [[2.46774626 0.         1.        ]]\n",
      "i_s=649 , [[1.22240114 1.         1.        ]]\n",
      "i_s=650 , [[4.6775713 0.        0.       ]]\n",
      "i_s=651 , [[1.81282616 0.         1.        ]]\n",
      "i_s=652 , [[1.3335793 0.        1.       ]]\n",
      "i_s=653 , [[2.09739923 0.         1.        ]]\n",
      "i_s=654 , [[1.64700818 0.         1.        ]]\n",
      "i_s=655 , [[2.16140461 0.         1.        ]]\n",
      "i_s=656 , [[1.39437163 0.         1.        ]]\n",
      "i_s=657 , [[1.39446473 0.         1.        ]]\n",
      "i_s=658 , [[3.54069471 0.         0.        ]]\n",
      "i_s=659 , [[2.90084481 0.         0.        ]]\n",
      "i_s=660 , [[1.41774487 0.         1.        ]]\n",
      "i_s=661 , [[0.62287199 1.         1.        ]]\n",
      "i_s=662 , [[1.66682601 0.         1.        ]]\n",
      "i_s=663 , [[2.99253321 0.         0.        ]]\n",
      "i_s=664 , [[1.32561839 1.         1.        ]]\n",
      "i_s=665 , [[1.85865533 0.         1.        ]]\n",
      "i_s=666 , [[2.24704456 0.         1.        ]]\n",
      "i_s=667 , [[0.32121876 1.         1.        ]]\n",
      "i_s=668 , [[1.31924927 1.         1.        ]]\n",
      "i_s=669 , [[1.23275197 0.         1.        ]]\n",
      "i_s=670 , [[2.77857685 0.         1.        ]]\n",
      "i_s=671 , [[2.21553135 0.         1.        ]]\n",
      "i_s=672 , [[1.96074009 1.         1.        ]]\n",
      "i_s=673 , [[0.69470572 1.         1.        ]]\n",
      "i_s=674 , [[1.20196283 1.         1.        ]]\n",
      "i_s=675 , [[3.7179935 0.        0.       ]]\n",
      "i_s=676 , [[1.97014642 0.         1.        ]]\n",
      "i_s=677 , [[2.52045083 0.         1.        ]]\n",
      "i_s=678 , [[1.17016101 1.         1.        ]]\n",
      "i_s=679 , [[2.14841485 0.         1.        ]]\n",
      "i_s=680 , [[3.16471815 0.         0.        ]]\n",
      "i_s=681 , [[5.89966488 0.         0.        ]]\n",
      "i_s=682 , [[1.96304953 0.         1.        ]]\n",
      "i_s=683 , [[2.62667274 0.         1.        ]]\n",
      "i_s=684 , [[1.13437247 1.         1.        ]]\n",
      "i_s=685 , [[0.47711331 1.         1.        ]]\n",
      "i_s=686 , [[0.4435105 1.        1.       ]]\n",
      "i_s=687 , [[2.09246874 0.         1.        ]]\n",
      "i_s=688 , [[0.42960501 1.         1.        ]]\n",
      "i_s=689 , [[1.61621809 0.         1.        ]]\n",
      "i_s=690 , [[1.25684226 1.         1.        ]]\n",
      "i_s=691 , [[0.63612199 1.         1.        ]]\n",
      "i_s=692 , [[1.51647592 0.         1.        ]]\n",
      "i_s=693 , [[0.89127421 1.         1.        ]]\n",
      "i_s=694 , [[6.75500441 0.         0.        ]]\n",
      "i_s=695 , [[2.62906742 0.         1.        ]]\n",
      "i_s=696 , [[2.54574442 0.         1.        ]]\n",
      "i_s=697 , [[0.95787215 1.         1.        ]]\n",
      "i_s=698 , [[3.42616749 0.         0.        ]]\n",
      "i_s=699 , [[3.23232818 0.         0.        ]]\n",
      "i_s=700 , [[2.83386397 0.         1.        ]]\n",
      "i_s=701 , [[1.5298692 1.        1.       ]]\n",
      "i_s=702 , [[4.84586573 0.         0.        ]]\n",
      "i_s=703 , [[0.63048542 1.         1.        ]]\n",
      "i_s=704 , [[2.18962765 0.         1.        ]]\n",
      "i_s=705 , [[1.78299224 0.         1.        ]]\n",
      "i_s=706 , [[1.74622595 0.         1.        ]]\n",
      "i_s=707 , [[3.19607759 0.         0.        ]]\n",
      "i_s=708 , [[1.33293092 1.         1.        ]]\n",
      "i_s=709 , [[2.40478659 0.         1.        ]]\n",
      "i_s=710 , [[1.82094789 0.         1.        ]]\n",
      "i_s=711 , [[3.05447435 0.         1.        ]]\n",
      "i_s=712 , [[4.36374712 0.         0.        ]]\n",
      "i_s=713 , [[0.18362033 1.         1.        ]]\n",
      "i_s=714 , [[0.46098149 1.         1.        ]]\n",
      "i_s=715 , [[3.05368614 0.         0.        ]]\n",
      "i_s=716 , [[1.67292571 0.         1.        ]]\n",
      "i_s=717 , [[1.23004651 0.         1.        ]]\n",
      "i_s=718 , [[2.64939666 0.         1.        ]]\n",
      "i_s=719 , [[3.22911072 0.         1.        ]]\n",
      "i_s=720 , [[5.61298466 0.         0.        ]]\n",
      "i_s=721 , [[3.22081757 0.         0.        ]]\n",
      "i_s=722 , [[4.50298405 0.         1.        ]]\n",
      "i_s=723 , [[1.09075284 1.         1.        ]]\n",
      "i_s=724 , [[4.31899452 0.         0.        ]]\n",
      "i_s=725 , [[0.89872098 1.         1.        ]]\n",
      "i_s=726 , [[1.75626659 0.         1.        ]]\n",
      "i_s=727 , [[0.59653783 1.         1.        ]]\n",
      "i_s=728 , [[0.79774398 1.         1.        ]]\n",
      "i_s=729 , [[1.42004955 1.         1.        ]]\n",
      "i_s=730 , [[0.47338235 1.         1.        ]]\n",
      "i_s=731 , [[4.88692284 0.         0.        ]]\n",
      "i_s=732 , [[2.66068482 0.         1.        ]]\n",
      "i_s=733 , [[1.65976226 1.         1.        ]]\n",
      "i_s=734 , [[1.44689918 1.         1.        ]]\n",
      "i_s=735 , [[0.99203569 0.         1.        ]]\n",
      "i_s=736 , [[1.24860513 1.         1.        ]]\n",
      "i_s=737 , [[4.01741409 0.         0.        ]]\n",
      "i_s=738 , [[0.35756779 1.         1.        ]]\n",
      "i_s=739 , [[1.42686343 1.         1.        ]]\n",
      "i_s=740 , [[1.18008327 1.         1.        ]]\n",
      "i_s=741 , [[2.05271268 0.         1.        ]]\n",
      "i_s=742 , [[2.16881466 0.         1.        ]]\n",
      "i_s=743 , [[0.90161896 1.         1.        ]]\n",
      "i_s=744 , [[2.37051821 0.         1.        ]]\n",
      "i_s=745 , [[1.1100266 1.        1.       ]]\n",
      "i_s=746 , [[2.1057601 0.        1.       ]]\n",
      "i_s=747 , [[2.99786997 0.         0.        ]]\n",
      "i_s=748 , [[1.8550936 0.        1.       ]]\n",
      "i_s=749 , [[1.9259361 0.        1.       ]]\n",
      "i_s=750 , [[3.48396015 0.         0.        ]]\n",
      "i_s=751 , [[4.6782155 0.        0.       ]]\n",
      "i_s=752 , [[3.55692649 0.         0.        ]]\n",
      "i_s=753 , [[0.62691343 1.         1.        ]]\n",
      "i_s=754 , [[0.2743755 1.        1.       ]]\n",
      "i_s=755 , [[3.97140622 0.         0.        ]]\n",
      "i_s=756 , [[1.3427217 1.        1.       ]]\n",
      "i_s=757 , [[0.38493362 1.         1.        ]]\n",
      "i_s=758 , [[2.18669558 0.         1.        ]]\n",
      "i_s=759 , [[1.67802238 0.         1.        ]]\n",
      "i_s=760 , [[0.96206999 1.         1.        ]]\n",
      "i_s=761 , [[0.89541978 1.         1.        ]]\n",
      "i_s=762 , [[4.28645086 0.         0.        ]]\n",
      "i_s=763 , [[1.75046539 0.         1.        ]]\n",
      "i_s=764 , [[0.44132456 1.         1.        ]]\n",
      "i_s=765 , [[0.75845098 1.         1.        ]]\n",
      "i_s=766 , [[0.59140551 1.         1.        ]]\n",
      "i_s=767 , [[0.72515553 1.         1.        ]]\n",
      "i_s=768 , [[3.34602714 0.         0.        ]]\n",
      "i_s=769 , [[0.51619053 1.         1.        ]]\n",
      "i_s=770 , [[2.51643038 0.         0.        ]]\n",
      "i_s=771 , [[2.27160025 0.         1.        ]]\n",
      "i_s=772 , [[1.98571503 0.         1.        ]]\n",
      "i_s=773 , [[3.34722996 0.         0.        ]]\n",
      "i_s=774 , [[1.85004163 0.         1.        ]]\n",
      "i_s=775 , [[2.642699 0.       1.      ]]\n",
      "i_s=776 , [[2.37398767 0.         1.        ]]\n",
      "i_s=777 , [[2.76152039 0.         0.        ]]\n",
      "i_s=778 , [[2.66595554 0.         1.        ]]\n",
      "i_s=779 , [[2.75398493 0.         0.        ]]\n",
      "i_s=780 , [[2.00393057 0.         1.        ]]\n",
      "i_s=781 , [[0.61359602 1.         1.        ]]\n",
      "i_s=782 , [[2.6364007 0.        1.       ]]\n",
      "i_s=783 , [[1.75310111 0.         1.        ]]\n",
      "i_s=784 , [[1.94900227 0.         1.        ]]\n",
      "i_s=785 , [[1.00652623 1.         1.        ]]\n",
      "i_s=786 , [[0.88102287 1.         1.        ]]\n",
      "i_s=787 , [[2.8293159 0.        1.       ]]\n",
      "i_s=788 , [[2.21630955 0.         1.        ]]\n",
      "i_s=789 , [[3.46247768 0.         0.        ]]\n",
      "i_s=790 , [[1.12528658 1.         1.        ]]\n",
      "i_s=791 , [[6.17500782 0.         0.        ]]\n",
      "i_s=792 , [[5.99558592 0.         0.        ]]\n",
      "i_s=793 , [[3.11140203 0.         0.        ]]\n",
      "i_s=794 , [[0.09156773 1.         1.        ]]\n",
      "i_s=795 , [[1.48239267 0.         1.        ]]\n",
      "i_s=796 , [[3.01009297 0.         1.        ]]\n",
      "i_s=797 , [[0.82909483 1.         1.        ]]\n",
      "i_s=798 , [[2.54758644 0.         1.        ]]\n",
      "i_s=799 , [[3.22368193 0.         0.        ]]\n",
      "i_s=800 , [[1.10131752 1.         1.        ]]\n",
      "i_s=801 , [[2.46561289 0.         1.        ]]\n",
      "i_s=802 , [[1.49768138 1.         1.        ]]\n",
      "i_s=803 , [[1.83190596 0.         1.        ]]\n",
      "i_s=804 , [[1.75673735 0.         1.        ]]\n",
      "i_s=805 , [[3.48643923 0.         0.        ]]\n",
      "i_s=806 , [[0.86623484 1.         1.        ]]\n",
      "i_s=807 , [[0.26229307 1.         1.        ]]\n",
      "i_s=808 , [[0.73268795 1.         1.        ]]\n",
      "i_s=809 , [[0.54008156 1.         1.        ]]\n",
      "i_s=810 , [[0.56460011 1.         1.        ]]\n",
      "i_s=811 , [[0.80972904 1.         1.        ]]\n",
      "i_s=812 , [[2.09011507 0.         1.        ]]\n",
      "i_s=813 , [[3.27016592 0.         1.        ]]\n",
      "i_s=814 , [[2.00763559 0.         1.        ]]\n",
      "i_s=815 , [[1.60311079 1.         1.        ]]\n",
      "i_s=816 , [[0.44363841 1.         1.        ]]\n",
      "i_s=817 , [[1.33648074 0.         1.        ]]\n",
      "i_s=818 , [[0.73384815 1.         1.        ]]\n",
      "i_s=819 , [[0.85212851 1.         1.        ]]\n",
      "i_s=820 , [[1.14968395 0.         1.        ]]\n",
      "i_s=821 , [[1.22572041 1.         1.        ]]\n",
      "i_s=822 , [[1.30415368 1.         1.        ]]\n",
      "i_s=823 , [[2.14617968 0.         1.        ]]\n",
      "i_s=824 , [[0.8944838 1.        1.       ]]\n",
      "i_s=825 , [[3.16227579 0.         0.        ]]\n",
      "i_s=826 , [[0.41730267 1.         1.        ]]\n",
      "i_s=827 , [[2.64491224 0.         0.        ]]\n",
      "i_s=828 , [[2.27822447 0.         1.        ]]\n",
      "i_s=829 , [[1.98187602 0.         1.        ]]\n",
      "i_s=830 , [[1.95509005 0.         1.        ]]\n",
      "i_s=831 , [[1.07666183 1.         1.        ]]\n",
      "i_s=832 , [[5.91580153 0.         0.        ]]\n",
      "i_s=833 , [[2.19470906 0.         1.        ]]\n",
      "i_s=834 , [[1.48210931 0.         1.        ]]\n",
      "i_s=835 , [[1.19619453 1.         1.        ]]\n",
      "i_s=836 , [[4.60536003 0.         0.        ]]\n",
      "i_s=837 , [[2.63869023 0.         1.        ]]\n",
      "i_s=838 , [[2.87906075 0.         1.        ]]\n",
      "i_s=839 , [[2.38172674 0.         0.        ]]\n",
      "i_s=840 , [[0.69535571 1.         1.        ]]\n",
      "i_s=841 , [[1.64686453 1.         1.        ]]\n",
      "i_s=842 , [[1.70735812 0.         1.        ]]\n",
      "i_s=843 , [[3.44278431 0.         0.        ]]\n",
      "i_s=844 , [[2.87454128 0.         0.        ]]\n",
      "i_s=845 , [[1.26542699 1.         1.        ]]\n",
      "i_s=846 , [[1.14392996 1.         1.        ]]\n",
      "i_s=847 , [[2.8059864 0.        0.       ]]\n",
      "i_s=848 , [[2.86982656 0.         1.        ]]\n",
      "i_s=849 , [[3.27418566 0.         0.        ]]\n",
      "i_s=850 , [[1.05003321 1.         1.        ]]\n",
      "i_s=851 , [[0.3943128 1.        1.       ]]\n",
      "i_s=852 , [[1.3843441 1.        1.       ]]\n",
      "i_s=853 , [[0.74498701 1.         1.        ]]\n",
      "i_s=854 , [[1.18318641 0.         1.        ]]\n",
      "i_s=855 , [[0.74920893 1.         1.        ]]\n",
      "i_s=856 , [[0.97868204 1.         1.        ]]\n",
      "i_s=857 , [[3.37834668 0.         0.        ]]\n",
      "i_s=858 , [[3.21485233 0.         0.        ]]\n",
      "i_s=859 , [[0.29659441 1.         1.        ]]\n",
      "i_s=860 , [[1.84465396 0.         1.        ]]\n",
      "i_s=861 , [[2.38269591 0.         1.        ]]\n",
      "i_s=862 , [[0.79522955 1.         1.        ]]\n",
      "i_s=863 , [[2.44210267 0.         1.        ]]\n",
      "i_s=864 , [[2.15830469 0.         1.        ]]\n",
      "i_s=865 , [[3.79928446 0.         0.        ]]\n",
      "i_s=866 , [[1.57676637 1.         1.        ]]\n",
      "i_s=867 , [[2.51220918 0.         1.        ]]\n",
      "i_s=868 , [[2.88220549 0.         0.        ]]\n",
      "i_s=869 , [[4.17151976 0.         0.        ]]\n",
      "i_s=870 , [[0.73288524 1.         1.        ]]\n",
      "i_s=871 , [[0.7940436 1.        1.       ]]\n",
      "i_s=872 , [[2.6353898 0.        1.       ]]\n",
      "i_s=873 , [[1.77289724 0.         1.        ]]\n",
      "i_s=874 , [[1.20553374 1.         1.        ]]\n",
      "i_s=875 , [[2.09478593 0.         1.        ]]\n",
      "i_s=876 , [[1.60987437 0.         1.        ]]\n",
      "i_s=877 , [[2.45121145 0.         0.        ]]\n",
      "i_s=878 , [[1.77082253 1.         1.        ]]\n",
      "i_s=879 , [[2.15605283 0.         1.        ]]\n",
      "i_s=880 , [[3.59358764 0.         1.        ]]\n",
      "i_s=881 , [[0.82142252 1.         1.        ]]\n",
      "i_s=882 , [[3.29505873 0.         0.        ]]\n",
      "i_s=883 , [[0.42755145 1.         1.        ]]\n",
      "i_s=884 , [[2.92761374 0.         0.        ]]\n",
      "i_s=885 , [[2.61769557 0.         1.        ]]\n",
      "i_s=886 , [[0.47812885 1.         1.        ]]\n",
      "i_s=887 , [[0.76116824 1.         1.        ]]\n",
      "i_s=888 , [[3.13412476 0.         0.        ]]\n",
      "i_s=889 , [[2.88680816 0.         1.        ]]\n",
      "i_s=890 , [[1.59930289 0.         1.        ]]\n",
      "i_s=891 , [[4.61819792 0.         0.        ]]\n",
      "i_s=892 , [[3.58728647 0.         0.        ]]\n",
      "i_s=893 , [[1.44345725 1.         1.        ]]\n",
      "i_s=894 , [[2.66868997 0.         1.        ]]\n",
      "i_s=895 , [[0.57297808 1.         1.        ]]\n",
      "i_s=896 , [[1.53316569 0.         1.        ]]\n",
      "i_s=897 , [[1.51860988 1.         1.        ]]\n",
      "i_s=898 , [[1.247051 0.       1.      ]]\n",
      "i_s=899 , [[2.47545981 0.         1.        ]]\n",
      "i_s=900 , [[1.74995565 0.         1.        ]]\n",
      "i_s=901 , [[1.02064717 1.         1.        ]]\n",
      "i_s=902 , [[0.49930042 1.         1.        ]]\n",
      "i_s=903 , [[5.12135077 0.         0.        ]]\n",
      "i_s=904 , [[1.53052628 1.         1.        ]]\n",
      "i_s=905 , [[3.44327736 0.         0.        ]]\n",
      "i_s=906 , [[3.6567378 0.        0.       ]]\n",
      "i_s=907 , [[2.58466339 0.         1.        ]]\n",
      "i_s=908 , [[3.14120913 0.         0.        ]]\n",
      "i_s=909 , [[1.58595848 1.         1.        ]]\n",
      "i_s=910 , [[2.01628971 0.         1.        ]]\n",
      "i_s=911 , [[3.06770945 0.         1.        ]]\n",
      "i_s=912 , [[1.63141119 0.         1.        ]]\n",
      "i_s=913 , [[2.23533821 0.         1.        ]]\n",
      "i_s=914 , [[2.74956465 0.         1.        ]]\n",
      "i_s=915 , [[2.13914061 0.         1.        ]]\n",
      "i_s=916 , [[1.37036514 1.         1.        ]]\n",
      "i_s=917 , [[1.75355339 0.         1.        ]]\n",
      "i_s=918 , [[1.12508941 1.         1.        ]]\n",
      "i_s=919 , [[1.2875911 1.        1.       ]]\n",
      "i_s=920 , [[1.4490757 1.        1.       ]]\n",
      "i_s=921 , [[3.27318382 0.         0.        ]]\n",
      "i_s=922 , [[0.24822873 1.         1.        ]]\n",
      "i_s=923 , [[1.21459639 1.         1.        ]]\n",
      "i_s=924 , [[2.47098422 0.         1.        ]]\n",
      "i_s=925 , [[2.61726284 0.         0.        ]]\n",
      "i_s=926 , [[2.18799138 0.         1.        ]]\n",
      "i_s=927 , [[2.41813421 0.         1.        ]]\n",
      "i_s=928 , [[2.86762381 0.         0.        ]]\n",
      "i_s=929 , [[1.93648434 0.         1.        ]]\n",
      "i_s=930 , [[3.71113539 0.         0.        ]]\n",
      "i_s=931 , [[2.89883256 0.         1.        ]]\n",
      "i_s=932 , [[4.44015265 0.         0.        ]]\n",
      "i_s=933 , [[1.37651038 0.         1.        ]]\n",
      "i_s=934 , [[1.50021756 1.         1.        ]]\n",
      "i_s=935 , [[0.53836846 1.         1.        ]]\n",
      "i_s=936 , [[2.2007308 0.        1.       ]]\n",
      "i_s=937 , [[2.09082365 0.         1.        ]]\n",
      "i_s=938 , [[4.21334696 0.         0.        ]]\n",
      "i_s=939 , [[0.98520947 1.         1.        ]]\n",
      "i_s=940 , [[4.4449563 0.        0.       ]]\n",
      "i_s=941 , [[0.99344909 1.         1.        ]]\n",
      "i_s=942 , [[2.19659948 0.         1.        ]]\n",
      "i_s=943 , [[1.41260254 1.         1.        ]]\n",
      "i_s=944 , [[0.21082085 1.         1.        ]]\n",
      "i_s=945 , [[1.94012177 0.         1.        ]]\n",
      "i_s=946 , [[2.97405624 0.         0.        ]]\n",
      "i_s=947 , [[1.71468675 0.         1.        ]]\n",
      "i_s=948 , [[2.5059123 0.        1.       ]]\n",
      "i_s=949 , [[1.31874382 1.         1.        ]]\n",
      "i_s=950 , [[2.21903849 1.         1.        ]]\n",
      "i_s=951 , [[3.57316685 0.         0.        ]]\n",
      "i_s=952 , [[5.21574116 0.         0.        ]]\n",
      "i_s=953 , [[2.66287804 0.         0.        ]]\n",
      "i_s=954 , [[2.1166172 0.        1.       ]]\n",
      "i_s=955 , [[0.22009631 1.         1.        ]]\n",
      "i_s=956 , [[2.99938583 0.         0.        ]]\n",
      "i_s=957 , [[0.82959211 1.         1.        ]]\n",
      "i_s=958 , [[0.29025871 1.         1.        ]]\n",
      "i_s=959 , [[1.99010742 0.         1.        ]]\n",
      "i_s=960 , [[1.05161166 1.         1.        ]]\n",
      "i_s=961 , [[0.23352827 1.         1.        ]]\n",
      "i_s=962 , [[0.71913439 1.         1.        ]]\n",
      "i_s=963 , [[0.47333974 1.         1.        ]]\n",
      "i_s=964 , [[3.6685648 0.        1.       ]]\n",
      "i_s=965 , [[1.41007364 1.         1.        ]]\n",
      "i_s=966 , [[2.16017246 0.         1.        ]]\n",
      "i_s=967 , [[1.51165497 1.         1.        ]]\n",
      "i_s=968 , [[1.79540873 0.         1.        ]]\n",
      "i_s=969 , [[1.91712224 0.         1.        ]]\n",
      "i_s=970 , [[0.3937521 1.        1.       ]]\n",
      "i_s=971 , [[1.90897894 0.         1.        ]]\n",
      "i_s=972 , [[1.59411061 0.         1.        ]]\n",
      "i_s=973 , [[4.03288126 0.         0.        ]]\n",
      "i_s=974 , [[2.61546636 0.         0.        ]]\n",
      "i_s=975 , [[2.92937899 0.         0.        ]]\n",
      "i_s=976 , [[2.06252575 0.         1.        ]]\n",
      "i_s=977 , [[3.25840378 0.         0.        ]]\n",
      "i_s=978 , [[0.93371177 1.         1.        ]]\n",
      "i_s=979 , [[2.55687308 0.         1.        ]]\n",
      "i_s=980 , [[2.95090723 0.         1.        ]]\n",
      "i_s=981 , [[4.00520229 0.         0.        ]]\n",
      "i_s=982 , [[0.36345914 1.         1.        ]]\n",
      "i_s=983 , [[1.15478003 1.         1.        ]]\n",
      "i_s=984 , [[2.87171245 0.         1.        ]]\n",
      "i_s=985 , [[1.53888333 1.         1.        ]]\n",
      "i_s=986 , [[1.51400018 1.         1.        ]]\n",
      "i_s=987 , [[1.72572339 0.         1.        ]]\n",
      "i_s=988 , [[1.10348785 1.         1.        ]]\n",
      "i_s=989 , [[0.76370913 1.         1.        ]]\n",
      "i_s=990 , [[2.00340939 0.         1.        ]]\n",
      "i_s=991 , [[3.17006111 0.         0.        ]]\n",
      "i_s=992 , [[5.57503366 0.         0.        ]]\n",
      "i_s=993 , [[1.76994252 0.         1.        ]]\n",
      "i_s=994 , [[0.66551614 1.         1.        ]]\n",
      "i_s=995 , [[0.94746125 1.         1.        ]]\n",
      "i_s=996 , [[0.79698074 1.         1.        ]]\n",
      "i_s=997 , [[1.07289052 1.         1.        ]]\n",
      "i_s=998 , [[3.22168922 0.         1.        ]]\n",
      "i_s=999 , [[0.72562951 1.         1.        ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2.11654385, 0.35      , 0.768     ])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####TEST DNN\n",
    "print('------------Building DNN model--------------')\n",
    "ShuffleInTraining=True\n",
    "N_EPOCHS_0=7\n",
    "N_EPOCHS=3\n",
    "N_HN_1=128\n",
    "N_HN=128\n",
    "N_LAYERS=1\n",
    "N_BATCH=64\n",
    "\n",
    "Rate_Val=0.8\n",
    "N_Val_OverSampler=int(np.around(N_AllTrain_OverSampler*Rate_Val))\n",
    "N_Train_OverSampler=int(N_AllTrain_OverSampler-N_Val_OverSampler)\n",
    "N_CLASS=len(allDF[\"Category\"].unique())\n",
    "input_dim=featuresArrayOverSampler.shape[1]\n",
    "output_dim=N_CLASS\n",
    "\n",
    "N_Split=1000\n",
    "BlockSize=int(np.floor(N_Val_OverSampler/N_Split))\n",
    "print('-----------------Build DNN model and start the 1st training!!---------------------')\n",
    "model = Sequential()\n",
    "model.add(Dense(N_HN_1,input_dim=input_dim))\n",
    "model.add(BatchNormalization())\n",
    "model.add(PReLU())\n",
    "for i in range(N_LAYERS):\n",
    "    model.add(Dense(N_HN))\n",
    "    model.add(BatchNormalization())   \n",
    "    model.add(PReLU())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(output_dim))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy', metrics.top_k_categorical_accuracy])\n",
    "labelsArrayOverSampler_1hot=keras.utils.to_categorical(LabelEncoder().fit_transform(np.array(labelsArrayOverSampler)), num_classes=N_CLASS)\n",
    "if ConsiderTime:\n",
    "    print('--------Spllit train val according to time!---------')\n",
    "    x_train=featuresArrayOverSampler[0:N_Train_OverSampler,:]\n",
    "    y_train=labelsArrayOverSampler_1hot[0:N_Train_OverSampler,:]\n",
    "else:\n",
    "    x_train,x_val,y_train,y_val = train_test_split(featuresArrayOverSampler,labelsArrayOverSampler_1hot,test_size=N_Val_OverSampler,train_size=N_Train_OverSampler, shuffle=True)\n",
    "# y_train = keras.utils.to_categorical(LabelEncoder().fit_transform(np.array(y_train)), num_classes=N_CLASS)\n",
    "print(str(x_train.shape))\n",
    "\n",
    "x_val_i=featuresArrayOverSampler[N_Train_OverSampler:N_Train_OverSampler+1,:]\n",
    "y_val_i=labelsArrayOverSampler_1hot[N_Train_OverSampler:N_Train_OverSampler+1,:]\n",
    "print(str(x_val_i.shape))\n",
    "\n",
    "# y_val_i = keras.utils.to_categorical(LabelEncoder().fit_transform(np.array(y_val_i)), num_classes=N_CLASS)\n",
    "print('------------DNN Training Go! Go! Go!!!!-----------')\n",
    "fitting=model.fit(x_train, y_train, epochs=N_EPOCHS_0, batch_size=N_BATCH,verbose=1,validation_data=(x_val_i,y_val_i),shuffle=True)\n",
    "# score01=model.predict(x_val_i, batch_size=None, verbose=0, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False)\n",
    "score0=model.evaluate(x=x_val_i, y=y_val_i, batch_size=None, verbose=1, sample_weight=None, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False)\n",
    "print(str(score0))\n",
    "print('-----------------Start the loop training!!---------------------')\n",
    "Scores_all=np.zeros([N_Split,3])\n",
    "for i_s in range(N_Split):\n",
    "    x_train=featuresArrayOverSampler[N_Train_OverSampler+i_s*BlockSize:N_Train_OverSampler+(i_s+1)*BlockSize,:]\n",
    "    y_train=labelsArrayOverSampler_1hot[N_Train_OverSampler+i_s*BlockSize:N_Train_OverSampler+(i_s+1)*BlockSize,:]\n",
    "    x_val_i=featuresArrayOverSampler[N_Train_OverSampler+(i_s+1)*BlockSize:N_Train_OverSampler+(i_s+1)*BlockSize+1,:]\n",
    "    y_val_i=labelsArrayOverSampler_1hot[N_Train_OverSampler+(i_s+1)*BlockSize:N_Train_OverSampler+(i_s+1)*BlockSize+1,:]\n",
    "    fitting=model.fit(x_train, y_train, epochs=N_EPOCHS, batch_size=N_BATCH,verbose=0,validation_data=(x_val_i,y_val_i),shuffle=True)\n",
    "    loss_i, acc_i, top5acc_i=model.evaluate(x=x_val_i, y=y_val_i, batch_size=None, verbose=0, sample_weight=None, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False)\n",
    "    print('i_s='+str(i_s)+' , '+str(np.array([[loss_i, acc_i, top5acc_i]])))\n",
    "    Scores_all[i_s,:]=np.array([[loss_i, acc_i, top5acc_i]])\n",
    "    \n",
    "np.mean(Scores_all, axis=0)    \n",
    "# y_val = keras.utils.to_categorical(LabelEncoder().fit_transform(np.array(y_val)), num_classes=N_CLASS)\n",
    "# \n",
    "# model.save('jjs_model_0124V3.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实验T7\\8\\9，5000Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------Building DNN model--------------\n",
      "-----------------Build DNN model and start the 1st training!!---------------------\n",
      "--------Spllit train val according to time!---------\n",
      "(162506, 110)\n",
      "(1, 110)\n",
      "------------DNN Training Go! Go! Go!!!!-----------\n",
      "Train on 162506 samples, validate on 1 samples\n",
      "Epoch 1/7\n",
      "162506/162506 [==============================] - 55s 338us/step - loss: 2.3322 - accuracy: 0.3052 - top_k_categorical_accuracy: 0.7169 - val_loss: 1.6945 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 2/7\n",
      "162506/162506 [==============================] - 53s 328us/step - loss: 2.1876 - accuracy: 0.3259 - top_k_categorical_accuracy: 0.7510 - val_loss: 1.7207 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 3/7\n",
      "162506/162506 [==============================] - 54s 329us/step - loss: 2.1689 - accuracy: 0.3290 - top_k_categorical_accuracy: 0.7547 - val_loss: 1.7543 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 4/7\n",
      "162506/162506 [==============================] - 54s 331us/step - loss: 2.1573 - accuracy: 0.3304 - top_k_categorical_accuracy: 0.7579 - val_loss: 1.5499 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 5/7\n",
      "162506/162506 [==============================] - 54s 334us/step - loss: 2.1478 - accuracy: 0.3337 - top_k_categorical_accuracy: 0.7610 - val_loss: 1.6387 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 6/7\n",
      "162506/162506 [==============================] - 54s 330us/step - loss: 2.1389 - accuracy: 0.3359 - top_k_categorical_accuracy: 0.7631 - val_loss: 1.6046 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 7/7\n",
      "162506/162506 [==============================] - 54s 333us/step - loss: 2.1310 - accuracy: 0.3367 - top_k_categorical_accuracy: 0.7648 - val_loss: 1.7346 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "[1.7346431016921997, 1.0, 1.0]\n",
      "-----------------Start the loop training!!---------------------\n",
      "i_s=0 , [[0.46827412 1.         1.        ]]\n",
      "i_s=500 , [[1.48951292 0.         1.        ]]\n",
      "i_s=1000 , [[1.26209486 0.         1.        ]]\n",
      "i_s=1500 , [[2.18416667 0.         1.        ]]\n",
      "i_s=2000 , [[6.23316622 0.         0.        ]]\n",
      "i_s=2500 , [[3.7906518 0.        0.       ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2.16264676, 0.35      , 0.76066667])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####TEST DNN\n",
    "print('------------Building DNN model--------------')\n",
    "ShuffleInTraining=True\n",
    "N_EPOCHS_0=7\n",
    "N_EPOCHS=3\n",
    "N_HN_1=128\n",
    "N_HN=128\n",
    "N_LAYERS=1\n",
    "N_BATCH=64\n",
    "\n",
    "Rate_Val=0.8\n",
    "N_Val_OverSampler=int(np.around(N_AllTrain_OverSampler*Rate_Val))\n",
    "N_Train_OverSampler=int(N_AllTrain_OverSampler-N_Val_OverSampler)\n",
    "N_CLASS=len(allDF[\"Category\"].unique())\n",
    "input_dim=featuresArrayOverSampler.shape[1]\n",
    "output_dim=N_CLASS\n",
    "\n",
    "N_Split=3000\n",
    "BlockSize=int(np.floor(N_Val_OverSampler/N_Split))\n",
    "print('-----------------Build DNN model and start the 1st training!!---------------------')\n",
    "model = Sequential()\n",
    "model.add(Dense(N_HN_1,input_dim=input_dim))\n",
    "model.add(BatchNormalization())\n",
    "model.add(PReLU())\n",
    "for i in range(N_LAYERS):\n",
    "    model.add(Dense(N_HN))\n",
    "    model.add(BatchNormalization())   \n",
    "    model.add(PReLU())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(output_dim))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy', metrics.top_k_categorical_accuracy])\n",
    "labelsArrayOverSampler_1hot=keras.utils.to_categorical(LabelEncoder().fit_transform(np.array(labelsArrayOverSampler)), num_classes=N_CLASS)\n",
    "if ConsiderTime:\n",
    "    print('--------Spllit train val according to time!---------')\n",
    "    x_train=featuresArrayOverSampler[0:N_Train_OverSampler,:]\n",
    "    y_train=labelsArrayOverSampler_1hot[0:N_Train_OverSampler,:]\n",
    "else:\n",
    "    x_train,x_val,y_train,y_val = train_test_split(featuresArrayOverSampler,labelsArrayOverSampler_1hot,test_size=N_Val_OverSampler,train_size=N_Train_OverSampler, shuffle=True)\n",
    "# y_train = keras.utils.to_categorical(LabelEncoder().fit_transform(np.array(y_train)), num_classes=N_CLASS)\n",
    "print(str(x_train.shape))\n",
    "\n",
    "x_val_i=featuresArrayOverSampler[N_Train_OverSampler:N_Train_OverSampler+1,:]\n",
    "y_val_i=labelsArrayOverSampler_1hot[N_Train_OverSampler:N_Train_OverSampler+1,:]\n",
    "print(str(x_val_i.shape))\n",
    "\n",
    "# y_val_i = keras.utils.to_categorical(LabelEncoder().fit_transform(np.array(y_val_i)), num_classes=N_CLASS)\n",
    "print('------------DNN Training Go! Go! Go!!!!-----------')\n",
    "fitting=model.fit(x_train, y_train, epochs=N_EPOCHS_0, batch_size=N_BATCH,verbose=1,validation_data=(x_val_i,y_val_i),shuffle=True)\n",
    "# score01=model.predict(x_val_i, batch_size=None, verbose=0, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False)\n",
    "score0=model.evaluate(x=x_val_i, y=y_val_i, batch_size=None, verbose=1, sample_weight=None, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False)\n",
    "print(str(score0))\n",
    "print('-----------------Start the loop training!!---------------------')\n",
    "Scores_all=np.zeros([N_Split,3])\n",
    "for i_s in range(N_Split):\n",
    "    x_train=featuresArrayOverSampler[N_Train_OverSampler+i_s*BlockSize:N_Train_OverSampler+(i_s+1)*BlockSize,:]\n",
    "    y_train=labelsArrayOverSampler_1hot[N_Train_OverSampler+i_s*BlockSize:N_Train_OverSampler+(i_s+1)*BlockSize,:]\n",
    "    x_val_i=featuresArrayOverSampler[N_Train_OverSampler+(i_s+1)*BlockSize:N_Train_OverSampler+(i_s+1)*BlockSize+1,:]\n",
    "    y_val_i=labelsArrayOverSampler_1hot[N_Train_OverSampler+(i_s+1)*BlockSize:N_Train_OverSampler+(i_s+1)*BlockSize+1,:]\n",
    "    fitting=model.fit(x_train, y_train, epochs=N_EPOCHS, batch_size=N_BATCH,verbose=0,validation_data=(x_val_i,y_val_i),shuffle=True)\n",
    "    loss_i, acc_i, top5acc_i=model.evaluate(x=x_val_i, y=y_val_i, batch_size=None, verbose=0, sample_weight=None, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False)\n",
    "    if i_s % 500==0:\n",
    "        print('i_s='+str(i_s)+' , '+str(np.array([[loss_i, acc_i, top5acc_i]])))\n",
    "    Scores_all[i_s,:]=np.array([[loss_i, acc_i, top5acc_i]])\n",
    "    \n",
    "np.mean(Scores_all, axis=0)    \n",
    "# y_val = keras.utils.to_categorical(LabelEncoder().fit_transform(np.array(y_val)), num_classes=N_CLASS)\n",
    "# \n",
    "# model.save('jjs_model_0124V3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 3ms/step\n",
      "[[6.4653861e-03 1.2605715e-01 3.7149497e-04 9.7461067e-05 4.2648759e-02\n",
      "  2.8877105e-03 9.5213682e-04 2.3450056e-02 1.6544767e-03 9.3566853e-04\n",
      "  1.0607555e-04 2.6104341e-03 6.1564622e-03 2.1268701e-02 3.2990030e-04\n",
      "  1.4005005e-03 8.2611606e-02 1.6742203e-03 9.3232881e-04 7.2806448e-02\n",
      "  9.3014769e-02 8.9774162e-02 2.5247902e-05 2.1651105e-03 1.4566442e-06\n",
      "  9.7008450e-03 4.3880912e-03 1.3907293e-03 6.0129119e-03 4.0470733e-04\n",
      "  1.4272645e-03 6.5214880e-04 4.4801380e-02 9.1407009e-07 7.7429931e-03\n",
      "  1.8327191e-01 8.3375528e-02 6.4030848e-02 1.2402017e-02]]\n",
      "[1.6967843770980835, 1.0, 1.0]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "score01=model.predict(x_val_i, batch_size=None, verbose=0, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False)\n",
    "score0=model.evaluate(x=x_val_i, y=y_val_i, batch_size=None, verbose=1, sample_weight=None, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False)\n",
    "print(str(score01))\n",
    "print(str(score0))\n",
    "print(y_val_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用打乱顺序的Train和Test集合来反向验证一下，前面的实验是否真的已经按照时间排序"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上实验的效果明显比按时间排序的要好4个百分点左右，反面证明了实验T1的有效性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=5.0\n",
    "b=3.0\n",
    "div = a // b \n",
    "div\n",
    "i_s=10000\n",
    "i_s % 500==0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  0., 444.,   0.,   0.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=np.array([[1, 2, 3]])\n",
    "print(a[0,1])\n",
    "n=np.zeros([1,4])\n",
    "n[0,1]=444\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
