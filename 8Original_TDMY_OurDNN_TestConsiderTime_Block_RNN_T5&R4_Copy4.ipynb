{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实验R3:引入LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "\n",
    "from keras import layers,metrics\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.layers import Dense, LSTM, Dropout, Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import PReLU, ELU\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.utils import np_utils, multi_gpu_model\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "from sklearn.utils import shuffle as reset\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV,StratifiedShuffleSplit\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import log_loss,make_scorer\n",
    "\n",
    "from matplotlib.colors import LogNorm\n",
    "# import \n",
    "from matplotlib.pylab import plt\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "from imblearn.over_sampling import RandomOverSampler #https://imbalanced-learn.org/stable/generated/imblearn.over_sampling.RandomOverSampler.html?highlight=randomoversampler\n",
    "from frplayer import FilterResponseNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_DataFrame(data, test_size=0.2, considerTime=True, random_state=None):\n",
    "    # ConsiderTime-------trainDF和testDF分割时是否考虑时间问题，即是否需要随机打乱。True:按照‘Dates’列进行降序排列,False：随机打乱样本的顺序，\n",
    "    if considerTime:\n",
    "        data=data.sort_values(by=\"Dates\", ascending=True)\n",
    "    else:\n",
    "        data=reset(data, random_state=random_state)\n",
    "    train=data[int(len(data)*test_size):].reset_index(drop=True)\n",
    "    test=data[:int(len(data)*test_size)].reset_index(drop=True)\n",
    "    return train, test\n",
    "\n",
    "def comparePlusMultResult(scoreDNN,scoreRNN,Y):\n",
    "    scorePrePlus=np.add(scoreDNN,scoreRNN)\n",
    "    scorePreMult=np.multiply(scoreDNN,scoreRNN)\n",
    "    \n",
    "    M=np.nanmax(scorePrePlus)\n",
    "    A=np.where(scorePrePlus == M, 1, scorePrePlus)\n",
    "    Plus_One=np.where(A < 1.0, 0, A)\n",
    "    \n",
    "    M=np.nanmax(scorePreMult)\n",
    "    A=np.where(scorePreMult == M, 1, scorePreMult)\n",
    "    Mult_One=np.where(A < 1.0, 0, A)    \n",
    "    \n",
    "    Plus_Result=0.0\n",
    "    Mult_Result=0.0\n",
    "    if np.sum(np.abs(np.array(Plus_One)-np.array(Y)))==0.0:\n",
    "        Plus_Result=1.0\n",
    "    if np.sum(np.abs(np.array(Mult_One)-np.array(Y)))==0.0:\n",
    "        Plus_Result=1.0\n",
    "    return Plus_Result, Mult_Result\n",
    "\n",
    "def compareResult(scoreNN,Y):\n",
    "    M=np.nanmax(scoreNN)\n",
    "    A=np.where(scoreNN == M, 1, scoreNN)\n",
    "    B=np.where(A < 1.0, 0, A)    \n",
    "    Result=0.0\n",
    "#     print(B)\n",
    "#     print(Y)\n",
    "#     print(np.sum(np.abs(np.array(B)-np.array(Y))))\n",
    "    if np.sum(np.abs(np.array(B)-np.array(Y)))==0.0:\n",
    "        Result=1.0\n",
    "    return Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_time(x):\n",
    "    if '-' in x:\n",
    "        DD=datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\")#jjs\n",
    "    else:\n",
    "        DD=datetime.strptime(x,\"%Y/%m/%d %H:%M\")#zj    \n",
    "    time=DD.hour#*60+DD.minute\n",
    "    day=DD.day\n",
    "    month=DD.month\n",
    "    year=DD.year\n",
    "    return time,day,month,year\n",
    "def Dates2TDMY(x):\n",
    "    if '-' in x:\n",
    "        DD=datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\")#jjs\n",
    "    else:\n",
    "        DD=datetime.strptime(x,\"%Y/%m/%d %H:%M\")#zj  \n",
    "    time=DD.hour#*60+DD.minute\n",
    "    day=DD.day\n",
    "    month=DD.month\n",
    "    year=DD.year\n",
    "    #T_D_M_Y=str(time)+str(day)+str(month)+str(year)\n",
    "    T_D_M_Y=str(time)+str(day)+str(month)\n",
    "    return T_D_M_Y\n",
    "def get_season(x):\n",
    "    summer=0\n",
    "    fall=0\n",
    "    winter=0\n",
    "    spring=0\n",
    "    if (x in [5, 6, 7]):\n",
    "        summer=1\n",
    "    if (x in [8, 9, 10]):\n",
    "        fall=1\n",
    "    if (x in [11, 0, 1]):\n",
    "        winter=1\n",
    "    if (x in [2, 3, 4]):\n",
    "        spring=1\n",
    "    return summer, fall, winter, spring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def field2Vec(trainDF,testDF,fieldStr):\n",
    "    fields=sorted(trainDF[fieldStr].unique())\n",
    "    categories=sorted(trainDF[\"Category\"].unique())\n",
    "    C_counts=trainDF.groupby([\"Category\"]).size()\n",
    "    F_C_counts=trainDF.groupby([fieldStr,\"Category\"]).size()\n",
    "    F_counts=trainDF.groupby([fieldStr]).size()\n",
    "    logodds={}\n",
    "    logoddsPF={}\n",
    "    MIN_CAT_COUNTS=2\n",
    "    default_logodds=np.log(C_counts/len(trainDF))-np.log(1.0-C_counts/float(len(trainDF)))\n",
    "    for f in fields:\n",
    "        PA=F_counts[f]/float(len(trainDF))\n",
    "        logoddsPF[f]=np.log(PA)-np.log(1.-PA)\n",
    "        logodds[f]=deepcopy(default_logodds)\n",
    "        for cat in F_C_counts[f].keys():\n",
    "            if (F_C_counts[f][cat]>MIN_CAT_COUNTS) and F_C_counts[f][cat]<F_counts[f]:\n",
    "                PA=F_C_counts[f][cat]/float(F_counts[f])\n",
    "                logodds[f][categories.index(cat)]=np.log(PA)-np.log(1.0-PA)\n",
    "        logodds[f]=pd.Series(logodds[f])\n",
    "        logodds[f].index=range(len(categories))\n",
    "    ########此部分代码，从逻辑上不应该出现在此处，但是为了编程的方便，放在了此处#########\n",
    "    #fieldsTest=sorted(testDF[fieldStr].unique())\n",
    "    #N_count=0\n",
    "    #for f in fieldsTest:\n",
    "        #if f not in fields:\n",
    "            #logoddsPF[f]=-50.0  #np.log(0.)-np.log(1.)=-inf,便于计算，改为-99999.0\n",
    "            #logodds[f]=deepcopy(default_logodds)\n",
    "            #pa=1.0/float(len(categories))\n",
    "            #logodds[f][range(len(categories))]=np.log(pa)-np.log(1.0-pa)\n",
    "            #logodds[f]=pd.Series(logodds[f])\n",
    "            #logodds[f].index=range(len(categories))\n",
    "            #N_count=N_count+1\n",
    "    #print(fieldStr+' N_count: '+str(N_count))\n",
    "    ########此部分代码，从逻辑上不应该出现在此处，但是为了编程的方便，放在了此处#########\n",
    "    #引进代码原作者的新思想\n",
    "    if testDF.shape[0]>0: #如果testDF里有样本,......\n",
    "        print('There are some new:'+fieldStr)\n",
    "        new_fields=sorted(testDF[fieldStr].unique())\n",
    "        new_F_counts=testDF.groupby(fieldStr).size()\n",
    "        only_new=set(new_fields+fields)-set(fields)\n",
    "        only_old=set(new_fields+fields)-set(new_fields)\n",
    "        in_both=set(new_fields).intersection(fields)\n",
    "        print('# only_new_fieldds:'+str(len(only_new)))\n",
    "        for f in only_new:\n",
    "            PA=new_F_counts[f]/float(len(testDF)+len(trainDF))\n",
    "            logoddsPF[f]=np.log(PA)-np.log(1.-PA)\n",
    "            logodds[f]=deepcopy(default_logodds)\n",
    "            logodds[f].index=range(len(categories))\n",
    "        for f in in_both:\n",
    "            PA=(F_counts[f]+new_F_counts[f])/float(len(testDF)+len(trainDF))\n",
    "            logoddsPF[f]=np.log(PA)-np.log(1.-PA)    \n",
    "    return logodds,logoddsPF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data(df,logodds_A,logoddsPF_A,logodds_T,logoddsPF_T,needT_D_M_Y=False):\n",
    "    feature_list=df.columns.tolist()\n",
    "    if \"Descript\" in feature_list:\n",
    "        feature_list.remove(\"Descript\")\n",
    "    if \"Resolution\" in feature_list:\n",
    "        feature_list.remove(\"Resolution\")\n",
    "    if \"Category\" in feature_list:\n",
    "        feature_list.remove(\"Category\")\n",
    "    if \"Id\" in feature_list:\n",
    "        feature_list.remove(\"Id\")\n",
    "\n",
    "    cleanData=df[feature_list]\n",
    "    cleanData.index=range(len(df))\n",
    "    print(\"Creating address features\")###Creating address features###\n",
    "    address_features=cleanData[\"Address\"].apply(lambda x: logodds_A[x])\n",
    "    address_features.columns=[\"logodds_A\"+str(x) for x in range(len(address_features.columns))]\n",
    "    if needT_D_M_Y:\n",
    "        print(\"Creating time T_D_M_Y features\")###Creating time T_D_M_Y features###\n",
    "        T_D_M_Y_features=cleanData[\"T_D_M_Y\"].apply(lambda xx: logodds_T[xx])\n",
    "        T_D_M_Y_features.columns=[\"logodds_T\"+str(xx) for xx in range(len(T_D_M_Y_features.columns))]\n",
    "\n",
    "    print(\"Parsing dates\")            ###Creating address features###\n",
    "    cleanData[\"Time\"], cleanData[\"Day\"], cleanData[\"Month\"], cleanData[\"Year\"]=zip(*cleanData[\"Dates\"].apply(parse_time))\n",
    "    #     dummy_ranks_DAY = pd.get_dummies(cleanData['DayOfWeek'], prefix='DAY')\n",
    "    days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "    #     cleanData[\"DayOfWeek\"]=cleanData[\"DayOfWeek\"].apply(lambda x: days.index(x)/float(len(days)))\n",
    "    print(\"Creating one-hot variables\")\n",
    "    dummy_ranks_PD = pd.get_dummies(cleanData['PdDistrict'], prefix='PD')\n",
    "    dummy_ranks_DAY = pd.get_dummies(cleanData[\"DayOfWeek\"], prefix='DAY')\n",
    "    cleanData[\"IsInterection\"]=cleanData[\"Address\"].apply(lambda x: 1 if \"/\" in x else 0)\n",
    "    cleanData[\"logoddsPF_A\"]=cleanData[\"Address\"].apply(lambda x: logoddsPF_A[x])\n",
    "    if needT_D_M_Y:\n",
    "        cleanData[\"logoddsPF_T\"]=cleanData[\"T_D_M_Y\"].apply(lambda x: logoddsPF_T[x])\n",
    "    print(\"droping processed columns\")\n",
    "    cleanData=cleanData.drop(\"PdDistrict\",axis=1)\n",
    "    cleanData=cleanData.drop(\"DayOfWeek\",axis=1)\n",
    "    cleanData=cleanData.drop(\"Address\",axis=1)    \n",
    "    cleanData=cleanData.drop(\"Dates\",axis=1)\n",
    "    if needT_D_M_Y:\n",
    "        cleanData=cleanData.drop(\"T_D_M_Y\",axis=1)\n",
    "    feature_list=cleanData.columns.tolist()\n",
    "    print(\"joining one-hot features\")\n",
    "    if needT_D_M_Y:\n",
    "        features = cleanData[feature_list].join(dummy_ranks_PD.iloc[:,:]).join(dummy_ranks_DAY.iloc[:,:]).join(address_features.iloc[:,:]).join(T_D_M_Y_features.iloc[:,:])\n",
    "    else:\n",
    "        features = cleanData[feature_list].join(dummy_ranks_PD.iloc[:,:]).join(dummy_ranks_DAY.iloc[:,:]).join(address_features.iloc[:,:])\n",
    "    print(\"creating new features\")\n",
    "    features[\"IsDup\"]=pd.Series(features.duplicated()|features.duplicated(keep='last')).apply(int)\n",
    "    features[\"Awake\"]=features[\"Time\"].apply(lambda x: 1 if (x==0 or (x>=8 and x<=23)) else 0)\n",
    "    features[\"Summer\"], features[\"Fall\"], features[\"Winter\"], features[\"Spring\"]=zip(*features[\"Month\"].apply(get_season))\n",
    "    if \"Category\" in df.columns:\n",
    "        labels = df[\"Category\"].astype('category')\n",
    "    else:\n",
    "        labels=None\n",
    "    return features,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(X, Y, lookback, delay, min_index, max_index, shuffle=False, batch_size=128, step=6):\n",
    "    if max_index is None:\n",
    "        max_index = len(X) - delay - 1\n",
    "    i = min_index + lookback\n",
    "    while 1:\n",
    "        if shuffle:\n",
    "            rows = np.random.randint(min_index + lookback, max_index, size=batch_size)#数值在[low, high)区间。\n",
    "        else:\n",
    "            if i + batch_size >= max_index:\n",
    "                i = min_index + lookback\n",
    "            rows = np.arange(i, min(i + batch_size, max_index))\n",
    "            i += len(rows)\n",
    "\n",
    "        samples = np.zeros((len(rows), lookback // step, X.shape[-1]))\n",
    "        targets = np.zeros((len(rows),Y.shape[1]))\n",
    "        for j, row in enumerate(rows):\n",
    "            indices = range(rows[j] - lookback, rows[j], step)\n",
    "            samples[j] = X[indices]\n",
    "            targets[j] = Y[rows[j]+delay]\n",
    "            #print('In the Generator,   shape of samples is : ' +str(samples.shape)) \n",
    "            #print('In the Generator Funciotn, samples j is : '+str(indices))\n",
    "            #print('In the Generator Funciotn, targets j is : '+str(rows[j]+delay))\n",
    "#         print('# row of Val: '+str(targets.shape[0]))###Tian\n",
    "        \n",
    "        yield samples, targets\n",
    "    #Now here is the data generator that we will use. It yields a tuple (samples, targets) where samples is one batch of input data and targets is the corresponding array of target temperatures. It takes the following arguments:\n",
    "        # •data: The original array of floating point data, which we just normalized in the code snippet above.\n",
    "        # •lookback: How many timesteps back should our input data go.\n",
    "        # •delay: How many timesteps in the future should our target be.\n",
    "        # •min_index and max_index: Indices in the data array that delimit which timesteps to draw from. This is useful for keeping a segment of the data for validation and another one for testing.\n",
    "        # •shuffle: Whether to shuffle our samples or draw them in chronological order.\n",
    "        # •batch_size: The number of samples per batch.\n",
    "        # •step: The period, in timesteps, at which we sample data. We will set it 6 in order to draw one data point every hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_Val(X, Y, lookback, delay, min_index, max_index, shuffle=False, batch_size=128, step=6):\n",
    "    if max_index is None:\n",
    "        max_index = len(X) - delay - 1\n",
    "    i = min_index + lookback\n",
    "    while 1:\n",
    "        if shuffle:\n",
    "            rows = np.random.randint(min_index + lookback, max_index, size=batch_size)#数值在[low, high)区间。\n",
    "        else:\n",
    "            if i + batch_size >= max_index:\n",
    "                i = min_index + lookback\n",
    "            rows = np.arange(i, min(i + batch_size, max_index))\n",
    "            i += len(rows)\n",
    "\n",
    "        samples = np.zeros((len(rows), lookback // step, X.shape[-1]))\n",
    "        targets = np.zeros((len(rows),Y.shape[1]))\n",
    "        for j, row in enumerate(rows):\n",
    "            indices = range(rows[j] - lookback, rows[j], step)\n",
    "            samples[j] = X[indices]\n",
    "            targets[j] = Y[rows[j]+delay]\n",
    "            #print('In the Generator Val,   shape of samples is : '+str(samples.shape)) \n",
    "            #print('In the Generator Val Funciotn, samples j is : '+str(indices))\n",
    "            #print('In the Generator Val Funciotn, targets j is : '+str(rows[j]+delay))\n",
    "#         print('# row of Val: '+str(targets.shape[0]))###Tian\n",
    "        \n",
    "        yield samples, targets\n",
    "    #Now here is the data generator that we will use. It yields a tuple (samples, targets) where samples is one batch of input data and targets is the corresponding array of target temperatures. It takes the following arguments:\n",
    "        # •data: The original array of floating point data, which we just normalized in the code snippet above.\n",
    "        # •lookback: How many timesteps back should our input data go.\n",
    "        # •delay: How many timesteps in the future should our target be.\n",
    "        # •min_index and max_index: Indices in the data array that delimit which timesteps to draw from. This is useful for keeping a segment of the data for validation and another one for testing.\n",
    "        # •shuffle: Whether to shuffle our samples or draw them in chronological order.\n",
    "        # •batch_size: The number of samples per batch.\n",
    "        # •step: The period, in timesteps, at which we sample data. We will set it 6 in order to draw one data point every hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of OrginalAllDF: (878049, 9)\n",
      "The shape of AllDF after del wrong X and Y values: (877982, 9)\n",
      "The shape of AllDF after drop_duplicates: (812529, 9)\n",
      "(689038, 2)\n",
      "Address_counts_allDF_trainDF_testDF: 23191_23191_0\n",
      "The # of AllDF, AllTrain, AllTest, is: 812529,812529,0\n",
      "-----------LOGOODS: Address-------------\n",
      "-----------LOGOODS: T_D_M_Y-------------\n",
      "-----------LOGOODS: parse_data of Alltrain  -------------\n",
      "Creating address features\n",
      "Creating time T_D_M_Y features\n",
      "Parsing dates\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating one-hot variables\n",
      "droping processed columns\n",
      "joining one-hot features\n",
      "creating new features\n",
      "['X', 'Y', 'Time', 'Day', 'Month', 'Year', 'IsInterection', 'logoddsPF_A', 'logoddsPF_T', 'PD_BAYVIEW', 'PD_CENTRAL', 'PD_INGLESIDE', 'PD_MISSION', 'PD_NORTHERN', 'PD_PARK', 'PD_RICHMOND', 'PD_SOUTHERN', 'PD_TARAVAL', 'PD_TENDERLOIN', 'DAY_Friday', 'DAY_Monday', 'DAY_Saturday', 'DAY_Sunday', 'DAY_Thursday', 'DAY_Tuesday', 'DAY_Wednesday', 'logodds_A0', 'logodds_A1', 'logodds_A2', 'logodds_A3', 'logodds_A4', 'logodds_A5', 'logodds_A6', 'logodds_A7', 'logodds_A8', 'logodds_A9', 'logodds_A10', 'logodds_A11', 'logodds_A12', 'logodds_A13', 'logodds_A14', 'logodds_A15', 'logodds_A16', 'logodds_A17', 'logodds_A18', 'logodds_A19', 'logodds_A20', 'logodds_A21', 'logodds_A22', 'logodds_A23', 'logodds_A24', 'logodds_A25', 'logodds_A26', 'logodds_A27', 'logodds_A28', 'logodds_A29', 'logodds_A30', 'logodds_A31', 'logodds_A32', 'logodds_A33', 'logodds_A34', 'logodds_A35', 'logodds_A36', 'logodds_A37', 'logodds_A38', 'logodds_T0', 'logodds_T1', 'logodds_T2', 'logodds_T3', 'logodds_T4', 'logodds_T5', 'logodds_T6', 'logodds_T7', 'logodds_T8', 'logodds_T9', 'logodds_T10', 'logodds_T11', 'logodds_T12', 'logodds_T13', 'logodds_T14', 'logodds_T15', 'logodds_T16', 'logodds_T17', 'logodds_T18', 'logodds_T19', 'logodds_T20', 'logodds_T21', 'logodds_T22', 'logodds_T23', 'logodds_T24', 'logodds_T25', 'logodds_T26', 'logodds_T27', 'logodds_T28', 'logodds_T29', 'logodds_T30', 'logodds_T31', 'logodds_T32', 'logodds_T33', 'logodds_T34', 'logodds_T35', 'logodds_T36', 'logodds_T37', 'logodds_T38', 'IsDup', 'Awake', 'Summer', 'Fall', 'Winter', 'Spring']\n",
      "110\n",
      "------------Attention: we do not RandomOverSampler---------------\n",
      "------------ConsiderTime:  Sorting--------------\n"
     ]
    }
   ],
   "source": [
    "#Import data\n",
    "ConsiderTime=True#False# True##trainDF和testDF分割时是否考虑时间问题，即是否需要随机打乱。True:按照‘Dates’列进行降序排列,False：随机打乱样本的顺序，\n",
    "Rate_ALL=0.0 #0.0即不保留测试机\n",
    "needOverSampler=False\n",
    "needT_D_M_Y=True #False  使用_T_D_M_Y和周几\n",
    "allDF=pd.read_csv(\"./train_addrCorrect.csv\")\n",
    "print('The shape of OrginalAllDF: '+str(allDF.shape))\n",
    "\n",
    "xy_scaler=preprocessing.StandardScaler()\n",
    "xy_scaler.fit(allDF[[\"X\",\"Y\"]])\n",
    "allDF[[\"X\",\"Y\"]]=xy_scaler.transform(allDF[[\"X\",\"Y\"]])\n",
    "allDF=allDF[abs(allDF[\"Y\"])<100]\n",
    "allDF.index=range(len(allDF))\n",
    "print('The shape of AllDF after del wrong X and Y values: '+str(allDF.shape))\n",
    "\n",
    "def listCat(x):\n",
    "    return list(x)\n",
    "allDF.drop_duplicates(inplace=True,subset=['Dates', 'DayOfWeek', 'PdDistrict', 'Address', 'X', 'Y', 'Category'])\n",
    "Train_duplicated=pd.pivot_table(allDF,index=['Dates','DayOfWeek','PdDistrict', 'Address', 'X', 'Y'], values='Category',aggfunc=[len,listCat])\n",
    "print('The shape of AllDF after drop_duplicates: '+str(allDF.shape))\n",
    "print(Train_duplicated.shape)\n",
    "\n",
    "trainDF,testDF=train_test_split_DataFrame(allDF, test_size=Rate_ALL, considerTime=ConsiderTime, random_state=None)\n",
    "print('Address_counts_allDF_trainDF_testDF: ' + str(len(allDF[\"Address\"].unique())) + '_'+ str(len(trainDF[\"Address\"].unique())) + '_' + str(len(testDF[\"Address\"].unique())))\n",
    "\n",
    "N_AllSample=allDF.shape[0]\n",
    "N_AllTrain=trainDF.shape[0]\n",
    "N_AllTest=testDF.shape[0]\n",
    "N_CLASS=len(allDF[\"Category\"].unique())\n",
    "print('The # of AllDF, AllTrain, AllTest, is: '+str(N_AllSample)+','+str(N_AllTrain)+','+str(N_AllTest))\n",
    "#################Now proceed as before#################\n",
    "print('-----------LOGOODS: Address-------------')\n",
    "logodds_A,logoddsPF_A=field2Vec(trainDF,testDF,\"Address\")\n",
    "if needT_D_M_Y:\n",
    "    trainDF[\"T_D_M_Y\"]=trainDF[\"Dates\"].apply(Dates2TDMY)\n",
    "    trainDF[\"T_D_M_Y\"]=trainDF[\"T_D_M_Y\"]+trainDF[\"DayOfWeek\"]\n",
    "    if Rate_ALL>0:\n",
    "        testDF[[\"X\",\"Y\"]]=xy_scaler.transform(testDF[[\"X\",\"Y\"]])\n",
    "        testDF[\"T_D_M_Y\"]=testDF[\"Dates\"].apply(Dates2TDMY)\n",
    "        testDF[\"T_D_M_Y\"]=testDF[\"T_D_M_Y\"]+testDF[\"DayOfWeek\"]\n",
    "    print('-----------LOGOODS: T_D_M_Y-------------')\n",
    "    logodds_T,logoddsPF_T=field2Vec(trainDF,testDF,\"T_D_M_Y\")    \n",
    "else:\n",
    "    logodds_T=None\n",
    "    logoddsPF_T=None\n",
    "    \n",
    "print('-----------LOGOODS: parse_data of Alltrain  -------------')\n",
    "features, labels=parse_data(trainDF,logodds_A,logoddsPF_A,logodds_T,logoddsPF_T,needT_D_M_Y) \n",
    "if Rate_ALL>0:\n",
    "    print('-----------LOGOODS: parse_data of Alltest  -------------')\n",
    "    features_test, labels_test=parse_data(testDF,logodds_A,logoddsPF_A,logodds_T,logoddsPF_T,needT_D_M_Y)###########和训练集使用同样的时间和地点Logoodds值#####\n",
    "    x_test=features_test.values\n",
    "    y_test=labels_test.values\n",
    "    y_test = keras.utils.to_categorical(LabelEncoder().fit_transform(np.array(y_test)), num_classes=N_CLASS)\n",
    "\n",
    "print(features.columns.tolist())\n",
    "print(len(features.columns))\n",
    "\n",
    "collist=features.columns.tolist()\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scaler.fit(features)\n",
    "features[collist]=scaler.transform(features)\n",
    "if Rate_ALL>0:\n",
    "    features_test[collist]=scaler.transform(features_test)###########和训练集使用同样的scaler值#####\n",
    "######################################################\n",
    "#############################先进行过采样，然后再根据时间来排序##################################\n",
    "if needOverSampler:\n",
    "    print('------------RandomOverSampler--------------')\n",
    "    ros = RandomOverSampler()\n",
    "    featuresArrayOverSampler, labelsArrayOverSampler = ros.fit_resample(features.values,labels.values)#####过采样#####\n",
    "    N_AllTrain_OverSampler=int(featuresArrayOverSampler.shape[0])\n",
    "    print('Shape of OverSampler of AllTrain: '+str(featuresArrayOverSampler.shape))\n",
    "else:\n",
    "    featuresArrayOverSampler=features.values\n",
    "    labelsArrayOverSampler=labels.values\n",
    "    N_AllTrain_OverSampler=int(featuresArrayOverSampler.shape[0])\n",
    "    print('------------Attention: we do not RandomOverSampler---------------')\n",
    "if ConsiderTime:\n",
    "    #####按照年（第6列）月（第5列）日（第4列）时（第3列）排序\n",
    "    print('------------ConsiderTime:  Sorting--------------')\n",
    "    time_temp=featuresArrayOverSampler[:,2]+np.dot(featuresArrayOverSampler[:,3],100)+np.dot(featuresArrayOverSampler[:,4],10000)+np.dot(featuresArrayOverSampler[:,5],1000000)\n",
    "    features_label_time=np.column_stack((featuresArrayOverSampler,labelsArrayOverSampler))\n",
    "    features_label_time=np.column_stack((features_label_time,time_temp))\n",
    "    features_label_time =features_label_time[np.argsort(features_label_time[:,-1])]\n",
    "    labelsArrayOverSampler=features_label_time[:,-2]\n",
    "    featuresArrayOverSampler=features_label_time[:,0:featuresArrayOverSampler.shape[1]]\n",
    "    del features_label_time\n",
    "    #############################先进行过采样，然后再根据时间来排序----结束############################\n",
    "if Rate_ALL>0:\n",
    "    print('------------RandomOverSampler for AllTest--------------')\n",
    "    ros = RandomOverSampler()\n",
    "    featuresArray_test, labelsArray_test = ros.fit_resample(features_test.values,labels_test.values)#####过采样#####\n",
    "    N_AllTest_OverSampler=int(featuresArray_test.shape[0])\n",
    "    labelsArray_test = keras.utils.to_categorical(LabelEncoder().fit_transform(np.array(labelsArray_test)), num_classes=N_CLASS)\n",
    "    print('Shape of OverSampler of AllTest: '+str(featuresArray_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_Train_OverSampler= 162506\n",
      "BlockSize is: 325\n",
      "-----------------Building DNN model---------------------\n",
      "------------Building LSTM model--------------\n"
     ]
    }
   ],
   "source": [
    "DnnTrainFromZero=False\n",
    "RnnTrainFromZero=True\n",
    "ShuffleInTraining=False\n",
    "N_EPOCHS_0_DNN=7\n",
    "N_EPOCHS_DNN=3\n",
    "N_EPOCHS_0_RNN=17\n",
    "N_EPOCHS_RNN=1\n",
    "N_HN_1=128\n",
    "N_HN=64\n",
    "N_LAYERS=1\n",
    "N_BATCH=128\n",
    "Rate_Val=0.8#用20%作为原始训练集，分别训练DNN和RNN\n",
    "N_Split=2000#1000#400#500\n",
    "delay=-1\n",
    "N_Val_OverSampler=int(np.around(N_AllTrain_OverSampler*Rate_Val))\n",
    "N_Train_OverSampler=int(N_AllTrain_OverSampler-N_Val_OverSampler)\n",
    "print('N_Train_OverSampler= '+str(N_Train_OverSampler))\n",
    "N_CLASS=len(allDF[\"Category\"].unique())\n",
    "input_dim=featuresArrayOverSampler.shape[1]\n",
    "output_dim=N_CLASS\n",
    "BlockSize=int(np.floor(N_Val_OverSampler/N_Split))\n",
    "print('BlockSize is: '+str(BlockSize)) \n",
    "lookback=int(BlockSize)\n",
    "\n",
    "print('-----------------Building DNN model---------------------')\n",
    "model = Sequential()\n",
    "model.add(Dense(N_HN_1,input_dim=input_dim))\n",
    "model.add(BatchNormalization())\n",
    "model.add(PReLU())\n",
    "for i in range(N_LAYERS):\n",
    "    model.add(Dense(N_HN))\n",
    "    model.add(BatchNormalization())   \n",
    "    model.add(PReLU())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(output_dim))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy', metrics.top_k_categorical_accuracy])\n",
    "\n",
    "print('------------Building LSTM model--------------')\n",
    "RNNmodel = Sequential()\n",
    "RNNmodel.add(LSTM(N_HN_1,dropout=0.5, recurrent_dropout=0.5,return_sequences=True,input_shape=(None,input_dim)))\n",
    "RNNmodel.add(LSTM(N_HN,dropout=0.5, recurrent_dropout=0.5,return_sequences=True,))\n",
    "RNNmodel.add(LSTM(N_HN,dropout=0.5, recurrent_dropout=0.5))\n",
    "RNNmodel.add(BatchNormalization())\n",
    "RNNmodel.add(Dense(output_dim))\n",
    "RNNmodel.add(Activation('softmax'))\n",
    "# RNNmodel.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy', metrics.top_k_categorical_accuracy])\n",
    "RNNmodel.compile(loss='categorical_crossentropy', optimizer=RMSprop(lr=0.001),metrics=['accuracy', metrics.top_k_categorical_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------Prepare dataset for DNN---------------------------------\n",
      "--------Spllit train val according to time!---------\n",
      "(162506, 110)\n",
      "The shape of x_val_i is: (1, 110)\n",
      "N_Train_OverSampler is: 162506\n",
      "--------------------------Generator AllTrain_set, Train_set and Val_set for LSTM---------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('-------------------------Prepare dataset for DNN---------------------------------')\n",
    "labelsArrayOverSampler_1hot=keras.utils.to_categorical(LabelEncoder().fit_transform(np.array(labelsArrayOverSampler)), num_classes=N_CLASS)\n",
    "if ConsiderTime:\n",
    "    print('--------Spllit train val according to time!---------')\n",
    "    x_train=featuresArrayOverSampler[0:N_Train_OverSampler,:]\n",
    "    y_train=labelsArrayOverSampler_1hot[0:N_Train_OverSampler,:]\n",
    "else:\n",
    "    x_train,x_val,y_train,y_val = train_test_split(featuresArrayOverSampler,labelsArrayOverSampler_1hot,test_size=N_Val_OverSampler,train_size=N_Train_OverSampler, shuffle=True)\n",
    "print(str(x_train.shape))\n",
    "x_val_i=featuresArrayOverSampler[N_Train_OverSampler:N_Train_OverSampler+1,:]\n",
    "y_val_i=labelsArrayOverSampler_1hot[N_Train_OverSampler:N_Train_OverSampler+1,:]\n",
    "print('The shape of x_val_i is: '+str(x_val_i.shape))\n",
    "print('N_Train_OverSampler is: '+str(N_Train_OverSampler))\n",
    "\n",
    "print('--------------------------Generator AllTrain_set, Train_set and Val_set for LSTM---------------------------------')\n",
    "train_generator=generator(featuresArrayOverSampler, labelsArrayOverSampler_1hot, lookback=lookback, delay=delay, min_index=0, max_index=N_Train_OverSampler+1, shuffle=ShuffleInTraining, batch_size=N_BATCH, step=1)\n",
    "val_generator=generator_Val(featuresArrayOverSampler, labelsArrayOverSampler_1hot, lookback=lookback, delay=delay, min_index=N_Train_OverSampler-lookback+1, max_index=N_Train_OverSampler+2, shuffle=False, batch_size=1, step=1)\n",
    "#数值在[min_index, max_index)区间。当delay=1时，就是用[min_index, max_index)区间的样本预测，第max_index个样本。当delay=2时，就是预测第max_index+1个\n",
    "train_steps= (N_Train_OverSampler-lookback) // N_BATCH\n",
    "val_steps =  1 #(N_Val-lookback) // N_BATCH\n",
    "# val_xx,val_yy=next(val_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------Dnn Model has been loaded!!!-----------------\n"
     ]
    }
   ],
   "source": [
    "if DnnTrainFromZero:\n",
    "    print('------------DNN Training Go! Go! Go!!!!-----------')\n",
    "    fitting=model.fit(x_train, y_train, epochs=N_EPOCHS_0_DNN, batch_size=N_BATCH,verbose=1,validation_data=(x_val_i,y_val_i),shuffle=True)\n",
    "    #score01=model.predict(x_val_i, batch_size=None, verbose=0, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False)\n",
    "    #score0=model.evaluate(x=x_val_i, y=y_val_i, batch_size=None, verbose=1, sample_weight=None, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False)\n",
    "    RNNmodel.save('jjs_model_DnnV1.h5')\n",
    "    print('------------DNN train finished--------------------')\n",
    "else:\n",
    "    RNNmodel=load_model('jjs_model_DnnV1.h5')\n",
    "    print('------------Dnn Model has been loaded!!!-----------------')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------LSTM GO GO GO!!!!---------------------------------------------\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/17\n",
      "1267/1267 [==============================] - 1008s 795ms/step - loss: 2.8408 - accuracy: 0.1658 - top_k_categorical_accuracy: 0.5471 - val_loss: 2.8575 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "Epoch 2/17\n",
      "1267/1267 [==============================] - 1081s 853ms/step - loss: 2.6589 - accuracy: 0.2032 - top_k_categorical_accuracy: 0.6108 - val_loss: 2.7940 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "Epoch 3/17\n",
      "1267/1267 [==============================] - 1088s 859ms/step - loss: 2.5336 - accuracy: 0.2401 - top_k_categorical_accuracy: 0.6532 - val_loss: 2.5120 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 4/17\n",
      "1267/1267 [==============================] - 1098s 866ms/step - loss: 2.4887 - accuracy: 0.2528 - top_k_categorical_accuracy: 0.6666 - val_loss: 2.5046 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 5/17\n",
      "1267/1267 [==============================] - 1042s 823ms/step - loss: 2.4751 - accuracy: 0.2548 - top_k_categorical_accuracy: 0.6678 - val_loss: 2.4304 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 6/17\n",
      "1267/1267 [==============================] - 903s 713ms/step - loss: 2.4667 - accuracy: 0.2583 - top_k_categorical_accuracy: 0.6711 - val_loss: 2.4930 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 7/17\n",
      "1267/1267 [==============================] - 1029s 812ms/step - loss: 2.4601 - accuracy: 0.2582 - top_k_categorical_accuracy: 0.6726 - val_loss: 2.5955 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "Epoch 8/17\n",
      "1267/1267 [==============================] - 969s 765ms/step - loss: 2.4546 - accuracy: 0.2610 - top_k_categorical_accuracy: 0.6750 - val_loss: 2.5435 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 9/17\n",
      "1267/1267 [==============================] - 935s 738ms/step - loss: 2.4523 - accuracy: 0.2625 - top_k_categorical_accuracy: 0.6739 - val_loss: 2.4302 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 10/17\n",
      "1267/1267 [==============================] - 867s 684ms/step - loss: 2.4495 - accuracy: 0.2621 - top_k_categorical_accuracy: 0.6746 - val_loss: 2.4234 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 11/17\n",
      "1267/1267 [==============================] - 930s 734ms/step - loss: 2.4492 - accuracy: 0.2625 - top_k_categorical_accuracy: 0.6753 - val_loss: 2.4925 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 12/17\n",
      "1267/1267 [==============================] - 936s 739ms/step - loss: 2.4455 - accuracy: 0.2639 - top_k_categorical_accuracy: 0.6769 - val_loss: 2.5097 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 13/17\n",
      "1267/1267 [==============================] - 881s 695ms/step - loss: 2.4448 - accuracy: 0.2632 - top_k_categorical_accuracy: 0.6768 - val_loss: 2.4814 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "Epoch 14/17\n",
      "1267/1267 [==============================] - 903s 712ms/step - loss: 2.4412 - accuracy: 0.2636 - top_k_categorical_accuracy: 0.6767 - val_loss: 2.4632 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "Epoch 15/17\n",
      "1267/1267 [==============================] - 892s 704ms/step - loss: 2.4424 - accuracy: 0.2644 - top_k_categorical_accuracy: 0.6773 - val_loss: 2.4103 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 16/17\n",
      "1267/1267 [==============================] - 874s 690ms/step - loss: 2.4418 - accuracy: 0.2641 - top_k_categorical_accuracy: 0.6774 - val_loss: 2.3891 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 17/17\n",
      "1267/1267 [==============================] - 859s 678ms/step - loss: 2.4404 - accuracy: 0.2638 - top_k_categorical_accuracy: 0.6792 - val_loss: 2.4140 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 1.0000\n",
      "------------LSTM train finished--------------------\n"
     ]
    }
   ],
   "source": [
    "if RnnTrainFromZero:\n",
    "    print('---------------------------------------LSTM GO GO GO!!!!---------------------------------------------')\n",
    "    #train_xx,train_yy=next(train_generator)\n",
    "    #print('Y For RNN Train: '+str(train_yy))    \n",
    "    #val_xx,val_yy=next(val_generator)\n",
    "    #print('Y For RNN Val: '+str(val_yy))\n",
    "    history = RNNmodel.fit_generator(train_generator,steps_per_epoch=train_steps,epochs=N_EPOCHS_0_RNN,verbose=1,validation_data=val_generator,validation_steps=val_steps)\n",
    "    print('------------LSTM train finished--------------------')\n",
    "    RNNmodel.save('jjs_model_0215LSTMV1.h5')\n",
    "else:\n",
    "    RNNmodel=load_model('jjs_model_0212LSTMV1.h5')\n",
    "    print('------------LSTM Model has been loaded!!!-----------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RNNmodel=load_model('jjs_model_0213LSTMV1.h5')\n",
    "# history = RNNmodel.fit_generator(train_generator,steps_per_epoch=train_steps,epochs=1,verbose=1,validation_data=val_generator,validation_steps=val_steps)\n",
    "# RNNmodel.save('jjs_model_0210LSTMV2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(2.5*lookback)//N_BATCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------Start the loop training!!---------------------\n"
     ]
    }
   ],
   "source": [
    "print('-----------------Start the loop training!!---------------------')\n",
    "ACC=np.zeros([N_Split,4])\n",
    "P_DNN=np.zeros([N_Split,labelsArrayOverSampler_1hot.shape[-1]])\n",
    "P_RNN=np.zeros([N_Split,labelsArrayOverSampler_1hot.shape[-1]])\n",
    "G_NN =np.zeros([N_Split,labelsArrayOverSampler_1hot.shape[-1]])\n",
    "acc_3=np.zeros([N_Split,3])\n",
    "for i_s in range(N_Split):    #N_Split\n",
    "    StartIndexTrain=N_Train_OverSampler+i_s*BlockSize\n",
    "    StopIndexTrain =N_Train_OverSampler+(i_s+1)*BlockSize\n",
    "    x_train=featuresArrayOverSampler[StartIndexTrain:StopIndexTrain,:]\n",
    "    y_train=labelsArrayOverSampler_1hot[StartIndexTrain:StopIndexTrain,:]\n",
    "    #print('The shape of x_train is: '+str(x_train.shape))\n",
    "    #print(StopIndexTrain)\n",
    "    StartIndexVal=N_Train_OverSampler+(i_s+1)*BlockSize\n",
    "    StopIndexVal =N_Train_OverSampler+(i_s+1)*BlockSize+1\n",
    "    x_val_i=featuresArrayOverSampler[StartIndexVal:StopIndexVal,:]\n",
    "    y_val_i=labelsArrayOverSampler_1hot[StartIndexVal:StopIndexVal,:]\n",
    "    #print('The shape of x_val_i is: '+str(x_val_i.shape))\n",
    "    #print(StopIndexVal)\n",
    "    fittingDNN=model.fit(x_train, y_train, epochs=N_EPOCHS_DNN, batch_size=N_BATCH,verbose=2,validation_data=(x_val_i,y_val_i),shuffle=True)\n",
    "#     scoreDNN=model.predict(x_val_i, batch_size=None, verbose=0, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False)\n",
    "        \n",
    "    train_generator=generator(featuresArrayOverSampler, labelsArrayOverSampler_1hot, lookback=lookback, delay=delay, min_index=StartIndexTrain, max_index=StopIndexTrain+1, shuffle=ShuffleInTraining, batch_size=N_BATCH, step=1)\n",
    "    fittingRNN=RNNmodel.fit_generator(train_generator,steps_per_epoch=(2.5*lookback)//N_BATCH,epochs=N_EPOCHS_RNN,verbose=2)\n",
    "    #train_xx,train_yy=next(train_generator)\n",
    "    #print('Y For RNN Train: '+str(train_yy))\n",
    "    \n",
    "    val_generator=generator_Val(featuresArrayOverSampler, labelsArrayOverSampler_1hot, lookback=lookback, delay=delay, min_index=StopIndexVal-lookback, max_index=StopIndexVal+2,shuffle=False, batch_size=1, step=1)\n",
    "#     val_xx,val_yy=next(val_generator)\n",
    "#     print('Y For RNN Val: '+str(val_yy))\n",
    "#     print(StartIndexVal)\n",
    "#     scoreRNN=RNNmodel.predict(val_generator, batch_size=None, verbose=0, steps=1, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False)\n",
    "    acc_3[i_s,:]=RNNmodel.evaluate(val_generator, batch_size=None, verbose=2, steps=1, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21.50807407  0.103       0.3915    ]\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(acc_3,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"P_DNN0215_1.txt\", P_DNN,fmt='%f',delimiter=',')\n",
    "np.savetxt(\"P_RNN0215_1.txt\", P_RNN,fmt='%f',delimiter=',')\n",
    "np.savetxt(\"G_NN0215_1.txt\", G_NN,fmt='%f',delimiter=',')\n",
    "np.savetxt(\"ACC0215_1.txt\", ACC,fmt='%f',delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "np.savetxt(\"ACC0211.txt\", ACC,fmt='%f',delimiter=',')\n",
    "P_DNN=np.zeros([N_Split,labelsArrayOverSampler_1hot.shape[-1]])\n",
    "P_RNN=np.zeros([N_Split,labelsArrayOverSampler_1hot.shape[-1]])\n",
    "G_NN =np.zeros([N_Split,labelsArrayOverSampler_1hot.shape[-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TR1:steps_per_epoch=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object generator at 0x7ff205d838e0>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
    "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "begin from 0206 23:35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5, 0.5, 0.5])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aaaaa=np.mean(ACC,axis=0)\n",
    "aaaaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "a=[[0,0,1,0]]\n",
    "b=[[1,0,0,0]]\n",
    "c=[[1,0,0,0]]\n",
    "rr=np.sum(np.abs(np.array(a)-np.array(b)))\n",
    "print(rr)\n",
    "ss=np.sum(np.abs(np.array(c)-np.array(b)))\n",
    "print(ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1300"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lookback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
